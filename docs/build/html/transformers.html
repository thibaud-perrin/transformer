<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>transformers package &#8212; Transformer v1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="transformers-package">
<h1>transformers package<a class="headerlink" href="#transformers-package" title="Permalink to this heading">¶</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="transformers.blocks.html">transformers.blocks package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transformers.blocks.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformers.blocks.layers.html">transformers.blocks.layers package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformers.blocks.layers.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformers.blocks.layers.html#module-transformers.blocks.layers.FeedForward">transformers.blocks.layers.FeedForward module</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformers.blocks.layers.html#module-transformers.blocks.layers.LayerNorm">transformers.blocks.layers.LayerNorm module</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformers.blocks.layers.html#module-transformers.blocks.layers.MultiHeadAttention">transformers.blocks.layers.MultiHeadAttention module</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformers.blocks.layers.html#module-transformers.blocks.layers">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformers.blocks.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformers.blocks.html#module-transformers.blocks.DecoderBlock">transformers.blocks.DecoderBlock module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformers.blocks.html#transformers.blocks.DecoderBlock.DecoderBlock"><code class="docutils literal notranslate"><span class="pre">DecoderBlock</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformers.blocks.html#transformers.blocks.DecoderBlock.DecoderBlock.forward"><code class="docutils literal notranslate"><span class="pre">DecoderBlock.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformers.blocks.html#module-transformers.blocks.EncoderBlock">transformers.blocks.EncoderBlock module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformers.blocks.html#transformers.blocks.EncoderBlock.EncoderBlock"><code class="docutils literal notranslate"><span class="pre">EncoderBlock</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformers.blocks.html#transformers.blocks.EncoderBlock.EncoderBlock.forward"><code class="docutils literal notranslate"><span class="pre">EncoderBlock.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformers.blocks.html#module-transformers.blocks">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading">¶</a></h2>
</section>
<section id="module-transformers.DataLoaderFactory">
<span id="transformers-dataloaderfactory-module"></span><h2>transformers.DataLoaderFactory module<a class="headerlink" href="#module-transformers.DataLoaderFactory" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformers.DataLoaderFactory.DataLoaderFactory">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformers.DataLoaderFactory.</span></span><span class="sig-name descname"><span class="pre">DataLoaderFactory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.DataLoaderFactory.DataLoaderFactory" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class to instantiate PyTorch DataLoaders for different splits of a HuggingFace Dataset.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>block_size (int): The maximum sequence length for tokenization.</p></li>
<li><p>batch_size (int): The batch size for DataLoader.</p></li>
<li><p>tokenizer (Tokenizer): a tokenizer that has an encode method.</p></li>
<li><p>device (str): ‘cpu’ or ‘cuda’, depending on whether we use CPU or GPU.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="transformers.DataLoaderFactory.DataLoaderFactory.get_batch">
<span class="sig-name descname"><span class="pre">get_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.DataLoaderFactory.DataLoaderFactory.get_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose the correct DataLoader and yield batches from it.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>split (str): ‘train’, ‘val’ or ‘test’.</p></li>
</ul>
</dd>
<dt>Yields:</dt><dd><ul class="simple">
<li><p>Dict: a dictionary with keys ‘inputs’, ‘targets’ and ‘translation’, containing a batch of tokenized input, target sequences and original translation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformers.DataLoaderFactory.TranslationDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformers.DataLoaderFactory.</span></span><span class="sig-name descname"><span class="pre">TranslationDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.DataLoaderFactory.TranslationDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>dataset (Dataset): a dataset from HuggingFace datasets library.</p></li>
<li><p>tokenizer (Tokenizer): The custom tiktoken tokenizer used to encode sequences.</p></li>
<li><p>block_size (int): The maximum sequence length for tokenization.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-transformers.Decoder">
<span id="transformers-decoder-module"></span><h2>transformers.Decoder module<a class="headerlink" href="#module-transformers.Decoder" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformers.Decoder.Decoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformers.Decoder.</span></span><span class="sig-name descname"><span class="pre">Decoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Decoder.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>This class implements the decoder part of the Transformer model.</p>
<p>The Decoder consists of several DecoderBlocks arranged in sequence. The input first goes through an embedding 
layer followed by a positional encoding layer. The output of this is then passed through each DecoderBlock in 
sequence.</p>
<dl class="simple">
<dt>Attributes:</dt><dd><ul class="simple">
<li><p>decoder (nn.ModuleDict): A dictionary of modules making up the transformer decoder.</p></li>
<li><p>lm_head (nn.Linear): The final linear layer mapping from the embedding dimension to the vocabulary size.</p></li>
<li><p>config (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Config</span></code>): The configuration object for the transformer model.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The weight of the embedding layer and the linear layer are shared.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>config (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Config</span></code>): The configuration object with attributes such as <cite>vocab_size</cite>, <cite>block_size</cite>, <cite>n_embd</cite>, <cite>dropout</cite>, <cite>n_layer</cite>, and <cite>bias</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="transformers.Decoder.Decoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enc_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Decoder.Decoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>idx (torch.Tensor): The input tensor to the forward pass.</p></li>
<li><p>enc_output (torch.Tensor): The output tensor from the encoder.</p></li>
<li><p>targets (torch.Tensor, optional): The target tensor against which the loss will be calculated.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>torch.Tensor: The output tensor (logits) of the model.</p></li>
<li><p>list: all layers of decoder attentions weights.</p></li>
<li><p>list: all layers cross attentions weights.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformers.Decoder.Decoder.get_num_params">
<span class="sig-name descname"><span class="pre">get_num_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">non_embedding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#transformers.Decoder.Decoder.get_num_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of parameters in the model.
For non-embedding count (default), the position embeddings get subtracted.
The token embeddings would too, except due to the parameter sharing these
params are actually used as weights in the final layer, so we include them.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>non_embedding (bool): If True, excludes the position embeddings count from the total. Default is True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>int: The number of parameters in the model.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-transformers.Encoder">
<span id="transformers-encoder-module"></span><h2>transformers.Encoder module<a class="headerlink" href="#module-transformers.Encoder" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformers.Encoder.Encoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformers.Encoder.</span></span><span class="sig-name descname"><span class="pre">Encoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Encoder.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A class that implements the encoder part of the Transformer model.</p>
<p>The encoder consists of several EncoderBlocks arranged in sequence.
The input first goes through an embedding layer followed by a positional encoding layer.
The output of this is then passed through each EncoderBlock in sequence.</p>
<dl class="simple">
<dt>Attributes:</dt><dd><ul class="simple">
<li><p>encoder (nn.ModuleDict): A dictionary of modules making up the transformer encoder.</p></li>
</ul>
</dd>
<dt>Args:</dt><dd><ul class="simple">
<li><p>config (Config): A configuration object with attributes such as <cite>vocab_size</cite>, <cite>block_size</cite>, <cite>n_embd</cite>, <cite>dropout</cite>, <cite>n_layer</cite>, and <cite>bias</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="transformers.Encoder.Encoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Encoder.Encoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>idx (torch.Tensor): The input tensor to the forward pass.</p></li>
<li><p>targets (torch.Tensor, optional): The target tensor against which the loss will be calculated.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>torch.Tensor: The output tensor (logits) of the model.</p></li>
<li><p>list: all encoder layers attentions weights.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformers.Encoder.Encoder.get_num_params">
<span class="sig-name descname"><span class="pre">get_num_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">non_embedding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Encoder.Encoder.get_num_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of parameters in the model.
For non-embedding count (default), the position embeddings get subtracted.
The token embeddings would too, except due to the parameter sharing these
params are actually used as weights in the final layer, so we include them.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>-non_embedding (bool, optional): If True, excludes the position embeddings count from the total (Default is True).</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>int: The number of parameters in the model.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-transformers.Tokenizer">
<span id="transformers-tokenizer-module"></span><h2>transformers.Tokenizer module<a class="headerlink" href="#module-transformers.Tokenizer" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformers.Tokenizer.Tokenizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformers.Tokenizer.</span></span><span class="sig-name descname"><span class="pre">Tokenizer</span></span><a class="headerlink" href="#transformers.Tokenizer.Tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A tokenizer class for encoding/decoding text sequences.</p>
<dl class="py method">
<dt class="sig sig-object py" id="transformers.Tokenizer.Tokenizer.sequence_cleaner">
<span class="sig-name descname"><span class="pre">sequence_cleaner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Tokenizer.Tokenizer.sequence_cleaner" title="Permalink to this definition">¶</a></dt>
<dd><p>Method used to remove BOS/PAD/EOS special tokens</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformers.Tokenizer.Tokenizer.sequence_padding">
<span class="sig-name descname"><span class="pre">sequence_padding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#transformers.Tokenizer.Tokenizer.sequence_padding" title="Permalink to this definition">¶</a></dt>
<dd><p>Method to add BOS/PAD/EOS special tokens and ensure the sequence length is within the maximum size.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>sequence (torch.Tensor or list): The input sequence.
max_size (int, optional): The maximum allowed size for the sequence. Defaults to 512.
device (str, optional): The device where the tensors will be allocated. Defaults to “cpu”.</p>
</dd>
<dt>Returns:</dt><dd><p>torch.Tensor: The processed sequence with special tokens added and length limited.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformers.Tokenizer.Tokenizer.tokenize">
<span class="sig-name descname"><span class="pre">tokenize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span></span></span><a class="headerlink" href="#transformers.Tokenizer.Tokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Method to generate a str list of separated tokens token.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>sequence (torch.Tensor or list): The input sequence.
device (str, optional): The device where the tensors will be allocated. Defaults to “cpu”.</p>
</dd>
<dt>Returns:</dt><dd><p>list: The processed sequence converted in a list of tokens in string format.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformers.Tokenizer.Tokenizer.tokenize_from_str">
<span class="sig-name descname"><span class="pre">tokenize_from_str</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span></span></span><a class="headerlink" href="#transformers.Tokenizer.Tokenizer.tokenize_from_str" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformers.Tokenizer.Tokenizer.vocab_size">
<span class="sig-name descname"><span class="pre">vocab_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#transformers.Tokenizer.Tokenizer.vocab_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Method to return the size of the vocabulary in the tokenizer’s encoding.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>int: The size of the vocabulary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-transformers.Transformer">
<span id="transformers-transformer-module"></span><h2>transformers.Transformer module<a class="headerlink" href="#module-transformers.Transformer" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformers.Transformer.Transformer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformers.Transformer.</span></span><span class="sig-name descname"><span class="pre">Transformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Transformer.Transformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>This class implements the Transformer model, which includes both the encoder and decoder.</p>
<p>The Transformer is a sequence transduction model that uses attention mechanisms.
It is primarily used in tasks that require understanding of context or relationships among words in a text.</p>
<dl class="simple">
<dt>Attributes:</dt><dd><ul class="simple">
<li><p>encoder (Encoder): The transformer encoder.</p></li>
<li><p>decoder (Decoder): The transformer decoder.</p></li>
<li><p>config (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Config</span></code>): The configuration object for the transformer model.</p></li>
</ul>
</dd>
<dt>Args:</dt><dd><ul class="simple">
<li><p>config (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Config</span></code>): The configuration object with attributes such as <cite>vocab_size</cite>, <cite>block_size</cite>, <cite>n_embd</cite>, <cite>dropout</cite>, <cite>n_layer</cite>, and <cite>bias</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="transformers.Transformer.Transformer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Transformer.Transformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>src (torch.Tensor): The input tensor to the encoder.</p></li>
<li><p>tgt (torch.Tensor): The input tensor to the decoder.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>torch.Tensor: The output tensor (logits) of the model.</p></li>
<li><p>torch.Tensor: The loss tensor calculated on the basis of the decoder’s output and target tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformers.Transformer.Transformer.load_model">
<span class="sig-name descname"><span class="pre">load_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Transformer.Transformer.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the model state from a file.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>path (str): The path to the file from where the model state should be loaded.</p>
</dd>
<dt>Raises:</dt><dd><p>ValueError: If the specified file does not exist.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformers.Transformer.Transformer.save_model">
<span class="sig-name descname"><span class="pre">save_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Transformer.Transformer.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current state of the model to a file.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>path (str): The path to the file where the model state should be saved.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformers.Transformer.Transformer.translate_beam_search">
<span class="sig-name descname"><span class="pre">translate_beam_search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.Transformer.Transformer.translate_beam_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates translations of the source sequences using beam search.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>src (torch.Tensor): The source sequences to translate.
beam_size (int, optional): The number of beams to use in beam search. Default is 5.</p>
</dd>
<dt>Returns:</dt><dd><p>Tuple[torch.Tensor, Dict[str, torch.Tensor]]: The best sequence found by beam search and a dictionary containing the attention weights.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-transformers.TransformerConfig">
<span id="transformers-transformerconfig-module"></span><h2>transformers.TransformerConfig module<a class="headerlink" href="#module-transformers.TransformerConfig" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformers.TransformerConfig.</span></span><span class="sig-name descname"><span class="pre">TransformerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_head</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_embd</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_interval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_iters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">visualize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Data class that stores the configuration for a Transformer model.</p>
<dl class="simple">
<dt>Attributes:</dt><dd><ul class="simple">
<li><p>tokenizer: An instance of the Tokenizer class.</p></li>
<li><p>block_size (int): Number of tokens in each sequence. Defaults to 512.</p></li>
<li><p>batch_size (int): Number of sequences in each batch. Defaults to 12.</p></li>
<li><p>vocab_size (int): Total size of the tokenizer vocabulary. It is set to the size of the tokenizer vocabulary.</p></li>
<li><p>n_layer (int): Number of transformer encoder and decoder blocks (N). Defaults to 1.</p></li>
<li><p>n_head (int): Number of heads in each attention block. Defaults to 2.</p></li>
<li><p>n_embd (int): Token embedding size. This is from the original Transformer paper. Defaults to 128.</p></li>
<li><p>dropout (float): Dropout rate to use in the Transformer model. Defaults to 0.1.</p></li>
<li><dl class="simple">
<dt>bias (bool): Indicates whether to use bias in Linears and LayerNorms.</dt><dd><p>If True, bias is used similar to GPT-2.
If False, it is a bit better and faster. Defaults to False.</p>
</dd>
</dl>
</li>
<li><p>device (str): The device to run the model on. Defaults to ‘cpu’. ‘cuda’ is used if a GPU is available.</p></li>
<li><p>learning_rate (float): Learning rate for the model optimization. Defaults to 3e-4.</p></li>
<li><p>max_iters (int): Number of training steps. Defaults to 20.</p></li>
<li><p>eval_interval (int): Number of steps between each validation dataset. Defaults to 5.</p></li>
<li><p>eval_iters (int): Number of validation epochs. Defaults to 20.</p></li>
<li><p>visualize (bool): Define if we want to get the attention scores.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.batch_size">
<span class="sig-name descname"><span class="pre">batch_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">12</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.batch_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.block_size">
<span class="sig-name descname"><span class="pre">block_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">256</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.block_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.device">
<span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'cpu'</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.dropout">
<span class="sig-name descname"><span class="pre">dropout</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.1</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.dropout" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.eval_interval">
<span class="sig-name descname"><span class="pre">eval_interval</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">200</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.eval_interval" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.eval_iters">
<span class="sig-name descname"><span class="pre">eval_iters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">20</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.eval_iters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.learning_rate">
<span class="sig-name descname"><span class="pre">learning_rate</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.0003</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.max_iters">
<span class="sig-name descname"><span class="pre">max_iters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">2000</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.max_iters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.n_embd">
<span class="sig-name descname"><span class="pre">n_embd</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">256</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.n_embd" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.n_head">
<span class="sig-name descname"><span class="pre">n_head</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">4</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.n_head" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.n_layer">
<span class="sig-name descname"><span class="pre">n_layer</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">2</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.n_layer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.tokenizer">
<span class="sig-name descname"><span class="pre">tokenizer</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">any</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.tokenizer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.visualize">
<span class="sig-name descname"><span class="pre">visualize</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.visualize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformers.TransformerConfig.TransformerConfig.vocab_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vocab_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#transformers.TransformerConfig.TransformerConfig.vocab_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the total size of the tokenizer vocabulary.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>int: The size of the tokenizer vocabulary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-transformers">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-transformers" title="Permalink to this heading">¶</a></h2>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Transformer</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, Thibaud Perrin.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.0.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="_sources/transformers.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>