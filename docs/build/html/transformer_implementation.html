<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>transformer_implementation package &#8212; Transformer v1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="transformer_implementation.blocks package" href="transformer_implementation.blocks.html" />
    <link rel="prev" title="transformer" href="modules.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="transformer-implementation-package">
<h1>transformer_implementation package<a class="headerlink" href="#transformer-implementation-package" title="Permalink to this heading">¶</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="transformer_implementation.blocks.html">transformer_implementation.blocks package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transformer_implementation.blocks.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_implementation.blocks.layers.html">transformer_implementation.blocks.layers package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_implementation.blocks.layers.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_implementation.blocks.layers.html#module-transformer_implementation.blocks.layers.FeedForward">transformer_implementation.blocks.layers.FeedForward module</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_implementation.blocks.layers.html#module-transformer_implementation.blocks.layers.LayerNorm">transformer_implementation.blocks.layers.LayerNorm module</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_implementation.blocks.layers.html#module-transformer_implementation.blocks.layers.MultiHeadAttention">transformer_implementation.blocks.layers.MultiHeadAttention module</a></li>
<li class="toctree-l4"><a class="reference internal" href="transformer_implementation.blocks.layers.html#module-transformer_implementation.blocks.layers">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformer_implementation.blocks.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformer_implementation.blocks.html#module-transformer_implementation.blocks.DecoderBlock">transformer_implementation.blocks.DecoderBlock module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_implementation.blocks.html#transformer_implementation.blocks.DecoderBlock.DecoderBlock"><code class="docutils literal notranslate"><span class="pre">DecoderBlock</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_implementation.blocks.html#transformer_implementation.blocks.DecoderBlock.DecoderBlock.forward"><code class="docutils literal notranslate"><span class="pre">DecoderBlock.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformer_implementation.blocks.html#module-transformer_implementation.blocks.EncoderBlock">transformer_implementation.blocks.EncoderBlock module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_implementation.blocks.html#transformer_implementation.blocks.EncoderBlock.EncoderBlock"><code class="docutils literal notranslate"><span class="pre">EncoderBlock</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="transformer_implementation.blocks.html#transformer_implementation.blocks.EncoderBlock.EncoderBlock.forward"><code class="docutils literal notranslate"><span class="pre">EncoderBlock.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformer_implementation.blocks.html#module-transformer_implementation.blocks">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading">¶</a></h2>
</section>
<section id="module-transformer_implementation.DataLoaderFactory">
<span id="transformer-implementation-dataloaderfactory-module"></span><h2>transformer_implementation.DataLoaderFactory module<a class="headerlink" href="#module-transformer_implementation.DataLoaderFactory" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_implementation.DataLoaderFactory.DataLoaderFactory">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_implementation.DataLoaderFactory.</span></span><span class="sig-name descname"><span class="pre">DataLoaderFactory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5000000</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.DataLoaderFactory.DataLoaderFactory" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Factory class to create dataloaders for training, validation, and testing datasets.</p>
<p>It initializes the datasets and dataloaders for the given block size, batch size, 
tokenizer, and device. The dataloaders can be accessed directly through the 
corresponding attributes.</p>
<section id="attributes">
<h3>Attributes<a class="headerlink" href="#attributes" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>train_data<span class="classifier">TranslationDataset</span></dt><dd><p>The training dataset.</p>
</dd>
<dt>val_data<span class="classifier">TranslationDataset</span></dt><dd><p>The validation dataset.</p>
</dd>
<dt>test_data<span class="classifier">TranslationDataset</span></dt><dd><p>The testing dataset.</p>
</dd>
<dt>dataloader_train<span class="classifier">torch.utils.data.DataLoader</span></dt><dd><p>Dataloader for the training dataset.</p>
</dd>
<dt>dataloader_val<span class="classifier">torch.utils.data.DataLoader</span></dt><dd><p>Dataloader for the validation dataset.</p>
</dd>
<dt>dataloader_test<span class="classifier">torch.utils.data.DataLoader</span></dt><dd><p>Dataloader for the testing dataset.</p>
</dd>
</dl>
</section>
<section id="methods">
<h3>Methods<a class="headerlink" href="#methods" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>__len__() -&gt; int:</dt><dd><p>Prints and returns the number of items in each dataset and total.</p>
</dd>
<dt>get_batch(split: str) -&gt; dict:</dt><dd><p>Returns a generator that iterates over the batches in the specified split.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.DataLoaderFactory.DataLoaderFactory.get_batch">
<span class="sig-name descname"><span class="pre">get_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.DataLoaderFactory.DataLoaderFactory.get_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a generator that iterates over the batches in the specified split.</p>
<section id="parameters">
<h4>Parameters<a class="headerlink" href="#parameters" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>split<span class="classifier">str</span></dt><dd><p>The split to use. Must be one of ‘train’, ‘val’, or ‘test’.</p>
</dd>
</dl>
</section>
<section id="yields">
<h4>Yields<a class="headerlink" href="#yields" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>dict</dt><dd><p>The next batch in the specified split. Each batch is a dictionary that 
contains tensors moved to the specified device and the ‘translation’ field.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_implementation.DataLoaderFactory.TranslationDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_implementation.DataLoaderFactory.</span></span><span class="sig-name descname"><span class="pre">TranslationDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.DataLoaderFactory.TranslationDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>Dataset for English to French translation tasks.</p>
<p>The dataset includes ‘translation’ field which is a dict that contains 
source text in English (‘en’) and target text in French (‘fr’).</p>
<section id="id1">
<h3>Attributes<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>dataset<span class="classifier">object</span></dt><dd><p>The dataset object containing translations.</p>
</dd>
<dt>tokenizer<span class="classifier">object</span></dt><dd><p>The tokenizer object used for encoding the translations.</p>
</dd>
<dt>block_size<span class="classifier">int</span></dt><dd><p>The maximum length of the tokenized sequences.</p>
</dd>
</dl>
</section>
<section id="id2">
<h3>Methods<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>__getitem__(index: int) -&gt; dict:</dt><dd><p>Returns the tokenized input and target sequences, their corresponding masks, 
and the original translation for a given index.</p>
</dd>
<dt>__len__() -&gt; int:</dt><dd><p>Returns the number of items in the dataset.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
<section id="module-transformer_implementation.Decoder">
<span id="transformer-implementation-decoder-module"></span><h2>transformer_implementation.Decoder module<a class="headerlink" href="#module-transformer_implementation.Decoder" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_implementation.Decoder.Decoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_implementation.Decoder.</span></span><span class="sig-name descname"><span class="pre">Decoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Decoder.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implements a decoder module in PyTorch.</p>
<p>This class is a child of the PyTorch nn.Module class. The decoder uses 
embeddings for both the vocabulary and the positions. The core of the 
decoder is a sequence of DecoderBlock modules. The output of the decoder 
is then processed by a linear layer to produce the final output.</p>
<section id="id3">
<h3>Attributes<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>config<span class="classifier">object</span></dt><dd><p>A configuration object with the necessary attributes for the decoder.</p>
</dd>
<dt>decoder<span class="classifier">torch.nn.ModuleDict</span></dt><dd><p>A dictionary containing the decoder components, including the 
embeddings, dropout, DecoderBlock sequence, and LayerNorm.</p>
</dd>
<dt>lm_head<span class="classifier">torch.nn.Linear</span></dt><dd><p>A linear layer for producing the final output of the decoder.</p>
</dd>
</dl>
</section>
<section id="id4">
<h3>Methods<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>get_num_params(non_embedding: bool = True) -&gt; int:</dt><dd><p>Returns the number of parameters in the decoder.</p>
</dd>
<dt>_init_weights(module):</dt><dd><p>Initializes the weights of the specified module.</p>
</dd>
<dt>forward(idx, enc_output=None, src_mask=None, tgt_mask=None):</dt><dd><p>Computes the forward pass of the decoder.</p>
</dd>
</dl>
</section>
<section id="id5">
<h3>Parameters<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>config<span class="classifier">object</span></dt><dd><p>A configuration object with necessary attributes, including 
vocab_size, n_embd, block_size, dropout, n_layer, and bias.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Decoder.Decoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enc_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Decoder.Decoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the forward pass of the decoder.</p>
<section id="id6">
<h4>Parameters<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>idx<span class="classifier">torch.Tensor</span></dt><dd><p>The input tensor with token indices.</p>
</dd>
<dt>enc_output<span class="classifier">torch.Tensor, optional</span></dt><dd><p>The output of the encoder. Default is None.</p>
</dd>
<dt>src_mask<span class="classifier">torch.Tensor, optional</span></dt><dd><p>The mask for the source sequence. Default is None.</p>
</dd>
<dt>tgt_mask<span class="classifier">torch.Tensor, optional</span></dt><dd><p>The mask for the target sequence. Default is None.</p>
</dd>
</dl>
</section>
<section id="returns">
<h4>Returns<a class="headerlink" href="#returns" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>Tuple[torch.Tensor, List[torch.Tensor], List[torch.Tensor]]</dt><dd><p>A tuple containing the output tensor, a list of attention scores 
from the decoder blocks, and a list of cross-attention scores.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Decoder.Decoder.get_num_params">
<span class="sig-name descname"><span class="pre">get_num_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">non_embedding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#transformer_implementation.Decoder.Decoder.get_num_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of parameters in the model.
For non-embedding count (default), the position embeddings get subtracted.
The token embeddings would too, except due to the parameter sharing these
params are actually used as weights in the final layer, so we include them.</p>
<section id="id7">
<h4>Parameters<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>non_embedding<span class="classifier">bool, optional</span></dt><dd><p>If True, does not count the parameters of the position embedding
layer. Default is True.</p>
</dd>
</dl>
</section>
<section id="id8">
<h4>Returns<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>int</dt><dd><p>The number of parameters in the decoder.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-transformer_implementation.Encoder">
<span id="transformer-implementation-encoder-module"></span><h2>transformer_implementation.Encoder module<a class="headerlink" href="#module-transformer_implementation.Encoder" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_implementation.Encoder.Encoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_implementation.Encoder.</span></span><span class="sig-name descname"><span class="pre">Encoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Encoder.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>The Encoder class implements a multi-layer transformer encoder.</p>
<p>This class inherits from the PyTorch nn.Module class and includes 
token and position embeddings, dropout, multiple encoder blocks, and
layer normalization.</p>
<section id="id9">
<h3>Attributes<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>config<span class="classifier">object</span></dt><dd><dl class="simple">
<dt>A configuration object with the following attributes:</dt><dd><p>vocab_size (int): The size of the vocabulary.
block_size (int): The maximum sequence length.
n_embd (int): The dimension of the embeddings.
dropout (float): The dropout rate.
n_layer (int): The number of transformer layers.
bias (bool): If True, the linear layers will include a bias term.</p>
</dd>
</dl>
</dd>
<dt>encoder<span class="classifier">torch.nn.ModuleDict</span></dt><dd><dl class="simple">
<dt>A dictionary-like module of several layers:</dt><dd><p>wte (torch.nn.Embedding): The token embeddings layer.
wpe (torch.nn.Embedding): The position embeddings layer.
drop (torch.nn.Dropout): The dropout layer.
h (torch.nn.ModuleList): The list of transformer layers.
ln_f (LayerNorm): The final layer normalization.</p>
</dd>
</dl>
</dd>
</dl>
</section>
<section id="id10">
<h3>Methods<a class="headerlink" href="#id10" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>get_num_params(non_embedding: bool = True) -&gt; int:</dt><dd><p>Returns the total number of parameters.</p>
</dd>
<dt>_init_weights(module):</dt><dd><p>Initializes the weights of the specified module.</p>
</dd>
<dt>forward(idx, mask=None):</dt><dd><p>Computes the forward pass of the encoder.</p>
</dd>
</dl>
</section>
<section id="id11">
<h3>Parameters<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>config<span class="classifier">object</span></dt><dd><dl class="simple">
<dt>A configuration object with the following attributes:</dt><dd><p>vocab_size (int): The size of the vocabulary.
block_size (int): The maximum sequence length.
n_embd (int): The dimension of the embeddings.
dropout (float): The dropout rate.
n_layer (int): The number of transformer layers.
bias (bool): If True, the linear layers will include a bias term.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Encoder.Encoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Encoder.Encoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the forward pass of the encoder.</p>
<section id="id12">
<h4>Parameters<a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>idx<span class="classifier">torch.Tensor</span></dt><dd><p>The input tensor with indices of tokens in the sequence.</p>
</dd>
<dt>mask<span class="classifier">torch.Tensor, optional</span></dt><dd><p>The mask tensor. If provided, it should have the same size as idx.</p>
</dd>
</dl>
</section>
<section id="id13">
<h4>Returns<a class="headerlink" href="#id13" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>Tuple[torch.Tensor, List[torch.Tensor]]</dt><dd><p>The output tensor after layer normalization and the list of attention
matrices from each transformer layer.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Encoder.Encoder.get_num_params">
<span class="sig-name descname"><span class="pre">get_num_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">non_embedding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Encoder.Encoder.get_num_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of parameters in the model.
For non-embedding count (default), the position embeddings get subtracted.
The token embeddings would too, except due to the parameter sharing these
params are actually used as weights in the final layer, so we include them.</p>
<section id="id14">
<h4>Parameters<a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>non_embedding<span class="classifier">bool, optional</span></dt><dd><p>If True, subtracts the number of embedding parameters from the total.</p>
</dd>
</dl>
</section>
<section id="id15">
<h4>Returns<a class="headerlink" href="#id15" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>int</dt><dd><p>The total number of parameters.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-transformer_implementation.Tokenizer">
<span id="transformer-implementation-tokenizer-module"></span><h2>transformer_implementation.Tokenizer module<a class="headerlink" href="#module-transformer_implementation.Tokenizer" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_implementation.Tokenizer.Tokenizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_implementation.Tokenizer.</span></span><span class="sig-name descname"><span class="pre">Tokenizer</span></span><a class="headerlink" href="#transformer_implementation.Tokenizer.Tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Implements a Tokenizer based on the tiktoken library for encoding and decoding sequences.</p>
<p>The tokenizer has special tokens for the beginning of sentence (BOS), end of sentence (EOS), and padding (PAD). 
The sequence can be padded to a fixed length for processing in batch.</p>
<section id="id16">
<h3>Attributes<a class="headerlink" href="#id16" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>BOS_IDX<span class="classifier">int</span></dt><dd><p>The index of the Beginning of Sentence (BOS) token.</p>
</dd>
<dt>EOS_IDX<span class="classifier">int</span></dt><dd><p>The index of the End of Sentence (EOS) token.</p>
</dd>
<dt>PAD_IDX<span class="classifier">int</span></dt><dd><p>The index of the Padding (PAD) token.</p>
</dd>
<dt>encoder<span class="classifier">tiktoken.Encoding</span></dt><dd><p>The encoding object used for converting sequences to and from tokens.</p>
</dd>
</dl>
</section>
<section id="id17">
<h3>Methods<a class="headerlink" href="#id17" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>vocab_size() -&gt; int:</dt><dd><p>Returns the size of the vocabulary.</p>
</dd>
<dt>sequence_padding(sequence, max_size: int, device: str) -&gt; torch.Tensor:</dt><dd><p>Returns the padded sequence as a tensor.</p>
</dd>
<dt>sequence_cleaner(sequence) -&gt; list:</dt><dd><p>Returns the cleaned sequence without any special tokens.</p>
</dd>
<dt>generate_padding_mask(seq, triu: bool, device: str) -&gt; torch.Tensor:</dt><dd><p>Returns a mask for the padding tokens in the sequence.</p>
</dd>
<dt>tokenize(sequence, device: str) -&gt; list:</dt><dd><p>Returns the tokenized sequence.</p>
</dd>
<dt>tokenize_from_str(sequence, device: str) -&gt; list:</dt><dd><p>Returns the tokenized sequence for a given string.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Tokenizer.Tokenizer.generate_padding_mask">
<span class="sig-name descname"><span class="pre">generate_padding_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">triu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Tokenizer.Tokenizer.generate_padding_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a mask for the padding tokens in the sequence.</p>
<section id="id18">
<h4>Parameters<a class="headerlink" href="#id18" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>seq<span class="classifier">torch.Tensor</span></dt><dd><p>The sequence for which the mask will be generated.</p>
</dd>
<dt>triu<span class="classifier">bool, optional</span></dt><dd><p>If True, the mask will be a upper triangular matrix. Defaults to False.</p>
</dd>
<dt>device<span class="classifier">str, optional</span></dt><dd><p>The device where the tensor will be allocated. Defaults to “cpu”.</p>
</dd>
</dl>
</section>
<section id="id19">
<h4>Returns<a class="headerlink" href="#id19" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>torch.Tensor</dt><dd><p>The mask for the sequence.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Tokenizer.Tokenizer.sequence_cleaner">
<span class="sig-name descname"><span class="pre">sequence_cleaner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Tokenizer.Tokenizer.sequence_cleaner" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the special tokens from the sequence.</p>
<section id="id20">
<h4>Parameters<a class="headerlink" href="#id20" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>sequence<span class="classifier">Union[torch.Tensor, list]</span></dt><dd><p>The sequence to be cleaned.</p>
</dd>
</dl>
</section>
<section id="id21">
<h4>Returns<a class="headerlink" href="#id21" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>list</dt><dd><p>The cleaned sequence.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Tokenizer.Tokenizer.sequence_padding">
<span class="sig-name descname"><span class="pre">sequence_padding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#transformer_implementation.Tokenizer.Tokenizer.sequence_padding" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the sequence to the max_size with the PAD token.</p>
<section id="id22">
<h4>Parameters<a class="headerlink" href="#id22" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>sequence<span class="classifier">Union[torch.Tensor, list]</span></dt><dd><p>The sequence to be padded.</p>
</dd>
<dt>max_size<span class="classifier">int, optional</span></dt><dd><p>The maximum size of the sequence after padding. Defaults to 512.</p>
</dd>
<dt>device<span class="classifier">str, optional</span></dt><dd><p>The device where the tensor will be allocated. Defaults to “cpu”.</p>
</dd>
</dl>
</section>
<section id="id23">
<h4>Returns<a class="headerlink" href="#id23" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>torch.Tensor</dt><dd><p>The padded sequence.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Tokenizer.Tokenizer.tokenize">
<span class="sig-name descname"><span class="pre">tokenize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span></span></span><a class="headerlink" href="#transformer_implementation.Tokenizer.Tokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenizes the sequence using the encoder.</p>
<section id="id24">
<h4>Parameters<a class="headerlink" href="#id24" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>sequence<span class="classifier">Union[torch.Tensor, list]</span></dt><dd><p>The sequence to be tokenized.</p>
</dd>
<dt>device<span class="classifier">str, optional</span></dt><dd><p>The device where the tensor will be allocated. Defaults to “cpu”.</p>
</dd>
</dl>
</section>
<section id="id25">
<h4>Returns<a class="headerlink" href="#id25" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>list</dt><dd><p>The tokenized sequence.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Tokenizer.Tokenizer.tokenize_from_str">
<span class="sig-name descname"><span class="pre">tokenize_from_str</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span></span></span><a class="headerlink" href="#transformer_implementation.Tokenizer.Tokenizer.tokenize_from_str" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenizes the string sequence using the encoder.</p>
<section id="id26">
<h4>Parameters<a class="headerlink" href="#id26" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>sequence<span class="classifier">str</span></dt><dd><p>The string sequence to be tokenized.</p>
</dd>
<dt>device<span class="classifier">str, optional</span></dt><dd><p>The device where the tensor will be allocated. Defaults to “cpu”.</p>
</dd>
</dl>
</section>
<section id="id27">
<h4>Returns<a class="headerlink" href="#id27" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>list</dt><dd><p>The tokenized sequence.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Tokenizer.Tokenizer.vocab_size">
<span class="sig-name descname"><span class="pre">vocab_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#transformer_implementation.Tokenizer.Tokenizer.vocab_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size of the vocabulary.</p>
<section id="id28">
<h4>Returns<a class="headerlink" href="#id28" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>int</dt><dd><p>The size of the vocabulary.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-transformer_implementation.Transformer">
<span id="transformer-implementation-transformer-module"></span><h2>transformer_implementation.Transformer module<a class="headerlink" href="#module-transformer_implementation.Transformer" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_implementation.Transformer.Transformer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_implementation.Transformer.</span></span><span class="sig-name descname"><span class="pre">Transformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Transformer.Transformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A PyTorch implementation of a Transformer model.</p>
<p>The Transformer model consists of an Encoder and a Decoder. 
It supports functionalities like forward pass, token generation, optimizer configuration,
and save/load model state.</p>
<section id="id29">
<h3>Attributes<a class="headerlink" href="#id29" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>config<span class="classifier">object</span></dt><dd><p>A configuration object with necessary attributes for Transformer model.</p>
</dd>
<dt>encoder<span class="classifier">Encoder</span></dt><dd><p>The encoder part of the Transformer model.</p>
</dd>
<dt>decoder<span class="classifier">Decoder</span></dt><dd><p>The decoder part of the Transformer model.</p>
</dd>
</dl>
</section>
<section id="id30">
<h3>Methods<a class="headerlink" href="#id30" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>forward(src, tgt, src_mask=None, tgt_mask=None):</dt><dd><p>Implements the forward pass of the Transformer model and returns the output and loss.</p>
</dd>
<dt>generate(src, idx, src_mask=None, max_new_tokens=128, temperature=1.0, top_k=None):</dt><dd><p>Generates new tokens given a source tensor.</p>
</dd>
<dt>configure_optimizers(weight_decay, learning_rate, betas, device_type, eps):</dt><dd><p>Configures the AdamW optimizer for the Transformer model.</p>
</dd>
<dt>save_model(path: str):</dt><dd><p>Saves the model state to the given file path.</p>
</dd>
<dt>load_model(path: str):</dt><dd><p>Loads the model state from the given file path.</p>
</dd>
</dl>
</section>
<section id="id31">
<h3>Parameters<a class="headerlink" href="#id31" title="Permalink to this heading">¶</a></h3>
<dl class="simple">
<dt>config<span class="classifier">object</span></dt><dd><dl class="simple">
<dt>A configuration object with necessary parameters for Transformer model. It includes:</dt><dd><p>vocab_size (int): The size of vocabulary.
block_size (int): The size of a block for Transformer.
PAD_IDX (int): The index representing padding in token sequence.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Transformer.Transformer.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">betas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Transformer.Transformer.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Configures the AdamW optimizer for the Transformer model.</p>
<section id="id32">
<h4>Parameters<a class="headerlink" href="#id32" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>weight_decay<span class="classifier">float</span></dt><dd><p>The L2 penalty (regularization) coefficient.</p>
</dd>
<dt>learning_rate<span class="classifier">float</span></dt><dd><p>The learning rate for AdamW optimizer.</p>
</dd>
<dt>betas<span class="classifier">tuple(float, float)</span></dt><dd><p>Coefficients used for computing running averages of gradient and its square.</p>
</dd>
<dt>device_type<span class="classifier">str</span></dt><dd><p>The device type for the optimizer, either “cpu” or “cuda”.</p>
</dd>
<dt>eps<span class="classifier">float</span></dt><dd><p>A term added to the denominator to improve numerical stability.</p>
</dd>
</dl>
</section>
<section id="id33">
<h4>Returns<a class="headerlink" href="#id33" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>torch.optim.AdamW</dt><dd><p>The AdamW optimizer configured for the Transformer model.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Transformer.Transformer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Transformer.Transformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the forward pass of the Transformer model.</p>
<section id="id34">
<h4>Parameters<a class="headerlink" href="#id34" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>src<span class="classifier">torch.Tensor</span></dt><dd><p>The input tensor for the source sequence.</p>
</dd>
<dt>tgt<span class="classifier">torch.Tensor</span></dt><dd><p>The input tensor for the target sequence.</p>
</dd>
<dt>src_mask<span class="classifier">torch.Tensor, optional</span></dt><dd><p>The input tensor for source sequence masking.</p>
</dd>
<dt>tgt_mask<span class="classifier">torch.Tensor, optional</span></dt><dd><p>The input tensor for target sequence masking.</p>
</dd>
</dl>
</section>
<section id="id35">
<h4>Returns<a class="headerlink" href="#id35" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>torch.Tensor, torch.Tensor</dt><dd><p>The output tensor post-processed by the Transformer model and the calculated loss.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Transformer.Transformer.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_new_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Transformer.Transformer.generate" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates new tokens given a source tensor.</p>
<section id="id36">
<h4>Parameters<a class="headerlink" href="#id36" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>src<span class="classifier">torch.Tensor</span></dt><dd><p>The input tensor for the source sequence.</p>
</dd>
<dt>idx<span class="classifier">torch.Tensor</span></dt><dd><p>The input tensor with indices in the current context.</p>
</dd>
<dt>src_mask<span class="classifier">torch.Tensor, optional</span></dt><dd><p>The input tensor for source sequence masking.</p>
</dd>
<dt>max_new_tokens<span class="classifier">int, optional</span></dt><dd><p>The maximum number of new tokens to be generated.</p>
</dd>
<dt>temperature<span class="classifier">float, optional</span></dt><dd><p>The softmax temperature for controlling the randomness of predictions.</p>
</dd>
<dt>top_k<span class="classifier">int, optional</span></dt><dd><p>The number of highest probability vocabulary tokens to keep for next step prediction.</p>
</dd>
</dl>
</section>
<section id="id37">
<h4>Returns<a class="headerlink" href="#id37" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>torch.Tensor, dict</dt><dd><p>The tensor with new generated token indices and a dictionary with attentions.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Transformer.Transformer.load_model">
<span class="sig-name descname"><span class="pre">load_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Transformer.Transformer.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the model state from the given file path.</p>
<section id="id38">
<h4>Parameters<a class="headerlink" href="#id38" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>path<span class="classifier">str</span></dt><dd><p>The file path from where the model state is to be loaded.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_implementation.Transformer.Transformer.save_model">
<span class="sig-name descname"><span class="pre">save_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.Transformer.Transformer.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the model state to the given file path.</p>
<section id="id39">
<h4>Parameters<a class="headerlink" href="#id39" title="Permalink to this heading">¶</a></h4>
<dl class="simple">
<dt>path<span class="classifier">str</span></dt><dd><p>The file path where the model state is to be saved.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-transformer_implementation.TransformerConfig">
<span id="transformer-implementation-transformerconfig-module"></span><h2>transformer_implementation.TransformerConfig module<a class="headerlink" href="#module-transformer_implementation.TransformerConfig" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_implementation.TransformerConfig.</span></span><span class="sig-name descname"><span class="pre">TransformerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">BOS_IDX</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">EOS_IDX</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">PAD_IDX</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_data_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_accumulation_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_head</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_embd</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_iters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0006</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.95</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-09</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'float16'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'nccl'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Data class that stores the configuration for a Transformer model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Total size of the tokenizer vocabulary.</p></li>
<li><p><strong>BOS_IDX</strong> (<em>int</em><em>, </em><em>optional</em>) – Index of the BOS token, defaults to -1</p></li>
<li><p><strong>EOS_IDX</strong> (<em>int</em><em>, </em><em>optional</em>) – Index of the EOS token, defaults to -1</p></li>
<li><p><strong>PAD_IDX</strong> (<em>int</em><em>, </em><em>optional</em>) – Index of the PAD token, defaults to -1</p></li>
<li><p><strong>block_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of tokens in each sequence, defaults to 256</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of sequences in each batch, defaults to 12</p></li>
<li><p><strong>train_data_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Size of train data, defaults to 5000000</p></li>
<li><p><strong>grad_accumulation_steps</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of batch accumulates during training, defaults to 2</p></li>
<li><p><strong>n_layer</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of transformer encoder and decoder blocks (N), defaults to 2</p></li>
<li><p><strong>n_head</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of heads in each attention block, defaults to 4</p></li>
<li><p><strong>n_embd</strong> (<em>int</em><em>, </em><em>optional</em>) – Token embedding size, defaults to 256</p></li>
<li><p><strong>dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – Dropout rate to use in the Transformer model, defaults to 0.1</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – Indicates whether to use bias in Linears and LayerNorms, defaults to False</p></li>
<li><p><strong>max_epochs</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of training epochs, defaults to 100</p></li>
<li><p><strong>max_iters</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of training steps, defaults to 2000</p></li>
<li><p><strong>eval_iters</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of validation epochs, defaults to 20</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>optional</em>) – Learning rate for the model optimization, defaults to 6e-4</p></li>
<li><p><strong>beta1</strong> (<em>float</em><em>, </em><em>optional</em>) – Beta1 for the AdamW optimizer, defaults to 0.9</p></li>
<li><p><strong>beta2</strong> (<em>float</em><em>, </em><em>optional</em>) – Beta2 for the AdamW optimizer, defaults to 0.95</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – Weight decay for the AdamW optimizer, defaults to 1e-1</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Epsilon for the AdamW optimizer, defaults to 1e-9</p></li>
<li><p><strong>device</strong> (<em>str</em><em>, </em><em>optional</em>) – The device to run the model on, defaults to ‘cpu’. ‘cuda’ is used if a GPU is available.</p></li>
<li><p><strong>dtype</strong> (<em>str</em><em>, </em><em>optional</em>) – The data type for the model, defaults to ‘bfloat16’ if GPU is available and supports ‘bfloat16’, otherwise ‘float16’</p></li>
<li><p><strong>compile</strong> (<em>bool</em><em>, </em><em>optional</em>) – If set to True, use PyTorch 2.0 to compile the model to be faster, defaults to True</p></li>
<li><p><strong>backend</strong> (<em>str</em><em>, </em><em>optional</em>) – Backend for DDP settings, defaults to ‘nccl’</p></li>
<li><p><strong>ddp</strong> (<em>bool</em><em>, </em><em>optional</em>) – If set to True, this is a DDP run, defaults to the evaluation of the environment variable ‘RANK’ != -1</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.BOS_IDX">
<span class="sig-name descname"><span class="pre">BOS_IDX</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">-1</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.BOS_IDX" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.EOS_IDX">
<span class="sig-name descname"><span class="pre">EOS_IDX</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">-1</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.EOS_IDX" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.PAD_IDX">
<span class="sig-name descname"><span class="pre">PAD_IDX</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">-1</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.PAD_IDX" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.backend">
<span class="sig-name descname"><span class="pre">backend</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'nccl'</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.backend" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.batch_size">
<span class="sig-name descname"><span class="pre">batch_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">12</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.batch_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.beta1">
<span class="sig-name descname"><span class="pre">beta1</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.9</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.beta1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.beta2">
<span class="sig-name descname"><span class="pre">beta2</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.95</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.beta2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.block_size">
<span class="sig-name descname"><span class="pre">block_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">256</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.block_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.compile" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.ddp">
<span class="sig-name descname"><span class="pre">ddp</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.ddp" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.device">
<span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'cuda'</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.device_type">
<span class="sig-name descname"><span class="pre">device_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'cuda'</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.device_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.dropout">
<span class="sig-name descname"><span class="pre">dropout</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.1</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.dropout" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'float16'</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.eps">
<span class="sig-name descname"><span class="pre">eps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1e-09</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.eps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.eval_iters">
<span class="sig-name descname"><span class="pre">eval_iters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">20</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.eval_iters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.grad_accumulation_steps">
<span class="sig-name descname"><span class="pre">grad_accumulation_steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">40</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.grad_accumulation_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.learning_rate">
<span class="sig-name descname"><span class="pre">learning_rate</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.0006</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.max_epochs">
<span class="sig-name descname"><span class="pre">max_epochs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">100</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.max_epochs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.max_iters">
<span class="sig-name descname"><span class="pre">max_iters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">2000</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.max_iters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.n_embd">
<span class="sig-name descname"><span class="pre">n_embd</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">256</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.n_embd" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.n_head">
<span class="sig-name descname"><span class="pre">n_head</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">4</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.n_head" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.n_layer">
<span class="sig-name descname"><span class="pre">n_layer</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">2</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.n_layer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.train_data_size">
<span class="sig-name descname"><span class="pre">train_data_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">5000000</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.train_data_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.vocab_size">
<span class="sig-name descname"><span class="pre">vocab_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.vocab_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_implementation.TransformerConfig.TransformerConfig.weight_decay">
<span class="sig-name descname"><span class="pre">weight_decay</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.1</span></em><a class="headerlink" href="#transformer_implementation.TransformerConfig.TransformerConfig.weight_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-transformer_implementation">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-transformer_implementation" title="Permalink to this heading">¶</a></h2>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Transformer</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">transformer</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">transformer_implementation package</a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html">utils package</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="modules.html">transformer</a><ul>
      <li>Previous: <a href="modules.html" title="previous chapter">transformer</a></li>
      <li>Next: <a href="transformer_implementation.blocks.html" title="next chapter">transformer_implementation.blocks package</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, Thibaud Perrin.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.0.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="_sources/transformer_implementation.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>