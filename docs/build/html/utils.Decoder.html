<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Decoder Class &#8212; Transformer v1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Transformer Class" href="utils.Transformer.html" />
    <link rel="prev" title="Encoder Class" href="utils.Encoder.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="decoder-class">
<h1>Decoder Class<a class="headerlink" href="#decoder-class" title="Permalink to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="utils.Decoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">Decoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utils.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>This class implements the decoder part of the Transformer model.</p>
<p>The Decoder consists of several DecoderBlocks arranged in sequence. The input first goes through an embedding 
layer followed by a positional encoding layer. The output of this is then passed through each DecoderBlock in 
sequence.</p>
<dl class="simple">
<dt>Attributes:</dt><dd><ul class="simple">
<li><p>decoder (nn.ModuleDict): A dictionary of modules making up the transformer decoder.</p></li>
<li><p>lm_head (nn.Linear): The final linear layer mapping from the embedding dimension to the vocabulary size.</p></li>
<li><p>config (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Config</span></code>): The configuration object for the transformer model.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The weight of the embedding layer and the linear layer are shared.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>config (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Config</span></code>): The configuration object with attributes such as <cite>vocab_size</cite>, <cite>block_size</cite>, <cite>n_embd</cite>, <cite>dropout</cite>, <cite>n_layer</cite>, and <cite>bias</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="utils.Decoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enc_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utils.Decoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>idx (torch.Tensor): The input tensor to the forward pass.</p></li>
<li><p>enc_output (torch.Tensor): The output tensor from the encoder.</p></li>
<li><p>targets (torch.Tensor, optional): The target tensor against which the loss will be calculated.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>torch.Tensor: The output tensor (logits) of the model.</p></li>
<li><p>list: all layers of decoder attentions weights.</p></li>
<li><p>list: all layers cross attentions weights.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="utils.Decoder.get_num_params">
<span class="sig-name descname"><span class="pre">get_num_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">non_embedding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#utils.Decoder.get_num_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of parameters in the model.
For non-embedding count (default), the position embeddings get subtracted.
The token embeddings would too, except due to the parameter sharing these
params are actually used as weights in the final layer, so we include them.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p>non_embedding (bool): If True, excludes the position embeddings count from the total. Default is True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>int: The number of parameters in the model.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Transformer</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="utils.LayerNorm.html">LayerNorm Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.Tokenizer.html">Tokenizer Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.TransformerConfig.html">TransformerConfig Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.DataLoaderFactory.html">DataLoaderFactory Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.MultiHeadAttention.html">MultiHeadAttention Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.FeedForward.html">FeedForward Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.EncoderBlock.html">EncoderBlock Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.DecoderBlock.html">DecoderBlock Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.Encoder.html">Encoder Class</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Decoder Class</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#utils.Decoder"><code class="docutils literal notranslate"><span class="pre">Decoder</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.Transformer.html">Transformer Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.estimate_loss.html">estimate_loss Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.plot_losses.html">plot_losses Class</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="utils.Encoder.html" title="previous chapter">Encoder Class</a></li>
      <li>Next: <a href="utils.Transformer.html" title="next chapter">Transformer Class</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, Thibaud Perrin.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.0.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="_sources/utils.Decoder.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>