Search.setIndex({"docnames": ["index", "modules", "transformer_implementation", "transformer_implementation.blocks", "transformer_implementation.blocks.layers", "utils"], "filenames": ["index.rst", "modules.rst", "transformer_implementation.rst", "transformer_implementation.blocks.rst", "transformer_implementation.blocks.layers.rst", "utils.rst"], "titles": ["Welcome to Transformer\u2019s documentation!", "transformer", "transformer_implementation package", "transformer_implementation.blocks package", "transformer_implementation.blocks.layers package", "utils package"], "terms": {"index": 0, "modul": [0, 1], "search": [0, 2], "page": 0, "t": 4, "class": [2, 3, 4], "util": [0, 1], "ndim": 4, "int": [2, 4], "bia": [1, 2, 3, 4], "bool": [2, 4], "A": [2, 3, 4, 5], "layer": [2, 3], "normal": [3, 4], "option": [2, 4], "thi": [2, 4, 5], "implement": [2, 3, 4], "allow": [2, 4], "turn": 4, "off": 4, "term": 4, "which": [2, 4, 5], "i": [2, 3, 4], "directli": 4, "support": 4, "pytorch": [2, 4], "": [2, 3, 4], "function": [4, 5], "attribut": [2, 3, 4], "weight": [2, 3, 4], "learnabl": 4, "initi": 4, "an": [2, 4], "all": [2, 4], "ones": 4, "tensor": [2, 3, 4], "bias": 4, "zero": 4, "argument": 4, "constructor": 4, "true": [2, 4], "otherwis": 4, "set": [2, 4, 5], "none": [2, 4], "arg": [2, 3, 4, 5], "integ": 4, "dimens": [2, 4], "input": [2, 3, 4], "vector": 4, "boolean": 4, "add": [2, 4], "output": [2, 3, 4], "forward": [1, 2, 3, 4], "defin": [2, 3, 4, 5], "comput": [2, 3, 4], "perform": [2, 3, 4, 5], "everi": [2, 3, 4], "call": [2, 3, 4], "The": [2, 3, 4, 5], "return": [2, 3, 4, 5], "layernorm": [2, 3], "token": 1, "encod": [1, 3], "decod": [1, 3], "text": 2, "sequenc": [2, 4], "sequenceclearn": [], "method": [2, 5], "remov": 2, "bo": 2, "pad": 2, "eo": 2, "special": 2, "from": [2, 3], "torch": [2, 3, 4], "list": [2, 4, 5], "clean": [], "sequencepad": [], "max_siz": 2, "512": 2, "ensur": 2, "length": 2, "within": 2, "maximum": 2, "size": 2, "union": [], "default": 2, "process": 2, "ad": 2, "limit": 2, "vocabs": [], "vocabulari": 2, "transformerconfig": 1, "ani": 2, "block_siz": [1, 2], "batch_siz": [1, 2, 4], "12": 2, "n_layer": [1, 2], "1": 2, "n_head": [1, 2, 4], "2": 2, "n_embd": [1, 2, 3, 4], "128": 2, "dropout": [1, 2, 4], "float": [2, 4], "0": 2, "fals": 2, "data": [2, 5], "store": 2, "configur": [2, 3, 4, 5], "transform": [3, 4, 5], "model": [2, 3, 4, 5], "instanc": 2, "number": [2, 4, 5], "each": [2, 3, 5], "batch": 2, "vocab_s": [1, 2], "total": 2, "It": [2, 5], "block": [1, 2], "n": 2, "head": [2, 3, 4], "attent": [2, 3, 4], "embed": [2, 4], "origin": 2, "paper": 2, "rate": [2, 4], "us": [2, 5], "indic": [2, 4], "whether": [2, 4], "linear": [2, 4], "If": 2, "similar": 2, "gpt": 2, "bit": 2, "better": 2, "faster": 2, "devic": [1, 2], "str": [2, 5], "run": 2, "cpu": 2, "cuda": 2, "gpu": 2, "avail": [2, 4], "learning_r": [1, 2], "learn": 2, "optim": 2, "3e": 2, "4": 2, "eval_interv": [1, 2], "step": 2, "between": [2, 4], "valid": [2, 5], "dataset": [2, 5], "eval_it": [1, 2], "epoch": [2, 5], "20": 2, "properti": 2, "dataloaderfactori": 1, "instanti": 2, "dataload": 2, "differ": 2, "split": [2, 5], "huggingfac": 2, "ha": 2, "depend": 2, "we": 2, "get_batch": [1, 2, 5], "choos": 2, "correct": 2, "yield": 2, "train": [2, 5], "val": [2, 5], "test": 2, "dict": [2, 5], "dictionari": [2, 5], "kei": [2, 4, 5], "target": 2, "translat": 2, "contain": [2, 5], "sequence_clearn": [], "sequence_pad": [1, 2], "translationdataset": [1, 2], "creat": 2, "librari": 2, "custom": 2, "tiktoken": 2, "multiheadattent": [2, 3], "config": [2, 3, 4, 5], "multi": [3, 4], "appli": 4, "mechan": [2, 3, 4], "doesn": 4, "mask": 4, "over": [3, 4, 5], "score": [2, 4], "dimension": 4, "q_attn": 4, "nn": [2, 4], "queri": 4, "project": 4, "k_attn": 4, "v_attn": 4, "valu": [4, 5], "c_proj": 4, "attn_dropout": 4, "resid_dropout": 4, "residu": [3, 4], "connect": [3, 4], "flash": 4, "flag": 4, "q_x": 4, "k_x": 4, "v_x": 4, "pass": [2, 3, 4], "shape": 4, "seq_length": 4, "emb_dim": 4, "y": 4, "after": 4, "scaled_dot_product_attent": [3, 4], "q": 4, "k": 4, "v": 4, "scale": 4, "dot": 4, "product": 4, "num_head": 4, "feedforward": [2, 3], "posit": [2, 3, 4], "wise": [3, 4], "feed": [3, 4], "neural": 4, "network": [3, 4], "ffnn": 4, "consist": [2, 3, 4], "two": [3, 4], "gelu": 4, "activ": 4, "follow": [2, 3, 4], "regular": 4, "c_fc": 4, "first": [2, 3, 4], "fulli": [3, 4], "second": [3, 4], "object": [2, 3, 4, 5], "x": [3, 4], "encoderblock": [1, 2], "singl": 3, "sub": 3, "self": 3, "There": 3, "around": 3, "ln_1": 3, "befor": 3, "attn": 3, "ln_2": 3, "ffw": 3, "decoderblock": [1, 2], "three": 3, "attn1": 3, "attn2": 3, "attend": 3, "ln_3": 3, "encoder_output": 3, "last": 3, "part": 2, "sever": 2, "arrang": 2, "goe": 2, "through": 2, "moduledict": 2, "make": 2, "up": 2, "idx": 2, "against": 2, "loss": [2, 5], "calcul": [2, 5], "logit": 2, "were": [], "provid": 5, "get_num_param": [1, 2], "non_embed": 2, "paramet": 2, "For": 2, "non": 2, "count": 2, "get": 2, "subtract": 2, "would": 2, "too": 2, "except": 2, "due": 2, "share": 2, "param": 2, "ar": 2, "actual": 2, "final": 2, "so": 2, "includ": 2, "them": 2, "exclud": 2, "lm_head": 2, "map": 2, "enc_output": 2, "gener": 2, "max_new_token": [], "temperatur": [], "top_k": [], "take": [], "condit": [], "longtensor": [], "b": [], "complet": [], "time": [], "predict": [], "back": [], "most": [], "like": [], "you": [], "ll": [], "want": 2, "sure": [], "eval": [], "mode": 5, "oper": [], "both": 2, "transduct": 2, "primarili": 2, "task": 2, "requir": 2, "understand": 2, "context": 2, "relationship": 2, "among": 2, "word": 2, "src": 2, "tgt": 2, "basi": 2, "estimate_loss": 1, "plot_loss": 1, "cross": [2, 3], "encoder_attn": 3, "current": [2, 3], "cross_attn": 3, "decoder_attn": 3, "attn_weight": 4, "useful": 4, "visual": [1, 2, 4], "how": 4, "work": 4, "sequence_clean": [1, 2], "where": 2, "alloc": 2, "separ": 2, "convert": 2, "string": 2, "format": 2, "load_model": [1, 2], "path": 2, "load": 2, "state": 2, "file": 2, "should": [2, 5], "rais": 2, "valueerror": 2, "specifi": [2, 5], "doe": 2, "exist": 2, "save_model": [1, 2], "save": 2, "translate_beam_search": [1, 2], "beam_siz": 2, "5": 2, "sourc": 2, "beam": 2, "tupl": 2, "best": 2, "found": 2, "256": 2, "max_it": [1, 2], "2000": 2, "200": 2, "estim": 5, "without": 5, "backpropag": 5, "evalu": 5, "iter": 5, "averag": 5, "need": 5, "customdataset": 5, "name": 5, "out": 5, "correspond": 5, "plot": 5, "more": 5, "vibrant": 5, "color": 5, "black": 5, "background": 5, "record": 5, "build": [], "base": [2, 3, 4], "tokenize_from_str": [1, 2], "0003": 2, "packag": [0, 1], "subpackag": 1, "submodul": 1, "content": 1, "transformer_implement": [0, 1]}, "objects": {"": [[2, 0, 0, "-", "transformer_implementation"], [5, 0, 0, "-", "utils"]], "transformer_implementation": [[2, 0, 0, "-", "DataLoaderFactory"], [2, 0, 0, "-", "Decoder"], [2, 0, 0, "-", "Encoder"], [2, 0, 0, "-", "Tokenizer"], [2, 0, 0, "-", "Transformer"], [2, 0, 0, "-", "TransformerConfig"], [3, 0, 0, "-", "blocks"]], "transformer_implementation.DataLoaderFactory": [[2, 1, 1, "", "DataLoaderFactory"], [2, 1, 1, "", "TranslationDataset"]], "transformer_implementation.DataLoaderFactory.DataLoaderFactory": [[2, 2, 1, "", "get_batch"]], "transformer_implementation.Decoder": [[2, 1, 1, "", "Decoder"]], "transformer_implementation.Decoder.Decoder": [[2, 2, 1, "", "forward"], [2, 2, 1, "", "get_num_params"]], "transformer_implementation.Encoder": [[2, 1, 1, "", "Encoder"]], "transformer_implementation.Encoder.Encoder": [[2, 2, 1, "", "forward"], [2, 2, 1, "", "get_num_params"]], "transformer_implementation.Tokenizer": [[2, 1, 1, "", "Tokenizer"]], "transformer_implementation.Tokenizer.Tokenizer": [[2, 2, 1, "", "sequence_cleaner"], [2, 2, 1, "", "sequence_padding"], [2, 2, 1, "", "tokenize"], [2, 2, 1, "", "tokenize_from_str"], [2, 2, 1, "", "vocab_size"]], "transformer_implementation.Transformer": [[2, 1, 1, "", "Transformer"]], "transformer_implementation.Transformer.Transformer": [[2, 2, 1, "", "forward"], [2, 2, 1, "", "load_model"], [2, 2, 1, "", "save_model"], [2, 2, 1, "", "translate_beam_search"]], "transformer_implementation.TransformerConfig": [[2, 1, 1, "", "TransformerConfig"]], "transformer_implementation.TransformerConfig.TransformerConfig": [[2, 3, 1, "", "batch_size"], [2, 3, 1, "", "bias"], [2, 3, 1, "", "block_size"], [2, 3, 1, "", "device"], [2, 3, 1, "", "dropout"], [2, 3, 1, "", "eval_interval"], [2, 3, 1, "", "eval_iters"], [2, 3, 1, "", "learning_rate"], [2, 3, 1, "", "max_iters"], [2, 3, 1, "", "n_embd"], [2, 3, 1, "", "n_head"], [2, 3, 1, "", "n_layer"], [2, 3, 1, "", "tokenizer"], [2, 3, 1, "", "visualize"], [2, 4, 1, "", "vocab_size"]], "transformer_implementation.blocks": [[3, 0, 0, "-", "DecoderBlock"], [3, 0, 0, "-", "EncoderBlock"], [4, 0, 0, "-", "layers"]], "transformer_implementation.blocks.DecoderBlock": [[3, 1, 1, "", "DecoderBlock"]], "transformer_implementation.blocks.DecoderBlock.DecoderBlock": [[3, 2, 1, "", "forward"]], "transformer_implementation.blocks.EncoderBlock": [[3, 1, 1, "", "EncoderBlock"]], "transformer_implementation.blocks.EncoderBlock.EncoderBlock": [[3, 2, 1, "", "forward"]], "transformer_implementation.blocks.layers": [[4, 0, 0, "-", "FeedForward"], [4, 0, 0, "-", "LayerNorm"], [4, 0, 0, "-", "MultiHeadAttention"]], "transformer_implementation.blocks.layers.FeedForward": [[4, 1, 1, "", "FeedForward"]], "transformer_implementation.blocks.layers.FeedForward.FeedForward": [[4, 2, 1, "", "forward"]], "transformer_implementation.blocks.layers.LayerNorm": [[4, 1, 1, "", "LayerNorm"]], "transformer_implementation.blocks.layers.LayerNorm.LayerNorm": [[4, 2, 1, "", "forward"]], "transformer_implementation.blocks.layers.MultiHeadAttention": [[4, 1, 1, "", "MultiHeadAttention"]], "transformer_implementation.blocks.layers.MultiHeadAttention.MultiHeadAttention": [[4, 2, 1, "", "forward"], [4, 2, 1, "", "scaled_dot_product_attention"]], "utils": [[5, 0, 0, "-", "estimate_loss"], [5, 0, 0, "-", "plot_losses"]], "utils.estimate_loss": [[5, 5, 1, "", "estimate_loss"]], "utils.plot_losses": [[5, 5, 1, "", "plot_losses"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:property", "5": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "property", "Python property"], "5": ["py", "function", "Python function"]}, "titleterms": {"welcom": 0, "transform": [0, 1, 2], "": 0, "document": 0, "indic": 0, "tabl": 0, "layernorm": 4, "modul": [2, 3, 4, 5], "class": [], "token": 2, "content": [0, 2, 3, 4, 5], "transformerconfig": 2, "dataloaderfactori": 2, "multiheadattent": 4, "feedforward": 4, "encoderblock": 3, "decoderblock": 3, "encod": 2, "decod": 2, "estimate_loss": 5, "plot_loss": 5, "block": [3, 4], "layer": 4, "util": 5, "packag": [2, 3, 4, 5], "subpackag": [2, 3], "submodul": [2, 3, 4, 5], "transformer_implement": [2, 3, 4]}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 57}, "alltitles": {"Welcome to Transformer\u2019s documentation!": [[0, "welcome-to-transformer-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "transformer": [[1, "transformer"]], "transformer_implementation package": [[2, "transformer-implementation-package"]], "Subpackages": [[2, "subpackages"], [3, "subpackages"]], "Submodules": [[2, "submodules"], [3, "submodules"], [4, "submodules"], [5, "submodules"]], "transformer_implementation.DataLoaderFactory module": [[2, "module-transformer_implementation.DataLoaderFactory"]], "transformer_implementation.Decoder module": [[2, "module-transformer_implementation.Decoder"]], "transformer_implementation.Encoder module": [[2, "module-transformer_implementation.Encoder"]], "transformer_implementation.Tokenizer module": [[2, "module-transformer_implementation.Tokenizer"]], "transformer_implementation.Transformer module": [[2, "module-transformer_implementation.Transformer"]], "transformer_implementation.TransformerConfig module": [[2, "module-transformer_implementation.TransformerConfig"]], "Module contents": [[2, "module-transformer_implementation"], [3, "module-transformer_implementation.blocks"], [4, "module-transformer_implementation.blocks.layers"], [5, "module-utils"]], "transformer_implementation.blocks package": [[3, "transformer-implementation-blocks-package"]], "transformer_implementation.blocks.DecoderBlock module": [[3, "module-transformer_implementation.blocks.DecoderBlock"]], "transformer_implementation.blocks.EncoderBlock module": [[3, "module-transformer_implementation.blocks.EncoderBlock"]], "transformer_implementation.blocks.layers package": [[4, "transformer-implementation-blocks-layers-package"]], "transformer_implementation.blocks.layers.FeedForward module": [[4, "module-transformer_implementation.blocks.layers.FeedForward"]], "transformer_implementation.blocks.layers.LayerNorm module": [[4, "module-transformer_implementation.blocks.layers.LayerNorm"]], "transformer_implementation.blocks.layers.MultiHeadAttention module": [[4, "module-transformer_implementation.blocks.layers.MultiHeadAttention"]], "utils package": [[5, "utils-package"]], "utils.estimate_loss module": [[5, "module-utils.estimate_loss"]], "utils.plot_losses module": [[5, "module-utils.plot_losses"]]}, "indexentries": {"dataloaderfactory (class in transformer_implementation.dataloaderfactory)": [[2, "transformer_implementation.DataLoaderFactory.DataLoaderFactory"]], "decoder (class in transformer_implementation.decoder)": [[2, "transformer_implementation.Decoder.Decoder"]], "encoder (class in transformer_implementation.encoder)": [[2, "transformer_implementation.Encoder.Encoder"]], "tokenizer (class in transformer_implementation.tokenizer)": [[2, "transformer_implementation.Tokenizer.Tokenizer"]], "transformer (class in transformer_implementation.transformer)": [[2, "transformer_implementation.Transformer.Transformer"]], "transformerconfig (class in transformer_implementation.transformerconfig)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig"]], "translationdataset (class in transformer_implementation.dataloaderfactory)": [[2, "transformer_implementation.DataLoaderFactory.TranslationDataset"]], "batch_size (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.batch_size"]], "bias (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.bias"]], "block_size (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.block_size"]], "device (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.device"]], "dropout (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.dropout"]], "eval_interval (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.eval_interval"]], "eval_iters (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.eval_iters"]], "forward() (transformer_implementation.decoder.decoder method)": [[2, "transformer_implementation.Decoder.Decoder.forward"]], "forward() (transformer_implementation.encoder.encoder method)": [[2, "transformer_implementation.Encoder.Encoder.forward"]], "forward() (transformer_implementation.transformer.transformer method)": [[2, "transformer_implementation.Transformer.Transformer.forward"]], "get_batch() (transformer_implementation.dataloaderfactory.dataloaderfactory method)": [[2, "transformer_implementation.DataLoaderFactory.DataLoaderFactory.get_batch"]], "get_num_params() (transformer_implementation.decoder.decoder method)": [[2, "transformer_implementation.Decoder.Decoder.get_num_params"]], "get_num_params() (transformer_implementation.encoder.encoder method)": [[2, "transformer_implementation.Encoder.Encoder.get_num_params"]], "learning_rate (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.learning_rate"]], "load_model() (transformer_implementation.transformer.transformer method)": [[2, "transformer_implementation.Transformer.Transformer.load_model"]], "max_iters (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.max_iters"]], "module": [[2, "module-transformer_implementation"], [2, "module-transformer_implementation.DataLoaderFactory"], [2, "module-transformer_implementation.Decoder"], [2, "module-transformer_implementation.Encoder"], [2, "module-transformer_implementation.Tokenizer"], [2, "module-transformer_implementation.Transformer"], [2, "module-transformer_implementation.TransformerConfig"], [3, "module-transformer_implementation.blocks"], [3, "module-transformer_implementation.blocks.DecoderBlock"], [3, "module-transformer_implementation.blocks.EncoderBlock"], [4, "module-transformer_implementation.blocks.layers"], [4, "module-transformer_implementation.blocks.layers.FeedForward"], [4, "module-transformer_implementation.blocks.layers.LayerNorm"], [4, "module-transformer_implementation.blocks.layers.MultiHeadAttention"], [5, "module-utils"], [5, "module-utils.estimate_loss"], [5, "module-utils.plot_losses"]], "n_embd (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.n_embd"]], "n_head (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.n_head"]], "n_layer (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.n_layer"]], "save_model() (transformer_implementation.transformer.transformer method)": [[2, "transformer_implementation.Transformer.Transformer.save_model"]], "sequence_cleaner() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.sequence_cleaner"]], "sequence_padding() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.sequence_padding"]], "tokenize() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.tokenize"]], "tokenize_from_str() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.tokenize_from_str"]], "tokenizer (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.tokenizer"]], "transformer_implementation": [[2, "module-transformer_implementation"]], "transformer_implementation.dataloaderfactory": [[2, "module-transformer_implementation.DataLoaderFactory"]], "transformer_implementation.decoder": [[2, "module-transformer_implementation.Decoder"]], "transformer_implementation.encoder": [[2, "module-transformer_implementation.Encoder"]], "transformer_implementation.tokenizer": [[2, "module-transformer_implementation.Tokenizer"]], "transformer_implementation.transformer": [[2, "module-transformer_implementation.Transformer"]], "transformer_implementation.transformerconfig": [[2, "module-transformer_implementation.TransformerConfig"]], "translate_beam_search() (transformer_implementation.transformer.transformer method)": [[2, "transformer_implementation.Transformer.Transformer.translate_beam_search"]], "visualize (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.visualize"]], "vocab_size (transformer_implementation.transformerconfig.transformerconfig property)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.vocab_size"]], "vocab_size() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.vocab_size"]], "decoderblock (class in transformer_implementation.blocks.decoderblock)": [[3, "transformer_implementation.blocks.DecoderBlock.DecoderBlock"]], "encoderblock (class in transformer_implementation.blocks.encoderblock)": [[3, "transformer_implementation.blocks.EncoderBlock.EncoderBlock"]], "forward() (transformer_implementation.blocks.decoderblock.decoderblock method)": [[3, "transformer_implementation.blocks.DecoderBlock.DecoderBlock.forward"]], "forward() (transformer_implementation.blocks.encoderblock.encoderblock method)": [[3, "transformer_implementation.blocks.EncoderBlock.EncoderBlock.forward"]], "transformer_implementation.blocks": [[3, "module-transformer_implementation.blocks"]], "transformer_implementation.blocks.decoderblock": [[3, "module-transformer_implementation.blocks.DecoderBlock"]], "transformer_implementation.blocks.encoderblock": [[3, "module-transformer_implementation.blocks.EncoderBlock"]], "feedforward (class in transformer_implementation.blocks.layers.feedforward)": [[4, "transformer_implementation.blocks.layers.FeedForward.FeedForward"]], "layernorm (class in transformer_implementation.blocks.layers.layernorm)": [[4, "transformer_implementation.blocks.layers.LayerNorm.LayerNorm"]], "multiheadattention (class in transformer_implementation.blocks.layers.multiheadattention)": [[4, "transformer_implementation.blocks.layers.MultiHeadAttention.MultiHeadAttention"]], "forward() (transformer_implementation.blocks.layers.feedforward.feedforward method)": [[4, "transformer_implementation.blocks.layers.FeedForward.FeedForward.forward"]], "forward() (transformer_implementation.blocks.layers.layernorm.layernorm method)": [[4, "transformer_implementation.blocks.layers.LayerNorm.LayerNorm.forward"]], "forward() (transformer_implementation.blocks.layers.multiheadattention.multiheadattention method)": [[4, "transformer_implementation.blocks.layers.MultiHeadAttention.MultiHeadAttention.forward"]], "scaled_dot_product_attention() (transformer_implementation.blocks.layers.multiheadattention.multiheadattention method)": [[4, "transformer_implementation.blocks.layers.MultiHeadAttention.MultiHeadAttention.scaled_dot_product_attention"]], "transformer_implementation.blocks.layers": [[4, "module-transformer_implementation.blocks.layers"]], "transformer_implementation.blocks.layers.feedforward": [[4, "module-transformer_implementation.blocks.layers.FeedForward"]], "transformer_implementation.blocks.layers.layernorm": [[4, "module-transformer_implementation.blocks.layers.LayerNorm"]], "transformer_implementation.blocks.layers.multiheadattention": [[4, "module-transformer_implementation.blocks.layers.MultiHeadAttention"]], "estimate_loss() (in module utils.estimate_loss)": [[5, "utils.estimate_loss.estimate_loss"]], "plot_losses() (in module utils.plot_losses)": [[5, "utils.plot_losses.plot_losses"]], "utils": [[5, "module-utils"]], "utils.estimate_loss": [[5, "module-utils.estimate_loss"]], "utils.plot_losses": [[5, "module-utils.plot_losses"]]}})