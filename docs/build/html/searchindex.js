Search.setIndex({"docnames": ["index", "modules", "transformer_implementation", "transformer_implementation.blocks", "transformer_implementation.blocks.layers", "utils"], "filenames": ["index.rst", "modules.rst", "transformer_implementation.rst", "transformer_implementation.blocks.rst", "transformer_implementation.blocks.layers.rst", "utils.rst"], "titles": ["Welcome to Transformer\u2019s documentation!", "transformer", "transformer_implementation package", "transformer_implementation.blocks package", "transformer_implementation.blocks.layers package", "utils package"], "terms": {"index": [0, 2], "modul": [0, 1], "search": 0, "page": 0, "t": 2, "class": [2, 3, 4], "util": [0, 1], "ndim": 4, "int": [2, 3, 4], "bia": [1, 2, 3, 4], "bool": [2, 3, 4], "A": [2, 3, 4, 5], "layer": [2, 3], "normal": [2, 3, 4], "option": [2, 3, 4], "thi": [2, 3, 4, 5], "implement": [2, 3, 4], "allow": [2, 4], "turn": 4, "off": 4, "term": [2, 3, 4], "which": [2, 4, 5], "i": [2, 3, 4], "directli": 4, "support": [2, 4], "pytorch": [2, 3, 4], "": [2, 3, 4, 5], "function": [4, 5], "attribut": [], "weight": [2, 4], "learnabl": 4, "initi": [2, 4], "an": [2, 3], "all": [], "ones": [], "tensor": [2, 3, 4], "bias": [], "zero": 4, "argument": [], "constructor": [], "true": [2, 3, 4], "otherwis": 2, "set": [2, 5], "none": [2, 3, 4, 5], "arg": [2, 5], "integ": [], "dimens": [2, 4], "input": [2, 3, 4], "vector": [3, 4], "boolean": [], "add": [2, 3, 4], "output": [2, 3, 4], "forward": [1, 2, 3, 4], "defin": [2, 4, 5], "comput": [2, 3, 4], "perform": [2, 5], "everi": 2, "call": 2, "The": [2, 3, 4, 5], "return": 5, "layernorm": [2, 3], "token": 1, "encod": [1, 3], "decod": [1, 3], "text": 2, "sequenc": 2, "sequenceclearn": [], "method": 5, "remov": 2, "bo": 2, "pad": 2, "eo": 2, "special": 2, "from": [2, 3], "torch": [2, 3, 4, 5], "list": [2, 5], "clean": [], "sequencepad": [], "max_siz": 2, "512": 2, "ensur": [2, 4], "length": 2, "within": 2, "maximum": 2, "size": [2, 3, 4], "union": [], "default": [2, 3, 4], "process": [2, 4], "ad": [2, 3], "limit": 2, "vocabs": [], "vocabulari": 2, "transformerconfig": 1, "ani": [], "block_siz": [1, 2, 4], "batch_siz": [1, 2], "12": 2, "n_layer": [1, 2], "1": 2, "n_head": [1, 2, 4], "2": 2, "n_embd": [1, 2, 3, 4], "128": 2, "dropout": [1, 2, 4], "float": [2, 4], "0": 2, "fals": [2, 4], "data": [2, 5], "store": 2, "configur": [2, 3, 4, 5], "transform": [3, 4, 5], "model": [2, 3, 4, 5], "instanc": [], "number": [2, 4, 5], "each": [2, 3, 5], "batch": 2, "vocab_s": [1, 2], "total": 2, "It": [2, 3, 4, 5], "block": [1, 2], "n": 2, "head": [2, 3, 4], "attent": [2, 3, 4], "embed": [2, 3], "origin": 2, "paper": [], "rate": [2, 4], "us": [2, 3, 4, 5], "indic": 2, "whether": 2, "linear": [2, 4], "If": [2, 3, 4], "similar": [], "gpt": [], "bit": [], "better": [], "faster": 2, "devic": [1, 2], "str": [2, 5], "run": 2, "cpu": 2, "cuda": 2, "gpu": 2, "avail": 2, "learning_r": [1, 2], "learn": 2, "optim": [2, 5], "3e": [], "4": [2, 4], "eval_interv": [], "step": [2, 5], "between": 4, "valid": [2, 5], "dataset": [2, 5], "eval_it": [1, 2], "epoch": [2, 5], "20": 2, "properti": [], "dataloaderfactori": 1, "instanti": 2, "dataload": 2, "differ": 2, "split": [2, 5], "huggingfac": 2, "ha": [2, 4], "depend": 2, "we": [2, 4], "get_batch": [1, 2, 5], "choos": 2, "correct": 2, "yield": 2, "train": [2, 5], "val": [2, 5], "test": 2, "dict": [2, 5], "dictionari": [2, 5], "kei": [2, 4, 5], "target": [2, 3], "translat": 2, "contain": [2, 5], "sequence_clearn": [], "sequence_pad": [1, 2], "translationdataset": [1, 2], "creat": 2, "librari": 2, "custom": 2, "tiktoken": 2, "multiheadattent": [2, 3], "config": [2, 3, 4, 5], "multi": [2, 3, 4], "appli": [3, 4], "mechan": [2, 3, 4], "doesn": [], "mask": [2, 3, 4], "over": 5, "score": 2, "dimension": [], "q_attn": 4, "nn": [2, 3, 4], "queri": 4, "project": 4, "k_attn": 4, "v_attn": 4, "valu": [4, 5], "c_proj": 4, "attn_dropout": 4, "resid_dropout": 4, "residu": [], "connect": 4, "flash": [], "flag": [], "q_x": 4, "k_x": 4, "v_x": 4, "pass": [2, 3, 4], "shape": 4, "seq_length": [], "emb_dim": [], "y": [], "after": 2, "scaled_dot_product_attent": [3, 4], "q": 4, "k": 4, "v": 4, "scale": 4, "dot": 4, "product": 4, "num_head": [], "feedforward": [2, 3], "posit": [2, 4], "wise": 4, "feed": [3, 4], "neural": [3, 4], "network": [3, 4], "ffnn": 4, "consist": [3, 4], "two": [3, 4], "gelu": 4, "activ": 4, "follow": [2, 3, 4], "regular": 4, "c_fc": 4, "first": [3, 4], "fulli": 4, "second": 4, "object": [2, 3, 4, 5], "x": [3, 4], "encoderblock": [1, 2], "singl": [], "sub": [], "self": 3, "There": [], "around": [], "ln_1": 3, "befor": 3, "attn": 3, "ln_2": 3, "ffw": 3, "decoderblock": [1, 2], "three": [], "attn1": 3, "attn2": 3, "attend": [], "ln_3": 3, "encoder_output": 3, "last": [], "part": 3, "sever": 2, "arrang": [], "goe": [], "through": 3, "moduledict": 2, "make": [], "up": 5, "idx": 2, "against": [], "loss": [2, 5], "calcul": [2, 5], "logit": 2, "were": [], "provid": [2, 3, 5], "get_num_param": [1, 2], "non_embed": 2, "paramet": 5, "For": 2, "non": 2, "count": 2, "get": 2, "subtract": 2, "would": 2, "too": 2, "except": 2, "due": 2, "share": 2, "param": 2, "ar": 2, "actual": 2, "final": [2, 3], "so": 2, "includ": [2, 3, 4], "them": 2, "exclud": [], "lm_head": 2, "map": [], "enc_output": 2, "gener": [1, 2], "max_new_token": 2, "temperatur": 2, "top_k": 2, "take": 4, "condit": [], "longtensor": [], "b": 2, "complet": [], "time": [], "predict": [], "back": [], "most": [], "like": [2, 5], "you": [], "ll": [], "want": [], "sure": [], "eval": [], "mode": 5, "oper": [], "both": 2, "transduct": 2, "primarili": 2, "task": 2, "requir": 2, "understand": 2, "context": 2, "relationship": 2, "among": 2, "word": 2, "src": 2, "tgt": 2, "basi": 2, "estimate_loss": 1, "plot_loss": 1, "cross": [2, 3], "encoder_attn": [], "current": 2, "cross_attn": [], "decoder_attn": [], "attn_weight": [], "useful": [], "visual": [], "how": [], "work": [], "sequence_clean": [1, 2], "where": 2, "alloc": 2, "separ": 2, "convert": 2, "string": 2, "format": 2, "load_model": [1, 2], "path": 2, "load": 2, "state": 2, "file": 2, "should": [2, 5], "rais": 2, "valueerror": 2, "specifi": [2, 4, 5], "doe": 2, "exist": 2, "save_model": [1, 2], "save": 2, "translate_beam_search": [], "beam_siz": [], "5": [], "sourc": [2, 3], "beam": [], "tupl": [2, 3, 4], "best": [], "found": [], "256": 2, "max_it": [1, 2, 5], "2000": 2, "200": [], "estim": 5, "without": 5, "backpropag": 5, "evalu": [2, 5], "iter": 5, "averag": 5, "need": 5, "customdataset": 5, "name": 5, "out": 5, "correspond": 5, "plot": 5, "more": 5, "vibrant": 5, "color": 5, "black": 5, "background": 5, "record": 5, "build": [], "base": [2, 3, 4], "tokenize_from_str": [1, 2], "0003": [], "packag": [0, 1], "subpackag": 1, "submodul": 1, "content": 1, "transformer_implement": [0, 1], "generate_padding_mask": [1, 2], "configure_optim": [1, 2], "bos_idx": [1, 2], "eos_idx": [1, 2], "pad_idx": [1, 2], "backend": [1, 2], "beta1": [1, 2], "beta2": [1, 2], "compil": [1, 2], "ddp": [1, 2], "device_typ": [1, 2], "dtype": [1, 2], "ep": [1, 2], "grad_accumulation_step": [1, 2], "max_epoch": [1, 2], "train_data_s": [1, 2], "weight_decai": [1, 2], "training_loop": 1, "get_linear_schedule_with_warmup": [1, 5], "train_dataset_s": 2, "5000000": 2, "child": [2, 3, 4], "core": 2, "produc": 2, "necessari": 2, "compon": [2, 3], "_init_weight": 2, "src_mask": [2, 3], "tgt_mask": [2, 3], "inherit": 2, "multipl": 2, "wte": 2, "wpe": 2, "drop": 2, "h": 2, "modulelist": 2, "ln_f": 2, "have": [2, 4], "same": [2, 4], "matric": [2, 3], "seq": 2, "triu": 2, "beta": 2, "input_mask": 2, "target_mask": 2, "40": 2, "100": 2, "0006": 2, "9": 2, "95": 2, "1e": 2, "09": 2, "float16": 2, "nccl": 2, "accumul": 2, "dure": 2, "6e": 2, "adamw": 2, "decai": [2, 5], "epsilon": 2, "type": 2, "bfloat16": 2, "environ": 2, "variabl": 2, "rank": 2, "architectur": 3, "while": 3, "featur": [3, 4], "previou": 3, "main": 3, "anoth": 3, "its": 3, "gaussian": 4, "error": 4, "unit": 4, "post": 4, "factor": 4, "one": 4, "els": 4, "causal": 4, "is_mask": 4, "mha": 4, "do": 4, "triangular": 4, "ctx": 5, "xlim": 5, "ylim": 5, "num_warmup_step": 5, "num_training_step": 5, "warm": 5, "strategi": 5, "scaler": 5, "saved_path": 5, "transformer_state_dict": 5, "pth": 5, "loop": 5, "given": 5, "accord": 5, "updat": 5, "losses_list": 5, "per": 5}, "objects": {"": [[2, 0, 0, "-", "transformer_implementation"], [5, 0, 0, "-", "utils"]], "transformer_implementation": [[2, 0, 0, "-", "DataLoaderFactory"], [2, 0, 0, "-", "Decoder"], [2, 0, 0, "-", "Encoder"], [2, 0, 0, "-", "Tokenizer"], [2, 0, 0, "-", "Transformer"], [2, 0, 0, "-", "TransformerConfig"], [3, 0, 0, "-", "blocks"]], "transformer_implementation.DataLoaderFactory": [[2, 1, 1, "", "DataLoaderFactory"], [2, 1, 1, "", "TranslationDataset"]], "transformer_implementation.DataLoaderFactory.DataLoaderFactory": [[2, 2, 1, "", "get_batch"]], "transformer_implementation.Decoder": [[2, 1, 1, "", "Decoder"]], "transformer_implementation.Decoder.Decoder": [[2, 2, 1, "", "forward"], [2, 2, 1, "", "get_num_params"]], "transformer_implementation.Encoder": [[2, 1, 1, "", "Encoder"]], "transformer_implementation.Encoder.Encoder": [[2, 2, 1, "", "forward"], [2, 2, 1, "", "get_num_params"]], "transformer_implementation.Tokenizer": [[2, 1, 1, "", "Tokenizer"]], "transformer_implementation.Tokenizer.Tokenizer": [[2, 2, 1, "", "generate_padding_mask"], [2, 2, 1, "", "sequence_cleaner"], [2, 2, 1, "", "sequence_padding"], [2, 2, 1, "", "tokenize"], [2, 2, 1, "", "tokenize_from_str"], [2, 2, 1, "", "vocab_size"]], "transformer_implementation.Transformer": [[2, 1, 1, "", "Transformer"]], "transformer_implementation.Transformer.Transformer": [[2, 2, 1, "", "configure_optimizers"], [2, 2, 1, "", "forward"], [2, 2, 1, "", "generate"], [2, 2, 1, "", "load_model"], [2, 2, 1, "", "save_model"]], "transformer_implementation.TransformerConfig": [[2, 1, 1, "", "TransformerConfig"]], "transformer_implementation.TransformerConfig.TransformerConfig": [[2, 3, 1, "", "BOS_IDX"], [2, 3, 1, "", "EOS_IDX"], [2, 3, 1, "", "PAD_IDX"], [2, 3, 1, "", "backend"], [2, 3, 1, "", "batch_size"], [2, 3, 1, "", "beta1"], [2, 3, 1, "", "beta2"], [2, 3, 1, "", "bias"], [2, 3, 1, "", "block_size"], [2, 3, 1, "", "compile"], [2, 3, 1, "", "ddp"], [2, 3, 1, "", "device"], [2, 3, 1, "", "device_type"], [2, 3, 1, "", "dropout"], [2, 3, 1, "", "dtype"], [2, 3, 1, "", "eps"], [2, 3, 1, "", "eval_iters"], [2, 3, 1, "", "grad_accumulation_steps"], [2, 3, 1, "", "learning_rate"], [2, 3, 1, "", "max_epochs"], [2, 3, 1, "", "max_iters"], [2, 3, 1, "", "n_embd"], [2, 3, 1, "", "n_head"], [2, 3, 1, "", "n_layer"], [2, 3, 1, "", "train_data_size"], [2, 3, 1, "", "vocab_size"], [2, 3, 1, "", "weight_decay"]], "transformer_implementation.blocks": [[3, 0, 0, "-", "DecoderBlock"], [3, 0, 0, "-", "EncoderBlock"], [4, 0, 0, "-", "layers"]], "transformer_implementation.blocks.DecoderBlock": [[3, 1, 1, "", "DecoderBlock"]], "transformer_implementation.blocks.DecoderBlock.DecoderBlock": [[3, 2, 1, "", "forward"]], "transformer_implementation.blocks.EncoderBlock": [[3, 1, 1, "", "EncoderBlock"]], "transformer_implementation.blocks.EncoderBlock.EncoderBlock": [[3, 2, 1, "", "forward"]], "transformer_implementation.blocks.layers": [[4, 0, 0, "-", "FeedForward"], [4, 0, 0, "-", "LayerNorm"], [4, 0, 0, "-", "MultiHeadAttention"]], "transformer_implementation.blocks.layers.FeedForward": [[4, 1, 1, "", "FeedForward"]], "transformer_implementation.blocks.layers.FeedForward.FeedForward": [[4, 2, 1, "", "forward"]], "transformer_implementation.blocks.layers.LayerNorm": [[4, 1, 1, "", "LayerNorm"]], "transformer_implementation.blocks.layers.LayerNorm.LayerNorm": [[4, 2, 1, "", "forward"]], "transformer_implementation.blocks.layers.MultiHeadAttention": [[4, 1, 1, "", "MultiHeadAttention"]], "transformer_implementation.blocks.layers.MultiHeadAttention.MultiHeadAttention": [[4, 2, 1, "", "forward"], [4, 2, 1, "", "scaled_dot_product_attention"]], "utils": [[5, 0, 0, "-", "estimate_loss"], [5, 0, 0, "-", "plot_losses"], [5, 0, 0, "-", "training_loop"]], "utils.estimate_loss": [[5, 4, 1, "", "estimate_loss"]], "utils.plot_losses": [[5, 4, 1, "", "plot_losses"]], "utils.training_loop": [[5, 4, 1, "", "get_linear_schedule_with_warmup"], [5, 4, 1, "", "training_loop"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "function", "Python function"]}, "titleterms": {"welcom": 0, "transform": [0, 1, 2], "": 0, "document": 0, "indic": 0, "tabl": 0, "layernorm": 4, "modul": [2, 3, 4, 5], "class": [], "token": 2, "content": [0, 2, 3, 4, 5], "transformerconfig": 2, "dataloaderfactori": 2, "multiheadattent": 4, "feedforward": 4, "encoderblock": 3, "decoderblock": 3, "encod": 2, "decod": 2, "estimate_loss": 5, "plot_loss": 5, "block": [3, 4], "layer": 4, "util": 5, "packag": [2, 3, 4, 5], "subpackag": [2, 3], "submodul": [2, 3, 4, 5], "transformer_implement": [2, 3, 4], "attribut": [2, 3, 4], "method": [2, 3, 4], "paramet": [2, 3, 4], "return": [2, 3, 4], "training_loop": 5}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 57}, "alltitles": {"Welcome to Transformer\u2019s documentation!": [[0, "welcome-to-transformer-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "transformer": [[1, "transformer"]], "transformer_implementation package": [[2, "transformer-implementation-package"]], "Subpackages": [[2, "subpackages"], [3, "subpackages"]], "Submodules": [[2, "submodules"], [3, "submodules"], [4, "submodules"], [5, "submodules"]], "transformer_implementation.DataLoaderFactory module": [[2, "module-transformer_implementation.DataLoaderFactory"]], "transformer_implementation.Decoder module": [[2, "module-transformer_implementation.Decoder"]], "Attributes": [[2, "attributes"], [2, "id4"], [3, "attributes"], [3, "id2"], [4, "attributes"], [4, "id2"], [4, "id7"]], "Methods": [[2, "methods"], [2, "id5"], [3, "methods"], [3, "id3"], [4, "methods"], [4, "id3"], [4, "id8"]], "Parameters": [[2, "parameters"], [2, "id1"], [2, "id2"], [2, "id6"], [2, "id7"], [2, "id9"], [3, "parameters"], [3, "id1"], [3, "id4"], [3, "id5"], [4, "parameters"], [4, "id1"], [4, "id4"], [4, "id5"], [4, "id9"], [4, "id10"], [4, "id12"]], "Returns": [[2, "returns"], [2, "id3"], [2, "id8"], [2, "id10"], [3, "returns"], [3, "id6"], [4, "returns"], [4, "id6"], [4, "id11"], [4, "id13"]], "transformer_implementation.Encoder module": [[2, "module-transformer_implementation.Encoder"]], "transformer_implementation.Tokenizer module": [[2, "module-transformer_implementation.Tokenizer"]], "transformer_implementation.Transformer module": [[2, "module-transformer_implementation.Transformer"]], "transformer_implementation.TransformerConfig module": [[2, "module-transformer_implementation.TransformerConfig"]], "Module contents": [[2, "module-transformer_implementation"], [3, "module-transformer_implementation.blocks"], [4, "module-transformer_implementation.blocks.layers"], [5, "module-utils"]], "transformer_implementation.blocks package": [[3, "transformer-implementation-blocks-package"]], "transformer_implementation.blocks.DecoderBlock module": [[3, "module-transformer_implementation.blocks.DecoderBlock"]], "transformer_implementation.blocks.EncoderBlock module": [[3, "module-transformer_implementation.blocks.EncoderBlock"]], "transformer_implementation.blocks.layers package": [[4, "transformer-implementation-blocks-layers-package"]], "transformer_implementation.blocks.layers.FeedForward module": [[4, "module-transformer_implementation.blocks.layers.FeedForward"]], "transformer_implementation.blocks.layers.LayerNorm module": [[4, "module-transformer_implementation.blocks.layers.LayerNorm"]], "transformer_implementation.blocks.layers.MultiHeadAttention module": [[4, "module-transformer_implementation.blocks.layers.MultiHeadAttention"]], "utils package": [[5, "utils-package"]], "utils.estimate_loss module": [[5, "module-utils.estimate_loss"]], "utils.plot_losses module": [[5, "module-utils.plot_losses"]], "utils.training_loop module": [[5, "module-utils.training_loop"]]}, "indexentries": {"bos_idx (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.BOS_IDX"]], "dataloaderfactory (class in transformer_implementation.dataloaderfactory)": [[2, "transformer_implementation.DataLoaderFactory.DataLoaderFactory"]], "decoder (class in transformer_implementation.decoder)": [[2, "transformer_implementation.Decoder.Decoder"]], "eos_idx (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.EOS_IDX"]], "encoder (class in transformer_implementation.encoder)": [[2, "transformer_implementation.Encoder.Encoder"]], "pad_idx (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.PAD_IDX"]], "tokenizer (class in transformer_implementation.tokenizer)": [[2, "transformer_implementation.Tokenizer.Tokenizer"]], "transformer (class in transformer_implementation.transformer)": [[2, "transformer_implementation.Transformer.Transformer"]], "transformerconfig (class in transformer_implementation.transformerconfig)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig"]], "translationdataset (class in transformer_implementation.dataloaderfactory)": [[2, "transformer_implementation.DataLoaderFactory.TranslationDataset"]], "backend (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.backend"]], "batch_size (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.batch_size"]], "beta1 (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.beta1"]], "beta2 (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.beta2"]], "bias (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.bias"]], "block_size (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.block_size"]], "compile (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.compile"]], "configure_optimizers() (transformer_implementation.transformer.transformer method)": [[2, "transformer_implementation.Transformer.Transformer.configure_optimizers"]], "ddp (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.ddp"]], "device (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.device"]], "device_type (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.device_type"]], "dropout (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.dropout"]], "dtype (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.dtype"]], "eps (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.eps"]], "eval_iters (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.eval_iters"]], "forward() (transformer_implementation.decoder.decoder method)": [[2, "transformer_implementation.Decoder.Decoder.forward"]], "forward() (transformer_implementation.encoder.encoder method)": [[2, "transformer_implementation.Encoder.Encoder.forward"]], "forward() (transformer_implementation.transformer.transformer method)": [[2, "transformer_implementation.Transformer.Transformer.forward"]], "generate() (transformer_implementation.transformer.transformer method)": [[2, "transformer_implementation.Transformer.Transformer.generate"]], "generate_padding_mask() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.generate_padding_mask"]], "get_batch() (transformer_implementation.dataloaderfactory.dataloaderfactory method)": [[2, "transformer_implementation.DataLoaderFactory.DataLoaderFactory.get_batch"]], "get_num_params() (transformer_implementation.decoder.decoder method)": [[2, "transformer_implementation.Decoder.Decoder.get_num_params"]], "get_num_params() (transformer_implementation.encoder.encoder method)": [[2, "transformer_implementation.Encoder.Encoder.get_num_params"]], "grad_accumulation_steps (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.grad_accumulation_steps"]], "learning_rate (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.learning_rate"]], "load_model() (transformer_implementation.transformer.transformer method)": [[2, "transformer_implementation.Transformer.Transformer.load_model"]], "max_epochs (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.max_epochs"]], "max_iters (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.max_iters"]], "module": [[2, "module-transformer_implementation"], [2, "module-transformer_implementation.DataLoaderFactory"], [2, "module-transformer_implementation.Decoder"], [2, "module-transformer_implementation.Encoder"], [2, "module-transformer_implementation.Tokenizer"], [2, "module-transformer_implementation.Transformer"], [2, "module-transformer_implementation.TransformerConfig"], [3, "module-transformer_implementation.blocks"], [3, "module-transformer_implementation.blocks.DecoderBlock"], [3, "module-transformer_implementation.blocks.EncoderBlock"], [4, "module-transformer_implementation.blocks.layers"], [4, "module-transformer_implementation.blocks.layers.FeedForward"], [4, "module-transformer_implementation.blocks.layers.LayerNorm"], [4, "module-transformer_implementation.blocks.layers.MultiHeadAttention"], [5, "module-utils"], [5, "module-utils.estimate_loss"], [5, "module-utils.plot_losses"], [5, "module-utils.training_loop"]], "n_embd (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.n_embd"]], "n_head (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.n_head"]], "n_layer (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.n_layer"]], "save_model() (transformer_implementation.transformer.transformer method)": [[2, "transformer_implementation.Transformer.Transformer.save_model"]], "sequence_cleaner() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.sequence_cleaner"]], "sequence_padding() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.sequence_padding"]], "tokenize() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.tokenize"]], "tokenize_from_str() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.tokenize_from_str"]], "train_data_size (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.train_data_size"]], "transformer_implementation": [[2, "module-transformer_implementation"]], "transformer_implementation.dataloaderfactory": [[2, "module-transformer_implementation.DataLoaderFactory"]], "transformer_implementation.decoder": [[2, "module-transformer_implementation.Decoder"]], "transformer_implementation.encoder": [[2, "module-transformer_implementation.Encoder"]], "transformer_implementation.tokenizer": [[2, "module-transformer_implementation.Tokenizer"]], "transformer_implementation.transformer": [[2, "module-transformer_implementation.Transformer"]], "transformer_implementation.transformerconfig": [[2, "module-transformer_implementation.TransformerConfig"]], "vocab_size (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.vocab_size"]], "vocab_size() (transformer_implementation.tokenizer.tokenizer method)": [[2, "transformer_implementation.Tokenizer.Tokenizer.vocab_size"]], "weight_decay (transformer_implementation.transformerconfig.transformerconfig attribute)": [[2, "transformer_implementation.TransformerConfig.TransformerConfig.weight_decay"]], "decoderblock (class in transformer_implementation.blocks.decoderblock)": [[3, "transformer_implementation.blocks.DecoderBlock.DecoderBlock"]], "encoderblock (class in transformer_implementation.blocks.encoderblock)": [[3, "transformer_implementation.blocks.EncoderBlock.EncoderBlock"]], "forward() (transformer_implementation.blocks.decoderblock.decoderblock method)": [[3, "transformer_implementation.blocks.DecoderBlock.DecoderBlock.forward"]], "forward() (transformer_implementation.blocks.encoderblock.encoderblock method)": [[3, "transformer_implementation.blocks.EncoderBlock.EncoderBlock.forward"]], "transformer_implementation.blocks": [[3, "module-transformer_implementation.blocks"]], "transformer_implementation.blocks.decoderblock": [[3, "module-transformer_implementation.blocks.DecoderBlock"]], "transformer_implementation.blocks.encoderblock": [[3, "module-transformer_implementation.blocks.EncoderBlock"]], "feedforward (class in transformer_implementation.blocks.layers.feedforward)": [[4, "transformer_implementation.blocks.layers.FeedForward.FeedForward"]], "layernorm (class in transformer_implementation.blocks.layers.layernorm)": [[4, "transformer_implementation.blocks.layers.LayerNorm.LayerNorm"]], "multiheadattention (class in transformer_implementation.blocks.layers.multiheadattention)": [[4, "transformer_implementation.blocks.layers.MultiHeadAttention.MultiHeadAttention"]], "forward() (transformer_implementation.blocks.layers.feedforward.feedforward method)": [[4, "transformer_implementation.blocks.layers.FeedForward.FeedForward.forward"]], "forward() (transformer_implementation.blocks.layers.layernorm.layernorm method)": [[4, "transformer_implementation.blocks.layers.LayerNorm.LayerNorm.forward"]], "forward() (transformer_implementation.blocks.layers.multiheadattention.multiheadattention method)": [[4, "transformer_implementation.blocks.layers.MultiHeadAttention.MultiHeadAttention.forward"]], "scaled_dot_product_attention() (transformer_implementation.blocks.layers.multiheadattention.multiheadattention method)": [[4, "transformer_implementation.blocks.layers.MultiHeadAttention.MultiHeadAttention.scaled_dot_product_attention"]], "transformer_implementation.blocks.layers": [[4, "module-transformer_implementation.blocks.layers"]], "transformer_implementation.blocks.layers.feedforward": [[4, "module-transformer_implementation.blocks.layers.FeedForward"]], "transformer_implementation.blocks.layers.layernorm": [[4, "module-transformer_implementation.blocks.layers.LayerNorm"]], "transformer_implementation.blocks.layers.multiheadattention": [[4, "module-transformer_implementation.blocks.layers.MultiHeadAttention"]], "estimate_loss() (in module utils.estimate_loss)": [[5, "utils.estimate_loss.estimate_loss"]], "get_linear_schedule_with_warmup() (in module utils.training_loop)": [[5, "utils.training_loop.get_linear_schedule_with_warmup"]], "plot_losses() (in module utils.plot_losses)": [[5, "utils.plot_losses.plot_losses"]], "training_loop() (in module utils.training_loop)": [[5, "utils.training_loop.training_loop"]], "utils": [[5, "module-utils"]], "utils.estimate_loss": [[5, "module-utils.estimate_loss"]], "utils.plot_losses": [[5, "module-utils.plot_losses"]], "utils.training_loop": [[5, "module-utils.training_loop"]]}})