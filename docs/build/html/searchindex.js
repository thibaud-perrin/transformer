Search.setIndex({"docnames": ["index", "utils.DataLoaderFactory", "utils.LayerNorm", "utils.MultiHeadAttention", "utils.Tokenizer", "utils.TransformerConfig"], "filenames": ["index.rst", "utils.DataLoaderFactory.rst", "utils.LayerNorm.rst", "utils.MultiHeadAttention.rst", "utils.Tokenizer.rst", "utils.TransformerConfig.rst"], "titles": ["Welcome to Transformer\u2019s documentation!", "DataLoaderFactory Class", "LayerNorm Class", "MultiHeadAttention Class", "Tokenizer Class", "TransformerConfig Class"], "terms": {"index": 0, "modul": [0, 2, 3], "search": 0, "page": 0, "t": 3, "class": 0, "util": [1, 2, 3, 4, 5], "ndim": 2, "int": [1, 2, 3, 4, 5], "bia": [2, 5], "bool": [2, 3, 5], "A": [1, 2, 4], "layer": [2, 3], "normal": 2, "option": [2, 3, 4], "thi": [2, 3, 5], "implement": [2, 3], "allow": [2, 4], "turn": 2, "off": 2, "term": 2, "which": 2, "i": [2, 3, 4, 5], "directli": 2, "support": 2, "pytorch": [1, 2], "": [2, 4], "function": 2, "attribut": [2, 3, 5], "weight": 2, "learnabl": 2, "initi": 2, "an": [1, 2, 5], "all": 2, "ones": 2, "tensor": [2, 3, 4], "bias": 2, "zero": 2, "argument": 2, "constructor": 2, "true": [2, 5], "otherwis": 2, "set": [2, 5], "none": [2, 3], "arg": [1, 2, 3, 4], "integ": 2, "dimens": 2, "input": [1, 2, 3, 4], "vector": 2, "boolean": 2, "add": [2, 4], "output": [2, 3], "forward": [2, 3], "defin": 2, "comput": [2, 3], "perform": 2, "everi": 2, "call": 2, "The": [1, 2, 4, 5], "return": [2, 3, 4, 5], "layernorm": [0, 5], "token": [0, 1, 5], "encod": [1, 4, 5], "decod": [4, 5], "text": 4, "sequenc": [1, 3, 4, 5], "sequenceclearn": [], "method": [1, 4], "remov": 4, "bo": 4, "pad": 4, "eo": 4, "special": 4, "from": [1, 4, 5], "torch": 4, "list": 4, "clean": 4, "sequencepad": [], "max_siz": 4, "512": [4, 5], "ensur": 4, "length": [1, 4], "within": 4, "maximum": [1, 4], "size": [1, 4, 5], "union": 4, "default": [4, 5], "process": 4, "ad": 4, "limit": 4, "vocabs": [], "vocabulari": [4, 5], "transformerconfig": 0, "ani": 5, "block_siz": [1, 5], "batch_siz": [1, 3, 5], "12": 5, "n_layer": 5, "1": 5, "n_head": [3, 5], "2": 5, "n_embd": [3, 5], "128": 5, "dropout": [3, 5], "float": [3, 5], "0": 5, "fals": 5, "data": 5, "store": 5, "configur": 5, "transform": 5, "model": 5, "instanc": 5, "number": [3, 5], "each": 5, "batch": [1, 5], "vocab_s": [4, 5], "total": 5, "It": 5, "block": 5, "n": 5, "head": [3, 5], "attent": [3, 5], "embed": [3, 5], "origin": [1, 5], "paper": 5, "rate": [3, 5], "us": [1, 5], "indic": [3, 5], "whether": [1, 3, 5], "linear": [3, 5], "If": 5, "similar": 5, "gpt": 5, "bit": 5, "better": 5, "faster": 5, "devic": [1, 5], "str": [1, 5], "run": 5, "cpu": [1, 5], "cuda": [1, 5], "gpu": [1, 5], "avail": [3, 5], "learning_r": 5, "learn": 5, "optim": 5, "3e": 5, "4": 5, "eval_interv": 5, "step": 5, "between": 5, "valid": 5, "dataset": [1, 5], "eval_it": 5, "epoch": 5, "20": 5, "properti": 5, "dataloaderfactori": 0, "instanti": 1, "dataload": 1, "differ": 1, "split": 1, "huggingfac": 1, "ha": 1, "depend": 1, "we": 1, "get_batch": 1, "choos": 1, "correct": 1, "yield": 1, "train": 1, "val": 1, "test": 1, "dict": 1, "dictionari": 1, "kei": [1, 3], "target": 1, "translat": 1, "contain": 1, "sequence_clearn": 4, "sequence_pad": 4, "translationdataset": [0, 1], "creat": 1, "librari": 1, "custom": 1, "tiktoken": 1, "multiheadattent": 0, "config": 3, "multi": 3, "appli": 3, "mechan": 3, "doesn": 3, "mask": 3, "over": 3, "score": 3, "dimension": 3, "q_attn": 3, "nn": 3, "queri": 3, "project": 3, "k_attn": 3, "v_attn": 3, "valu": 3, "c_proj": 3, "attn_dropout": 3, "resid_dropout": 3, "residu": 3, "connect": 3, "flash": 3, "flag": 3, "q_x": 3, "k_x": 3, "v_x": 3, "pass": 3, "shape": 3, "seq_length": 3, "emb_dim": 3, "y": 3, "after": 3, "scaled_dot_product_attent": 3, "q": 3, "k": 3, "v": 3, "scale": 3, "dot": 3, "product": 3, "num_head": 3}, "objects": {"utils": [[1, 0, 1, "", "DataLoaderFactory"], [2, 0, 1, "", "LayerNorm"], [3, 0, 1, "", "MultiHeadAttention"], [4, 0, 1, "", "Tokenizer"], [5, 0, 1, "", "TransformerConfig"], [1, 0, 1, "", "TranslationDataset"]], "utils.DataLoaderFactory": [[1, 1, 1, "", "get_batch"]], "utils.LayerNorm": [[2, 1, 1, "", "forward"]], "utils.MultiHeadAttention": [[3, 1, 1, "", "forward"], [3, 1, 1, "", "scaled_dot_product_attention"]], "utils.Tokenizer": [[4, 1, 1, "", "sequence_clearner"], [4, 1, 1, "", "sequence_padding"], [4, 1, 1, "", "vocab_size"]], "utils.TransformerConfig": [[5, 2, 1, "", "vocab_size"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:property"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "property", "Python property"]}, "titleterms": {"welcom": 0, "transform": 0, "": 0, "document": 0, "indic": 0, "tabl": 0, "layernorm": 2, "modul": [], "class": [1, 2, 3, 4, 5], "token": 4, "content": 0, "transformerconfig": 5, "dataloaderfactori": 1, "multiheadattent": 3}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 57}, "alltitles": {"LayerNorm Class": [[2, "layernorm-class"]], "Tokenizer Class": [[4, "tokenizer-class"]], "TransformerConfig Class": [[5, "transformerconfig-class"]], "DataLoaderFactory Class": [[1, "dataloaderfactory-class"]], "Welcome to Transformer\u2019s documentation!": [[0, "welcome-to-transformer-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "MultiHeadAttention Class": [[3, "multiheadattention-class"]]}, "indexentries": {"multiheadattention (class in utils)": [[3, "utils.MultiHeadAttention"]], "forward() (utils.multiheadattention method)": [[3, "utils.MultiHeadAttention.forward"]], "scaled_dot_product_attention() (utils.multiheadattention method)": [[3, "utils.MultiHeadAttention.scaled_dot_product_attention"]]}})