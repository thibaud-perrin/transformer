Search.setIndex({"docnames": ["index", "modules", "transformers", "transformers.blocks", "transformers.blocks.layers", "utils"], "filenames": ["index.rst", "modules.rst", "transformers.rst", "transformers.blocks.rst", "transformers.blocks.layers.rst", "utils.rst"], "titles": ["Welcome to Transformer\u2019s documentation!", "transformer", "transformers package", "transformers.blocks package", "transformers.blocks.layers package", "utils package"], "terms": {"index": 0, "modul": [0, 1], "search": [0, 2], "page": 0, "t": 4, "class": [2, 3, 4], "util": [0, 1], "ndim": 4, "int": [2, 4], "bia": [1, 2, 3, 4], "bool": [2, 4], "A": [2, 3, 4, 5], "layer": [2, 3], "normal": [3, 4], "option": [2, 4], "thi": [2, 4, 5], "implement": [2, 3, 4], "allow": [2, 4], "turn": 4, "off": 4, "term": 4, "which": [2, 4, 5], "i": [2, 3, 4], "directli": 4, "support": 4, "pytorch": [2, 4], "": [2, 3, 4], "function": [4, 5], "attribut": [2, 3, 4], "weight": [2, 3, 4], "learnabl": 4, "initi": 4, "an": [2, 4], "all": [2, 4], "ones": 4, "tensor": [2, 3, 4], "bias": 4, "zero": 4, "argument": 4, "constructor": 4, "true": [2, 4], "otherwis": 4, "set": [2, 4, 5], "none": [2, 4], "arg": [2, 3, 4, 5], "integ": 4, "dimens": [2, 4], "input": [2, 3, 4], "vector": 4, "boolean": 4, "add": [2, 4], "output": [2, 3, 4], "forward": [1, 2, 3, 4], "defin": [2, 3, 4, 5], "comput": [2, 3, 4], "perform": [2, 3, 4, 5], "everi": [2, 3, 4], "call": [2, 3, 4], "The": [2, 3, 4, 5], "return": [2, 3, 4, 5], "layernorm": [2, 3], "token": 1, "encod": [1, 3], "decod": [1, 3], "text": 2, "sequenc": [2, 4], "sequenceclearn": [], "method": [2, 5], "remov": 2, "bo": 2, "pad": 2, "eo": 2, "special": 2, "from": [2, 3], "torch": [2, 3, 4], "list": [2, 4, 5], "clean": [], "sequencepad": [], "max_siz": 2, "512": 2, "ensur": 2, "length": 2, "within": 2, "maximum": 2, "size": 2, "union": [], "default": 2, "process": 2, "ad": 2, "limit": 2, "vocabs": [], "vocabulari": 2, "transformerconfig": 1, "ani": 2, "block_siz": [1, 2], "batch_siz": [1, 2, 4], "12": 2, "n_layer": [1, 2], "1": 2, "n_head": [1, 2, 4], "2": 2, "n_embd": [1, 2, 3, 4], "128": 2, "dropout": [1, 2, 4], "float": [2, 4], "0": 2, "fals": 2, "data": [2, 5], "store": 2, "configur": [2, 3, 4, 5], "transform": 5, "model": [2, 3, 4, 5], "instanc": 2, "number": [2, 4, 5], "each": [2, 3, 5], "batch": 2, "vocab_s": [1, 2], "total": 2, "It": [2, 5], "block": [1, 2], "n": 2, "head": [2, 3, 4], "attent": [2, 3, 4], "embed": [2, 4], "origin": 2, "paper": 2, "rate": [2, 4], "us": [2, 5], "indic": [2, 4], "whether": [2, 4], "linear": [2, 4], "If": 2, "similar": 2, "gpt": 2, "bit": 2, "better": 2, "faster": 2, "devic": [1, 2], "str": [2, 5], "run": 2, "cpu": 2, "cuda": 2, "gpu": 2, "avail": [2, 4], "learning_r": [1, 2], "learn": 2, "optim": 2, "3e": 2, "4": 2, "eval_interv": [1, 2], "step": 2, "between": [2, 4], "valid": [2, 5], "dataset": [2, 5], "eval_it": [1, 2], "epoch": [2, 5], "20": 2, "properti": 2, "dataloaderfactori": 1, "instanti": 2, "dataload": 2, "differ": 2, "split": [2, 5], "huggingfac": 2, "ha": 2, "depend": 2, "we": 2, "get_batch": [1, 2, 5], "choos": 2, "correct": 2, "yield": 2, "train": [2, 5], "val": [2, 5], "test": 2, "dict": [2, 5], "dictionari": [2, 5], "kei": [2, 4, 5], "target": 2, "translat": 2, "contain": [2, 5], "sequence_clearn": [], "sequence_pad": [1, 2], "translationdataset": [1, 2], "creat": 2, "librari": 2, "custom": 2, "tiktoken": 2, "multiheadattent": [2, 3], "config": [2, 3, 4, 5], "multi": [3, 4], "appli": 4, "mechan": [2, 3, 4], "doesn": 4, "mask": 4, "over": [3, 4, 5], "score": [2, 4], "dimension": 4, "q_attn": 4, "nn": [2, 4], "queri": 4, "project": 4, "k_attn": 4, "v_attn": 4, "valu": [4, 5], "c_proj": 4, "attn_dropout": 4, "resid_dropout": 4, "residu": [3, 4], "connect": [3, 4], "flash": 4, "flag": 4, "q_x": 4, "k_x": 4, "v_x": 4, "pass": [2, 3, 4], "shape": 4, "seq_length": 4, "emb_dim": 4, "y": 4, "after": 4, "scaled_dot_product_attent": [3, 4], "q": 4, "k": 4, "v": 4, "scale": 4, "dot": 4, "product": 4, "num_head": 4, "feedforward": [2, 3], "posit": [2, 3, 4], "wise": [3, 4], "feed": [3, 4], "neural": 4, "network": [3, 4], "ffnn": 4, "consist": [2, 3, 4], "two": [3, 4], "gelu": 4, "activ": 4, "follow": [2, 3, 4], "regular": 4, "c_fc": 4, "first": [2, 3, 4], "fulli": [3, 4], "second": [3, 4], "object": [2, 3, 4, 5], "x": [3, 4], "encoderblock": [1, 2], "singl": 3, "sub": 3, "self": 3, "There": 3, "around": 3, "ln_1": 3, "befor": 3, "attn": 3, "ln_2": 3, "ffw": 3, "decoderblock": [1, 2], "three": 3, "attn1": 3, "attn2": 3, "attend": 3, "ln_3": 3, "encoder_output": 3, "last": 3, "part": 2, "sever": 2, "arrang": 2, "goe": 2, "through": 2, "moduledict": 2, "make": 2, "up": 2, "idx": 2, "against": 2, "loss": [2, 5], "calcul": [2, 5], "logit": 2, "were": [], "provid": 5, "get_num_param": [1, 2], "non_embed": 2, "paramet": 2, "For": 2, "non": 2, "count": 2, "get": 2, "subtract": 2, "would": 2, "too": 2, "except": 2, "due": 2, "share": 2, "param": 2, "ar": 2, "actual": 2, "final": 2, "so": 2, "includ": 2, "them": 2, "exclud": 2, "lm_head": 2, "map": 2, "enc_output": 2, "gener": 2, "max_new_token": [], "temperatur": [], "top_k": [], "take": [], "condit": [], "longtensor": [], "b": [], "complet": [], "time": [], "predict": [], "back": [], "most": [], "like": [], "you": [], "ll": [], "want": 2, "sure": [], "eval": [], "mode": 5, "oper": [], "both": 2, "transduct": 2, "primarili": 2, "task": 2, "requir": 2, "understand": 2, "context": 2, "relationship": 2, "among": 2, "word": 2, "src": 2, "tgt": 2, "basi": 2, "estimate_loss": 1, "plot_loss": 1, "cross": [2, 3], "encoder_attn": 3, "current": [2, 3], "cross_attn": 3, "decoder_attn": 3, "attn_weight": 4, "useful": 4, "visual": [1, 2, 4], "how": 4, "work": 4, "sequence_clean": [1, 2], "where": 2, "alloc": 2, "separ": 2, "convert": 2, "string": 2, "format": 2, "load_model": [1, 2], "path": 2, "load": 2, "state": 2, "file": 2, "should": [2, 5], "rais": 2, "valueerror": 2, "specifi": [2, 5], "doe": 2, "exist": 2, "save_model": [1, 2], "save": 2, "translate_beam_search": [1, 2], "beam_siz": 2, "5": 2, "sourc": 2, "beam": 2, "tupl": 2, "best": 2, "found": 2, "256": 2, "max_it": [1, 2], "2000": 2, "200": 2, "estim": 5, "without": 5, "backpropag": 5, "evalu": 5, "iter": 5, "averag": 5, "need": 5, "customdataset": 5, "name": 5, "out": 5, "correspond": 5, "plot": 5, "more": 5, "vibrant": 5, "color": 5, "black": 5, "background": 5, "record": 5, "build": [], "base": [2, 3, 4], "tokenize_from_str": [1, 2], "0003": 2, "packag": [0, 1], "subpackag": 1, "submodul": 1, "content": 1}, "objects": {"": [[2, 0, 0, "-", "transformers"], [5, 0, 0, "-", "utils"]], "transformers": [[2, 0, 0, "-", "DataLoaderFactory"], [2, 0, 0, "-", "Decoder"], [2, 0, 0, "-", "Encoder"], [2, 0, 0, "-", "Tokenizer"], [2, 0, 0, "-", "Transformer"], [2, 0, 0, "-", "TransformerConfig"], [3, 0, 0, "-", "blocks"]], "transformers.DataLoaderFactory": [[2, 1, 1, "", "DataLoaderFactory"], [2, 1, 1, "", "TranslationDataset"]], "transformers.DataLoaderFactory.DataLoaderFactory": [[2, 2, 1, "", "get_batch"]], "transformers.Decoder": [[2, 1, 1, "", "Decoder"]], "transformers.Decoder.Decoder": [[2, 2, 1, "", "forward"], [2, 2, 1, "", "get_num_params"]], "transformers.Encoder": [[2, 1, 1, "", "Encoder"]], "transformers.Encoder.Encoder": [[2, 2, 1, "", "forward"], [2, 2, 1, "", "get_num_params"]], "transformers.Tokenizer": [[2, 1, 1, "", "Tokenizer"]], "transformers.Tokenizer.Tokenizer": [[2, 2, 1, "", "sequence_cleaner"], [2, 2, 1, "", "sequence_padding"], [2, 2, 1, "", "tokenize"], [2, 2, 1, "", "tokenize_from_str"], [2, 2, 1, "", "vocab_size"]], "transformers.Transformer": [[2, 1, 1, "", "Transformer"]], "transformers.Transformer.Transformer": [[2, 2, 1, "", "forward"], [2, 2, 1, "", "load_model"], [2, 2, 1, "", "save_model"], [2, 2, 1, "", "translate_beam_search"]], "transformers.TransformerConfig": [[2, 1, 1, "", "TransformerConfig"]], "transformers.TransformerConfig.TransformerConfig": [[2, 3, 1, "", "batch_size"], [2, 3, 1, "", "bias"], [2, 3, 1, "", "block_size"], [2, 3, 1, "", "device"], [2, 3, 1, "", "dropout"], [2, 3, 1, "", "eval_interval"], [2, 3, 1, "", "eval_iters"], [2, 3, 1, "", "learning_rate"], [2, 3, 1, "", "max_iters"], [2, 3, 1, "", "n_embd"], [2, 3, 1, "", "n_head"], [2, 3, 1, "", "n_layer"], [2, 3, 1, "", "tokenizer"], [2, 3, 1, "", "visualize"], [2, 4, 1, "", "vocab_size"]], "transformers.blocks": [[3, 0, 0, "-", "DecoderBlock"], [3, 0, 0, "-", "EncoderBlock"], [4, 0, 0, "-", "layers"]], "transformers.blocks.DecoderBlock": [[3, 1, 1, "", "DecoderBlock"]], "transformers.blocks.DecoderBlock.DecoderBlock": [[3, 2, 1, "", "forward"]], "transformers.blocks.EncoderBlock": [[3, 1, 1, "", "EncoderBlock"]], "transformers.blocks.EncoderBlock.EncoderBlock": [[3, 2, 1, "", "forward"]], "transformers.blocks.layers": [[4, 0, 0, "-", "FeedForward"], [4, 0, 0, "-", "LayerNorm"], [4, 0, 0, "-", "MultiHeadAttention"]], "transformers.blocks.layers.FeedForward": [[4, 1, 1, "", "FeedForward"]], "transformers.blocks.layers.FeedForward.FeedForward": [[4, 2, 1, "", "forward"]], "transformers.blocks.layers.LayerNorm": [[4, 1, 1, "", "LayerNorm"]], "transformers.blocks.layers.LayerNorm.LayerNorm": [[4, 2, 1, "", "forward"]], "transformers.blocks.layers.MultiHeadAttention": [[4, 1, 1, "", "MultiHeadAttention"]], "transformers.blocks.layers.MultiHeadAttention.MultiHeadAttention": [[4, 2, 1, "", "forward"], [4, 2, 1, "", "scaled_dot_product_attention"]], "utils": [[5, 0, 0, "-", "estimate_loss"], [5, 0, 0, "-", "plot_losses"]], "utils.estimate_loss": [[5, 5, 1, "", "estimate_loss"]], "utils.plot_losses": [[5, 5, 1, "", "plot_losses"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:property", "5": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "property", "Python property"], "5": ["py", "function", "Python function"]}, "titleterms": {"welcom": 0, "transform": [0, 1, 2, 3, 4], "": 0, "document": 0, "indic": 0, "tabl": 0, "layernorm": 4, "modul": [2, 3, 4, 5], "class": [], "token": 2, "content": [0, 2, 3, 4, 5], "transformerconfig": 2, "dataloaderfactori": 2, "multiheadattent": 4, "feedforward": 4, "encoderblock": 3, "decoderblock": 3, "encod": 2, "decod": 2, "estimate_loss": 5, "plot_loss": 5, "block": [3, 4], "layer": 4, "util": 5, "packag": [2, 3, 4, 5], "subpackag": [2, 3], "submodul": [2, 3, 4, 5]}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 57}, "alltitles": {"Welcome to Transformer\u2019s documentation!": [[0, "welcome-to-transformer-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "Submodules": [[5, "submodules"], [2, "submodules"], [3, "submodules"], [4, "submodules"]], "Module contents": [[5, "module-utils"], [2, "module-transformers"], [3, "module-transformers.blocks"], [4, "module-transformers.blocks.layers"]], "utils package": [[5, "utils-package"]], "utils.estimate_loss module": [[5, "module-utils.estimate_loss"]], "utils.plot_losses module": [[5, "module-utils.plot_losses"]], "transformers package": [[2, "transformers-package"]], "Subpackages": [[2, "subpackages"], [3, "subpackages"]], "transformers.DataLoaderFactory module": [[2, "module-transformers.DataLoaderFactory"]], "transformers.Decoder module": [[2, "module-transformers.Decoder"]], "transformers.Encoder module": [[2, "module-transformers.Encoder"]], "transformers.Tokenizer module": [[2, "module-transformers.Tokenizer"]], "transformers.Transformer module": [[2, "module-transformers.Transformer"]], "transformers.TransformerConfig module": [[2, "module-transformers.TransformerConfig"]], "transformers.blocks package": [[3, "transformers-blocks-package"]], "transformers.blocks.DecoderBlock module": [[3, "module-transformers.blocks.DecoderBlock"]], "transformers.blocks.EncoderBlock module": [[3, "module-transformers.blocks.EncoderBlock"]], "transformers.blocks.layers package": [[4, "transformers-blocks-layers-package"]], "transformers.blocks.layers.FeedForward module": [[4, "module-transformers.blocks.layers.FeedForward"]], "transformers.blocks.layers.LayerNorm module": [[4, "module-transformers.blocks.layers.LayerNorm"]], "transformers.blocks.layers.MultiHeadAttention module": [[4, "module-transformers.blocks.layers.MultiHeadAttention"]], "transformer": [[1, "transformer"]]}, "indexentries": {}})