{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e00ed2-2a4f-4a18-8437-b01e8114801f",
   "metadata": {},
   "source": [
    "# Train transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd84bd7-38bc-4892-9c1f-cf2dc0af6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformer_implementation import Transformer, Tokenizer, TransformerConfig, DataLoaderFactory\n",
    "from utils import training_loop, plot_losses, estimate_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f8cc4-1ed2-4c69-86bd-702686561503",
   "metadata": {},
   "source": [
    "## Init\n",
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab956e7-7067-48fc-816f-cc1814f2c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63bf7f-d62b-425a-b572-e4c7175ebc92",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f985eab2-93bd-42dd-88f5-8d363341f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig(\n",
      "\tself.tokenizer=<transformer_implementation.Tokenizer.Tokenizer object at 0x0000022A0C6BFE90>,\n",
      "\tself.block_size=256,\n",
      "\tself.batch_size=12,\n",
      "\tself.n_layer=3,\n",
      "\tself.n_head=8,\n",
      "\tself.n_embd=256,\n",
      "\tself.dropout=0.1,\n",
      "\tself.bias=False,\n",
      "\tself.device='cuda',\n",
      "\tself.learning_rate=0.0003,\n",
      "\tself.max_iters=2000,\n",
      "\tself.eval_interval=100,\n",
      "\tself.eval_iters=50,\n",
      "\tself.visualize=False,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# init config\n",
    "config = TransformerConfig(\n",
    "    tokenizer,\n",
    "    block_size = 256,\n",
    "    batch_size = 12,\n",
    "    n_layer = 3, # 6,\n",
    "    n_head = 8,\n",
    "    # n_embd = 512,\n",
    "    max_iters = 2000,\n",
    "    eval_iters = 50,\n",
    "    eval_interval = 100,\n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77fbb2-162b-4589-8ca0-98d80e539bb1",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9de1b3-95be-4adb-bfc1-de3a27eb0eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m\u001b[4mNumber of data by datasets splits\u001b[0m\n",
      "Train\t\t: 5000000\t-> 416666.6666666667\n",
      "Validation\t: 3000\t\t-> 250.0\n",
      "Test\t\t: 3003\t\t-> 250.25\n",
      "Total\t\t: 5006003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5006003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "dataset = DataLoaderFactory(config.block_size, config.batch_size, tokenizer, config.device, 5000000)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89467cb-b399-4906-b89c-fb9d554e7dd4",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c7c5a8-3fd1-4861-83d3-713d88291dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encoder parameters: 28.03M\n",
      "number of parameters: 28.82M\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Encoder.forward() missing 1 required positional argument: 'idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Use nn.DataParallel to wrap the model.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# This will distribute the operations to multiple GPUs if they are available.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Users\\thiba\\Documents\\Professionnel\\data-science\\Kaggle\\transformer\\transformer_implementation\\Transformer.py:33\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m Decoder(config)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# report number of parameters\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of parameters: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e6\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e6\u001b[39m,))\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: Encoder.forward() missing 1 required positional argument: 'idx'"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = Transformer(config)\n",
    "model.train()\n",
    "# Use nn.DataParallel to wrap the model.\n",
    "# This will distribute the operations to multiple GPUs if they are available.\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f428744-3d2f-4df9-8414-56e75ae5acbd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97e48a-b435-4d6e-97b7-416e67aef6f7",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb80bbc-39c7-4cbb-869a-9557cb968411",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_list = training_loop(model, optimizer, dataset, config, saved_path = \"./out/transformer-train.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96236360-9fa5-42d4-9beb-e802ba9c9cdf",
   "metadata": {},
   "source": [
    "### Plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac7486a-daa4-439a-ba76-d33dd9c61589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "plot_losses(losses_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c737bd4-e48a-489e-b8b4-276da31c5b72",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c786cd2-80d4-47fb-81c2-88c5cd31cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = estimate_loss(model, dataset, config, ['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47d53b-ce30-4cfb-92de-b5979258cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test loss = {test_loss['test'].item():4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a9991-a210-49af-acc3-17c63055651e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
