{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e00ed2-2a4f-4a18-8437-b01e8114801f",
   "metadata": {},
   "source": [
    "# Train transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd84bd7-38bc-4892-9c1f-cf2dc0af6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformer_implementation import Transformer, Tokenizer, TransformerConfig, DataLoaderFactory\n",
    "from utils import training_loop, plot_losses, estimate_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f8cc4-1ed2-4c69-86bd-702686561503",
   "metadata": {},
   "source": [
    "## Init\n",
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab956e7-7067-48fc-816f-cc1814f2c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63bf7f-d62b-425a-b572-e4c7175ebc92",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f985eab2-93bd-42dd-88f5-8d363341f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig(\n",
      "\tself.tokenizer=<transformer_implementation.Tokenizer.Tokenizer object at 0x0000016EB1267450>,\n",
      "\tself.block_size=256,\n",
      "\tself.batch_size=12,\n",
      "\tself.n_layer=6,\n",
      "\tself.n_head=8,\n",
      "\tself.n_embd=512,\n",
      "\tself.dropout=0.1,\n",
      "\tself.bias=False,\n",
      "\tself.device='cuda',\n",
      "\tself.learning_rate=0.0003,\n",
      "\tself.max_iters=2000,\n",
      "\tself.eval_interval=100,\n",
      "\tself.eval_iters=50,\n",
      "\tself.visualize=False,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# init config\n",
    "config = TransformerConfig(\n",
    "    tokenizer,\n",
    "    block_size = 256,\n",
    "    batch_size = 12,\n",
    "    n_layer = 6,\n",
    "    n_head = 8,\n",
    "    n_embd = 512,\n",
    "    max_iters = 2000,\n",
    "    eval_iters = 50,\n",
    "    eval_interval = 100,\n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77fbb2-162b-4589-8ca0-98d80e539bb1",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9de1b3-95be-4adb-bfc1-de3a27eb0eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m\u001b[4mNumber of data by datasets splits\u001b[0m\n",
      "Train\t\t: 5000000\t-> 416666.6666666667\n",
      "Validation\t: 3000\t\t-> 250.0\n",
      "Test\t\t: 3003\t\t-> 250.25\n",
      "Total\t\t: 5006003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5006003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "dataset = DataLoaderFactory(config.block_size, config.batch_size, tokenizer, config.device, 5000000)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89467cb-b399-4906-b89c-fb9d554e7dd4",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c7c5a8-3fd1-4861-83d3-713d88291dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encoder parameters: 70.22M\n",
      "number of parameters: 76.52M\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = Transformer(config)\n",
    "model.train()\n",
    "# Use nn.DataParallel to wrap the model.\n",
    "# This will distribute the operations to multiple GPUs if they are available.\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f428744-3d2f-4df9-8414-56e75ae5acbd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97e48a-b435-4d6e-97b7-416e67aef6f7",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb80bbc-39c7-4cbb-869a-9557cb968411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cb6676b3784ac1910ca7fd32fd0697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train loss nan, Val loss nan, Saved nan:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiba\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.14 GiB (GPU 0; 8.00 GiB total capacity; 5.47 GiB already allocated; 0 bytes free; 6.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses_list \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./out/transformer-train.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Users\\thiba\\Documents\\Professionnel\\data-science\\Kaggle\\transformer\\utils\\training_loop.py:73\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, optimizer, dataset, config, saved_path)\u001b[0m\n\u001b[0;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Perform backpropagation\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Update the model parameters\u001b[39;00m\n\u001b[0;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.14 GiB (GPU 0; 8.00 GiB total capacity; 5.47 GiB already allocated; 0 bytes free; 6.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "losses_list = training_loop(model, optimizer, dataset, config, saved_path = \"./out/transformer-train.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96236360-9fa5-42d4-9beb-e802ba9c9cdf",
   "metadata": {},
   "source": [
    "### Plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac7486a-daa4-439a-ba76-d33dd9c61589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "plot_losses(losses_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c737bd4-e48a-489e-b8b4-276da31c5b72",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c786cd2-80d4-47fb-81c2-88c5cd31cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = estimate_loss(model, dataset, config, ['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47d53b-ce30-4cfb-92de-b5979258cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test loss = {test_loss['test'].item():4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a9991-a210-49af-acc3-17c63055651e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
