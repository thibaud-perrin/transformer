{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e00ed2-2a4f-4a18-8437-b01e8114801f",
   "metadata": {},
   "source": [
    "# Train transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd84bd7-38bc-4892-9c1f-cf2dc0af6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from transformer_implementation import Tokenizer, TransformerConfig, DataLoaderFactory, LayerNorm, MultiHeadAttention, FeedForward\n",
    "from utils import plot_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f8cc4-1ed2-4c69-86bd-702686561503",
   "metadata": {},
   "source": [
    "## Init\n",
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab956e7-7067-48fc-816f-cc1814f2c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b354c-0d44-436a-a7e9-f0824d652ab9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19318411-ba0a-4e70-a0bb-18a2b1164eb1",
   "metadata": {},
   "source": [
    "### DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a49aa2-3668-459c-8375-530b3e2bdcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements a single decoder block in the Transformer model.\n",
    "\n",
    "    Each block consists of three sub-layers: a multi-head self-attention mechanism,\n",
    "    a multi-head attention mechanism over the encoder's output, and a position-wise \n",
    "    fully connected feed-forward network. There is a residual connection around \n",
    "    each of the three sub-layers, followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        - ln_1 (LayerNorm): Layer normalization before the first multi-head attention layer.\n",
    "        - attn1 (MultiHeadAttention): First multi-head attention layer, with self-attention.\n",
    "        - ln_2 (LayerNorm): Layer normalization before the second multi-head attention layer.\n",
    "        - attn2 (MultiHeadAttention): Second multi-head attention layer, attends to encoder outputs.\n",
    "        - ln_3 (LayerNorm): Layer normalization before the feed-forward network.\n",
    "        - ffw (FeedForward): Position-wise feed-forward network.\n",
    "\n",
    "    Args:\n",
    "        config (Config): A configuration object with attribute `n_embd` and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn1 = MultiHeadAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.ffw = FeedForward(config)\n",
    "\n",
    "    def forward(self, x, tgt_mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): The input tensor to the forward pass.\n",
    "            - tgt_mask (torch.Tensor, optional): The mask tensor to ignore padding, size (B, 1, 1, T).\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor of the block.\n",
    "            - encoder_attn: The encoder attention weight of the current block.\n",
    "        \"\"\"\n",
    "        # Masked MultiHeadAttention\n",
    "        x = self.ln_1(x)\n",
    "        x_attn, encoder_attn = checkpoint(self.attn1, x, x, x, True, tgt_mask)\n",
    "        x = self.ln_2(x + x_attn)\n",
    "        # FeedForward\n",
    "        x = x + checkpoint(self.ffw, x)\n",
    "        return x, encoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192033e-87b8-44da-b45c-767559b705fa",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663e4b2a-f252-4253-8279-ba859fad9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the decoder part of the Transformer model.\n",
    "\n",
    "    The Decoder consists of several DecoderBlocks arranged in sequence. The input first goes through an embedding \n",
    "    layer followed by a positional encoding layer. The output of this is then passed through each DecoderBlock in \n",
    "    sequence.\n",
    "\n",
    "    Attributes:\n",
    "        - decoder (nn.ModuleDict): A dictionary of modules making up the transformer decoder.\n",
    "        - lm_head (nn.Linear): The final linear layer mapping from the embedding dimension to the vocabulary size.\n",
    "        - config (:obj:`Config`): The configuration object for the transformer model.\n",
    "\n",
    "    .. note:: The weight of the embedding layer and the linear layer are shared.\n",
    "\n",
    "    Args:\n",
    "        - config (:obj:`Config`): The configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.decoder = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # Learned positional encoding:\n",
    "            # In this case, instead of using a fixed function to determine positional encoding,\n",
    "            # we initialize a tensor of positional encodings which gets updated during training via backpropagation.\n",
    "            # This method may potentially capture more complex position-related patterns than fixed positional encoding,\n",
    "            # but it also introduces additional parameters to the model.\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.decoder.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of Decoder parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "\n",
    "        Args:\n",
    "            - non_embedding (bool): If True, excludes the position embeddings count from the total. Default is True.\n",
    "\n",
    "        Returns:\n",
    "            - int: The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.decoder.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model.\n",
    "\n",
    "        Args:\n",
    "            - module (torch.nn.Module): The module of the model to be initialized.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # init Linear layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # bias initialization if necessary\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # init Embedding layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - idx (torch.Tensor): The input tensor to the forward pass.\n",
    "            - tgt_mask (torch.Tensor, optional): The mask tensor to ignore padding, size (B, 1, 1, T).\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "            - list: all layers of decoder attentions weights.\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "        tok_emb = self.decoder.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.decoder.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.decoder.drop(tok_emb + pos_emb) # Addition of input embd + positional encoding\n",
    "\n",
    "        decoder_attn_all = []\n",
    "        for block in self.decoder.h:\n",
    "            x, encoder_attn = block(x, tgt_mask)\n",
    "            decoder_attn_all.append(encoder_attn)\n",
    "            \n",
    "        x = self.decoder.ln_f(x)\n",
    "        return self.lm_head(x), decoder_attn_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b02cc7-ac93-4863-898b-eeb008a56f18",
   "metadata": {},
   "source": [
    "### MiniGpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6efea9-b482-4d75-984e-18a02d0036b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the Transformer model, which includes both the encoder and decoder.\n",
    "\n",
    "    The Transformer is a sequence transduction model that uses attention mechanisms.\n",
    "    It is primarily used in tasks that require understanding of context or relationships among words in a text.\n",
    "\n",
    "    Attributes:\n",
    "        - decoder (Decoder): The transformer decoder.\n",
    "        - config (:obj:`Config`): The configuration object for the transformer model.\n",
    "\n",
    "    Args:\n",
    "        - config (:obj:`Config`): The configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.decoder = Decoder(config)\n",
    "\n",
    "    def forward(self, tgt, tgt_tgt=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - tgt (torch.Tensor): The input tensor to the decoder.\n",
    "            - tgt_mask (torch.Tensor): The target_masks tensor to the decoder, size (B, 1, 1, T).\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "            - torch.Tensor: The loss tensor calculated on the basis of the decoder's output and target tensor.\n",
    "        \"\"\"\n",
    "        assert tgt.dim() == 2, \"tgt should be 2D (B, S)\"\n",
    "        if tgt_mask is not None:\n",
    "            assert tgt_mask.dim() == 4, \"tgt_mask should be 4D (B, 1, 1 S)\"\n",
    "\n",
    "        # tgt_shifted = tgt[:, :-1] # Shifted target\n",
    "        tgt_shifted = tgt\n",
    "        output, _, = self.decoder(tgt_shifted, tgt_mask[:, :, :-1, :-1] if tgt_mask else None)\n",
    "\n",
    "        if tgt_tgt is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = output.shape\n",
    "            output = output.view(B*T, C)\n",
    "            tgt_tgt = tgt_tgt.view(B*T)\n",
    "            # Calculate the loss, using both the output and the target\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=self.config.tokenizer.PAD_IDX) # Ignore padding tokens\n",
    "            loss = loss_fct(output, tgt_tgt)\n",
    "            \n",
    "        return output, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.config.block_size:]\n",
    "            # get the predictions\n",
    "            logits, decoder_attn_all = self.decoder(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        \n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"\n",
    "        Saves the current state of the model to a file.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the file where the model state should be saved.\n",
    "        \"\"\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"\n",
    "        Loads the model state from a file.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the file from where the model state should be loaded.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the specified file does not exist.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError(f\"{path} does not exist.\")\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63bf7f-d62b-425a-b572-e4c7175ebc92",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f985eab2-93bd-42dd-88f5-8d363341f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig(\n",
      "\tself.tokenizer=<transformer_implementation.Tokenizer.Tokenizer object at 0x000002028C917D90>,\n",
      "\tself.block_size=256,\n",
      "\tself.batch_size=12,\n",
      "\tself.n_layer=6,\n",
      "\tself.n_head=8,\n",
      "\tself.n_embd=256,\n",
      "\tself.dropout=0.1,\n",
      "\tself.bias=False,\n",
      "\tself.device='cuda',\n",
      "\tself.learning_rate=0.0003,\n",
      "\tself.max_epochs=100,\n",
      "\tself.max_iters=2500,\n",
      "\tself.eval_iters=250,\n",
      "\tself.train_data_size=30000,\n",
      "\tself.visualize=False,\n",
      "\tself.vocab_size=100277,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# init config\n",
    "config = TransformerConfig(\n",
    "    tokenizer,\n",
    "    block_size = 256,\n",
    "    batch_size = 12,\n",
    "    n_layer = 6, # 6,\n",
    "    n_head = 8,\n",
    "    n_embd = 256,\n",
    "    max_epochs=100,\n",
    "    train_data_size = 30000, # batch * 500 iters\n",
    "    max_iters = 2500,\n",
    "    eval_iters = 250,\n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77fbb2-162b-4589-8ca0-98d80e539bb1",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e486c67-c27d-4c8d-b3b7-da3057d7fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "\n",
    "    Args:\n",
    "        - dataset (Dataset): a dataset from HuggingFace datasets library.\n",
    "        - tokenizer (Tokenizer): The custom tiktoken tokenizer used to encode sequences.\n",
    "        - block_size (int): The maximum sequence length for tokenization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer, block_size):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a tokenized example from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            - index (int): the index of the example to fetch.\n",
    "\n",
    "        Returns:\n",
    "            - Dict: dictionary with keys 'inputs', 'inputs_mask', 'targets' , 'targets_mask' and 'translation', containing tokenized input,\n",
    "            target sequences and original translation.\n",
    "        \"\"\"\n",
    "        x = self.dataset[index:index+self.block_size]\n",
    "        y = self.dataset[index+1:index+self.block_size+1]\n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int :\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            - int: the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "class DataLoaderFactory():\n",
    "    \"\"\"\n",
    "    A class to instantiate PyTorch DataLoaders for different splits of a HuggingFace Dataset.\n",
    "\n",
    "    Args:\n",
    "        - block_size (int): The maximum sequence length for tokenization.\n",
    "        - batch_size (int): The batch size for DataLoader.\n",
    "        - tokenizer (Tokenizer): a tokenizer that has an encode method.\n",
    "        - device (str): 'cpu' or 'cuda', depending on whether we use CPU or GPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_size, batch_size, tokenizer, device):\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "        self.text, self.train_data, self.val_data = self.__load_data()\n",
    "        \n",
    "        self.dataloader_train = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "        self.dataloader_val = DataLoader(self.val_data, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    def __load_data(self):\n",
    "        with open('./data/input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        dataset = torch.tensor(tokenizer.encoder.encode(text), dtype=torch.long)\n",
    "        n = int(0.9*len(dataset)) # first 90% will be train, rest val\n",
    "        train_data = TranslationDataset(dataset[:n], self.tokenizer, self.block_size)\n",
    "        val_data = TranslationDataset(dataset[n:], self.tokenizer, self.block_size)\n",
    "        return text, train_data, val_data\n",
    "        \n",
    "    def __len__(self) -> int :\n",
    "        \"\"\"\n",
    "        Print the length of each dataset and returns the length of all datasets.\n",
    "\n",
    "        Returns:\n",
    "            - int: the length of all dataset (train + val + test).\n",
    "        \"\"\"\n",
    "        print(\"\\033[95m\\033[1m\\033[4mNumber of data by datasets splits\\033[0m\")\n",
    "        print(f\"Train\\t\\t: {len(self.train_data)}\\t-> {len(self.train_data)/self.batch_size}\")\n",
    "        print(f\"Validation\\t: {len(self.val_data)}\\t\\t-> {len(self.val_data)/self.batch_size}\")\n",
    "        total = len(self.train_data) + len(self.val_data)\n",
    "        print(f\"Total\\t\\t: {total}\")\n",
    "        return total\n",
    "\n",
    "    def get_batch(self, split):\n",
    "        \"\"\"\n",
    "        Choose the correct DataLoader and yield batches from it.\n",
    "\n",
    "        Args:\n",
    "            - split (str): 'train', 'val' or 'test'.\n",
    "\n",
    "        Yields:\n",
    "            - Dict: a dictionary with keys 'inputs', 'targets' and 'translation', containing a batch of tokenized input,\n",
    "            target sequences and original translation.\n",
    "        \"\"\"\n",
    "        # choose the correct dataloader\n",
    "        if split == 'train':\n",
    "            dataloader = self.dataloader_train\n",
    "        else:\n",
    "            dataloader = self.dataloader_val\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # Move tensors to device\n",
    "            batch_on_device = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            yield batch_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5014acde-f098-467b-a620-acc251e9ff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m\u001b[4mNumber of data by datasets splits\u001b[0m\n",
      "Train\t\t: 271646\t-> 22637.166666666668\n",
      "Validation\t: 30183\t\t-> 2515.25\n",
      "Total\t\t: 301829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "301829"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "dataset = DataLoaderFactory(config.block_size, config.batch_size, tokenizer, config.device)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d5ca9a-f7a4-4f05-82a7-9f5ae7eb7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dataset.get_batch('train')\n",
    "next_batch = next(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2e61202-b384-429c-9e91-6501458c20c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_batch['x'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b8d019a-d1c4-4ec3-a57e-8a1f3c31327b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   11,   477,   832,  ...,   279,   198, 65227],\n",
       "        [  556,  2751,   198,  ...,    11, 14174,   301],\n",
       "        [ 3021,  4250,  4295,  ...,  2795,  4400,    25],\n",
       "        ...,\n",
       "        [  198,    76,   372,  ...,   279, 59213,  3647],\n",
       "        [  305,   552,  1461,  ...,    11,   856, 38031],\n",
       "        [  872, 11427,  8703,  ..., 53120,   287,   198]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_batch['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bfc46-f914-4ce3-b53c-c0bd0b697a79",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fd50550-8554-4ff8-8ad4-50181c5549f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, dataset, config, splits = ['train', 'val']):\n",
    "    \"\"\"\n",
    "    This function estimates the loss of a model on specified data splits without performing backpropagation.\n",
    "    It sets the model to evaluation mode, iterates over the data splits and calculates the average loss.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): The model for which loss needs to be estimated.\n",
    "        dataset (CustomDataset): The dataset used for estimation. It should provide a 'get_batch' method.\n",
    "        config (Config): The configuration object defining the number of evaluation iterations.\n",
    "        splits (list[str]): List of the names of data splits to use for estimation.\n",
    "\n",
    "    Returns:\n",
    "        out (dict): A dictionary with split names as keys and corresponding average loss as values.\n",
    "    \"\"\"\n",
    "    # Create an empty dictionary to store the average loss for each split\n",
    "    out = {}\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Loop over the data splits\n",
    "    for split in splits:\n",
    "        # Initialize a tensor to store the losses for each iteration in the current split\n",
    "        losses = torch.zeros(config.eval_iters)\n",
    "\n",
    "        # Get a batch iterator for the current split\n",
    "        batch = dataset.get_batch(split)\n",
    "        \n",
    "        # Initialize a progress bar for the inner loop\n",
    "        inner_loop = tqdm(range(config.eval_iters), desc=f\"Evaluation - {split}\", leave=False)\n",
    "\n",
    "        # Start the inner loop\n",
    "        for k in inner_loop:\n",
    "            # Sample a new batch of data\n",
    "            n_batch = next(batch)\n",
    "            X = n_batch['x']\n",
    "            Y = n_batch['y']\n",
    "            \n",
    "            # Evaluate the loss for the current batch\n",
    "            logits, loss = model(X, Y)\n",
    "\n",
    "            # Store the current loss\n",
    "            losses[k] = loss.item()\n",
    "            \n",
    "        # Calculate and store the mean loss for the current split\n",
    "        out[split] = losses.mean()\n",
    "\n",
    "    # Return the dictionary with the average losses\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89467cb-b399-4906-b89c-fb9d554e7dd4",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c7c5a8-3fd1-4861-83d3-713d88291dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Decoder parameters: 30.39M\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = MiniGPT(config)\n",
    "model.train()\n",
    "# Use nn.DataParallel to wrap the model.\n",
    "# This will distribute the operations to multiple GPUs if they are available.\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f428744-3d2f-4df9-8414-56e75ae5acbd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97e48a-b435-4d6e-97b7-416e67aef6f7",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cae882c-957d-4227-a3df-ccb75c5f3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate warmup and then decay, which is a standard practice in Transformer training.\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\" \"warm-up, then decay\" strategy. \"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def training_loop(model, optimizer, dataset, config, saved_path = \"./out/minigpt_state_dict.pth\"):\n",
    "    \"\"\"\n",
    "    This function performs the training loop for the given transformer model. It trains the model using the provided \n",
    "    optimizer and dataset according to the specified configuration. \n",
    "\n",
    "    Args:\n",
    "        - model (Transformer): The transformer model to be trained.\n",
    "        - optimizer (torch.optim.Optimizer): The optimizer used to update the model's parameters.\n",
    "        - dataset (CustomDataset): The dataset used for training and validation. It should provide a 'get_batch' method.\n",
    "        - config (Config): The configuration object that defines parameters like max_iters.\n",
    "\n",
    "    Returns:\n",
    "        - losses_list (dict): A dictionary that contains the training and validation losses per evaluation step.\n",
    "    \"\"\"\n",
    "    # This is the total number of training steps,\n",
    "    # which is typically the number of training examples times the number of epochs.\n",
    "    num_training_steps = config.train_data_size * config.max_epochs\n",
    "    # Choose warmup_steps such that it's 1% of total steps\n",
    "    num_warmup_steps = num_training_steps // 100\n",
    "    # You can add this after defining your optimizer\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    # Initialize a dictionary to keep track of training and validation losses\n",
    "    losses_list = {\n",
    "        'train': [],\n",
    "        'val': [],\n",
    "    }\n",
    "    \n",
    "    # Initialize minimum loss with a high value and the iteration number where the minimum was observed\n",
    "    iter_saved = 0\n",
    "\n",
    "    # init early stop\n",
    "    best_loss = float('inf')\n",
    "    val_loss = float('inf')\n",
    "    # This is the number of epochs with no improvement after which training will be stopped.\n",
    "    patience = 3\n",
    "    # This is used to keep track of the number of epochs without improvement.\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Initialize a progress bar for the outer training loop\n",
    "    outer_loop = tqdm(range(config.max_epochs), desc=\"Train loss nan, Val loss nan, Saved nan\", leave=True)\n",
    "\n",
    "    # Start the training loop\n",
    "    for epochs in outer_loop:\n",
    "        # Initialize a batch of data from the 'train' part of the dataset\n",
    "        iter_loop = tqdm(range(config.max_iters), leave=False)\n",
    "        batch = dataset.get_batch('train')\n",
    "        for iter in iter_loop:\n",
    "            # Sample a new batch of data\n",
    "            n_batch = next(batch)\n",
    "            xb = n_batch['x']\n",
    "            yb = n_batch['y']\n",
    "        \n",
    "            # Evaluate the loss\n",
    "            logits, loss = model(xb, yb)\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "            # update sheduler\n",
    "            scheduler.step()\n",
    "\n",
    "        ############\n",
    "        # Evaluation\n",
    "        ############\n",
    "        # Estimate the losses for both training and validation datasets\n",
    "        losses = estimate_loss(model, dataset, config)\n",
    "        # Return the model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Record the estimated losses\n",
    "        losses_list['train'].append(losses['train'])\n",
    "        losses_list['val'].append(losses['val'])\n",
    "                    \n",
    "        # Get the latest losses\n",
    "        last_loss_train = losses_list['train'][-1]\n",
    "        last_loss_val = losses_list['val'][-1]\n",
    "        \n",
    "        ############\n",
    "        # early stop\n",
    "        ############\n",
    "        val_loss = losses['val']\n",
    "        if val_loss < best_loss:\n",
    "            torch.save(model.module.state_dict(), saved_path)\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            iter_saved = epochs\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Update the description of the progress bar\n",
    "        outer_loop.set_description(f\"Train loss {last_loss_train:.4f}, Val loss {last_loss_val:.4f}, Saved {iter_saved}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    # Return the list of losses\n",
    "    return losses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bb80bbc-39c7-4cbb-869a-9557cb968411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc741ccd165476a8038cc89f783429b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train loss nan, Val loss nan, Saved nan:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa0087cd11547c0bb8475d63dd9a3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n",
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n",
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n",
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n",
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n",
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n",
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n",
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n",
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n",
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n",
      "output.size()=torch.Size([12, 256, 100277])\n",
      "tgt_tgt.size()=torch.Size([12, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:127\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m}\u001b[49m)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [256] at entry 0 and [78] at entry 10",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses_list \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 60\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, optimizer, dataset, config, saved_path)\u001b[0m\n\u001b[0;32m     57\u001b[0m batch \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m iter_loop:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Sample a new batch of data\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     n_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(batch)\n\u001b[0;32m     61\u001b[0m     xb \u001b[38;5;241m=\u001b[39m n_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     62\u001b[0m     yb \u001b[38;5;241m=\u001b[39m n_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[7], line 106\u001b[0m, in \u001b[0;36mDataLoaderFactory.get_batch\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader_val\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# Move tensors to device\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     batch_on_device \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m batch_on_device\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:629\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 629\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[0;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\autograd\\profiler.py:495\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_enter_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit:\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses_list = training_loop(model, optimizer, dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96236360-9fa5-42d4-9beb-e802ba9c9cdf",
   "metadata": {},
   "source": [
    "### Plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac7486a-daa4-439a-ba76-d33dd9c61589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "plot_losses(losses_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c737bd4-e48a-489e-b8b4-276da31c5b72",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c786cd2-80d4-47fb-81c2-88c5cd31cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.full((src.size(0), 1), self.config.tokenizer.BOS_IDX).long().to(src.device)\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47d53b-ce30-4cfb-92de-b5979258cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
