{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012f19fa-d88b-4356-b1cb-fd2dce3f7bbb",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359954e4-ec88-4389-ad02-998016bb0311",
   "metadata": {},
   "source": [
    "## Config\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4be0056-d6ee-4416-aa13-69ef8593a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import tiktoken\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e811e4-75c2-4774-ba77-5b5347c6313d",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997eff3a-807b-4131-8848-26ae66f1854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887222e1-02e2-457e-af0b-5616dcbef351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    \"\"\"A tokenizer class for encoding/decoding text sequences.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor method to initialize special token indices and tokenizer encoding. \n",
    "        \"\"\"\n",
    "        # Initialize special token indices\n",
    "        self.BOS_IDX: int = 100264  # Index for the Beginning of Sentence token\n",
    "        self.EOS_IDX: int = 100265  # Index for the End of Sentence token\n",
    "        self.PAD_IDX: int = 100266  # Index for the Padding token\n",
    "\n",
    "        # Initialize base encoding from tiktoken\n",
    "        cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "        # Initialize the tokenizer's encoding with special tokens added\n",
    "        self.encoder = tiktoken.Encoding(\n",
    "            name=\"cl100k_bep\", # Name for the encoder with BOS, EOS, and PAD tokens added\n",
    "            pat_str=cl100k_base._pat_str, # Pattern string from the base encoding\n",
    "            mergeable_ranks=cl100k_base._mergeable_ranks, # Mergeable ranks from the base encoding\n",
    "            special_tokens={\n",
    "                **cl100k_base._special_tokens, # Special tokens from the base encoding\n",
    "                \"<|bos|>\": self.BOS_IDX,  # BOS token\n",
    "                \"<|eos|>\": self.EOS_IDX,  # EOS token\n",
    "                \"<|pad|>\": self.PAD_IDX,  # PAD token\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Method to return the size of the vocabulary in the tokenizer's encoding.\n",
    "\n",
    "        Returns:\n",
    "            int: The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        return self.encoder.n_vocab\n",
    "\n",
    "\n",
    "    def sequence_padding(self, sequence, max_size: int = 512, device: str = \"cpu\") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Method to add BOS/PAD/EOS special tokens and ensure the sequence length is within the maximum size.\n",
    "\n",
    "        Args:\n",
    "            sequence (torch.Tensor or list): The input sequence.\n",
    "            max_size (int, optional): The maximum allowed size for the sequence. Defaults to 512.\n",
    "            device (str, optional): The device where the tensors will be allocated. Defaults to \"cpu\".\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The processed sequence with special tokens added and length limited.\n",
    "        \"\"\"\n",
    "        assert max_size > 2, f\"[max_size]: {max_size} should be greater than 2\"\n",
    "        # Ensure the sequence is a torch tensor\n",
    "        tensor_sequence = torch.tensor(sequence, dtype=torch.long).to(device) if not torch.is_tensor(sequence) else sequence.to(device)\n",
    "\n",
    "        # Limit the sequence length within (max_size - 2) where 2 corresponding to bos and eos tags\n",
    "        cutted_sequence_size = max(0, min(max_size - 2, tensor_sequence.size()[0]))\n",
    "        tensor_sequence = tensor_sequence[:cutted_sequence_size]\n",
    "        \n",
    "        # Add BOS token\n",
    "        tensor_sequence = torch.cat([torch.tensor([self.BOS_IDX], dtype=torch.long, device=device), tensor_sequence], dim=0)\n",
    "\n",
    "        # Calculate the padding size\n",
    "        padding_size = max_size - tensor_sequence.size()[0] - 1 # expected size - current size - EOS tag\n",
    "\n",
    "        # Create PAD tensor\n",
    "        pad_tensor = torch.full((padding_size,), self.PAD_IDX, dtype=torch.long, device=device)\n",
    "\n",
    "        # Add PAD and EOS tokens\n",
    "        tensor_sequence = torch.cat([tensor_sequence, pad_tensor, torch.tensor([self.EOS_IDX], dtype=torch.long, device=device)], dim=0)\n",
    "        \n",
    "        return tensor_sequence\n",
    "    \n",
    "    def sequence_cleaner(self, sequence):\n",
    "        \"\"\" Method used to remove BOS/PAD/EOS special tokens \"\"\"\n",
    "        # Checking tensor format\n",
    "        list_sequence = sequence.tolist() if torch.is_tensor(sequence) else sequence\n",
    "        def check_special(number):\n",
    "            return number not in [self.BOS_IDX, self.EOS_IDX, self.PAD_IDX]\n",
    "        return list(filter(check_special, list_sequence))\n",
    "\n",
    "    def tokenize(self, sequence, device=\"cpu\") -> list:\n",
    "        \"\"\"\n",
    "        Method to generate a str list of separated tokens token.\n",
    "\n",
    "        Args:\n",
    "            sequence (torch.Tensor or list): The input sequence.\n",
    "            device (str, optional): The device where the tensors will be allocated. Defaults to \"cpu\".\n",
    "\n",
    "        Returns:\n",
    "            list: The processed sequence converted in a list of tokens in string format.\n",
    "        \"\"\"\n",
    "        # Ensure the sequence is a torch tensor\n",
    "        tensor_sequence = torch.tensor(sequence, dtype=torch.long).to(device) if not torch.is_tensor(sequence) else sequence.to(device)\n",
    "        # create batch of idx tokens\n",
    "        tensor_sequence = tensor_sequence.unsqueeze(0).T\n",
    "        # decode all batch to recreate list of separated tokens \n",
    "        tensor_sequence = self.encoder.decode_batch(tensor_sequence.detach().tolist())\n",
    "        return tensor_sequence\n",
    "\n",
    "    def tokenize_from_str(self, sequence, device=\"cpu\") -> list:\n",
    "        return self.tokenize(self.encoder.encode(sequence), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321a98a8-144b-4815-956a-b866c679fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7d7325-63c2-4c89-887d-1d28c6288293",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    \"\"\"Data class that stores the configuration for a Transformer model.\n",
    "\n",
    "    Attributes:\n",
    "        - tokenizer: An instance of the Tokenizer class.\n",
    "        - block_size (int): Number of tokens in each sequence. Defaults to 512.\n",
    "        - batch_size (int): Number of sequences in each batch. Defaults to 12.\n",
    "        - vocab_size (int): Total size of the tokenizer vocabulary. It is set to the size of the tokenizer vocabulary.\n",
    "        - n_layer (int): Number of transformer encoder and decoder blocks (N). Defaults to 1.\n",
    "        - n_head (int): Number of heads in each attention block. Defaults to 2.\n",
    "        - n_embd (int): Token embedding size. This is from the original Transformer paper. Defaults to 128.\n",
    "        - dropout (float): Dropout rate to use in the Transformer model. Defaults to 0.1.\n",
    "        - bias (bool): Indicates whether to use bias in Linears and LayerNorms.\n",
    "            If True, bias is used similar to GPT-2.\n",
    "            If False, it is a bit better and faster. Defaults to False.\n",
    "        - device (str): The device to run the model on. Defaults to 'cpu'. 'cuda' is used if a GPU is available.\n",
    "        - learning_rate (float): Learning rate for the model optimization. Defaults to 3e-4.\n",
    "        - eval_interval (int): Number of steps between each validation dataset. Defaults to 1.\n",
    "        - eval_iters (int): Number of validation epochs. Defaults to 20.\n",
    "        - visualize (bool): Define if we want to get the attention scores.\n",
    "    \"\"\"\n",
    "    tokenizer: any\n",
    "    block_size: int = 512\n",
    "    batch_size: int = 12\n",
    "    n_layer: int = 1 # 6\n",
    "    n_head: int = 2 # 8\n",
    "    n_embd: int = 128 # 512\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = False # True:\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = 'cpu'\n",
    "    learning_rate = 3e-4\n",
    "    eval_interval: int = 1\n",
    "    eval_iters: int = 20 # 200\n",
    "    visualize: bool = False\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Returns the total size of the tokenizer vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            int: The size of the tokenizer vocabulary.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a4b719-f507-46bc-b389-892b9efd52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb57475-96fe-42c1-9568-06926c2c8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "\n",
    "    Args:\n",
    "        - dataset (Dataset): a dataset from HuggingFace datasets library.\n",
    "        - tokenizer (Tokenizer): The custom tiktoken tokenizer used to encode sequences.\n",
    "        - block_size (int): The maximum sequence length for tokenization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer, block_size):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a tokenized example from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            - index (int): the index of the example to fetch.\n",
    "\n",
    "        Returns:\n",
    "            - Dict: dictionary with keys 'inputs', 'targets' and 'translation', containing tokenized input,\n",
    "            target sequences and original translation.\n",
    "        \"\"\"\n",
    "        translation = self.dataset[index]['translation']\n",
    "        encode = self.tokenizer.encoder.encode\n",
    "        inputs = self.tokenizer.sequence_padding(encode(translation['en']), self.block_size) # source language\n",
    "        targets = self.tokenizer.sequence_padding(encode(translation['fr']), self.block_size) # target language\n",
    "        return {'inputs': inputs, 'targets': targets, 'translation': translation}\n",
    "\n",
    "    def __len__(self) -> int :\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            - int: the length of the dataset.\n",
    "        \"\"\"\n",
    "        return self.dataset.num_rows\n",
    "\n",
    "\n",
    "class DataLoaderFactory():\n",
    "    \"\"\"\n",
    "    A class to instantiate PyTorch DataLoaders for different splits of a HuggingFace Dataset.\n",
    "\n",
    "    Args:\n",
    "        - block_size (int): The maximum sequence length for tokenization.\n",
    "        - batch_size (int): The batch size for DataLoader.\n",
    "        - tokenizer (Tokenizer): a tokenizer that has an encode method.\n",
    "        - device (str): 'cpu' or 'cuda', depending on whether we use CPU or GPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_size, batch_size, tokenizer, device):\n",
    "        self.train_data = TranslationDataset(load_dataset(\"wmt14\", \"fr-en\", split=\"train[:500000]\"), tokenizer, block_size)\n",
    "        self.val_data = TranslationDataset(load_dataset(\"wmt14\", \"fr-en\", split=\"validation\"), tokenizer, block_size)\n",
    "        self.test_data = TranslationDataset(load_dataset(\"wmt14\", \"fr-en\", split=\"test\"), tokenizer, block_size)\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.dataloader_train = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "        self.dataloader_val = DataLoader(self.val_data, batch_size=batch_size, shuffle=True)\n",
    "        self.dataloader_test = DataLoader(self.test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int :\n",
    "        \"\"\"\n",
    "        Print the length of each dataset and returns the length of all datasets.\n",
    "\n",
    "        Returns:\n",
    "            - int: the length of all dataset (train + val + test).\n",
    "        \"\"\"\n",
    "        print(\"\\033[95m\\033[1m\\033[4mNumber of data by datasets splits\\033[0m\")\n",
    "        print(f\"Train\\t\\t: {len(self.train_data)}\")\n",
    "        print(f\"Validation\\t: {len(self.val_data)}\")\n",
    "        print(f\"Test\\t\\t: {len(self.test_data)}\")\n",
    "        total = len(self.train_data) + len(self.val_data) + len(self.test_data)\n",
    "        print(f\"Total\\t\\t: {total}\")\n",
    "        return total\n",
    "\n",
    "    def get_batch(self, split):\n",
    "        \"\"\"\n",
    "        Choose the correct DataLoader and yield batches from it.\n",
    "\n",
    "        Args:\n",
    "            - split (str): 'train', 'val' or 'test'.\n",
    "\n",
    "        Yields:\n",
    "            - Dict: a dictionary with keys 'inputs', 'targets' and 'translation', containing a batch of tokenized input,\n",
    "            target sequences and original translation.\n",
    "        \"\"\"\n",
    "        # choose the correct dataloader\n",
    "        if split == 'train':\n",
    "            dataloader = self.dataloader_train\n",
    "        elif split == 'val':\n",
    "            dataloader = self.dataloader_val\n",
    "        else:\n",
    "            dataloader = self.dataloader_test\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # Separate the 'translation' from the rest of the batch\n",
    "            translation = batch.pop('translation')\n",
    "    \n",
    "            # Move tensors to device\n",
    "            batch_on_device = {k: v.to(self.device) for k, v in batch.items()}\n",
    "    \n",
    "            # Add 'translation' back into the batch\n",
    "            batch_on_device['translation'] = translation\n",
    "    \n",
    "            yield batch_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b64c8f-7fd6-456f-b6ce-afa6c25f27fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    }
   ],
   "source": [
    "config = TransformerConfig(tokenizer)\n",
    "dataset = DataLoaderFactory(config.block_size, config.batch_size, tokenizer, config.device)\n",
    "# dataset.DataSize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d0e6a1-4b17-4955-9600-b5cac119e801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataLoaderFactory.get_batch at 0x0000024032CC3540>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = dataset.get_batch('train')\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020363b6-98ed-4d32-a522-41d78b804600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_batch['inputs']=tensor([[100264,     40,    690,  ..., 100266, 100266, 100265],\n",
      "        [100264,    644,   5369,  ..., 100266, 100266, 100265],\n",
      "        [100264,   1687,    527,  ..., 100266, 100266, 100265],\n",
      "        ...,\n",
      "        [100264,   4897,    374,  ..., 100266, 100266, 100265],\n",
      "        [100264,  24901,     11,  ..., 100266, 100266, 100265],\n",
      "        [100264,     40,   1053,  ..., 100266, 100266, 100265]])\n",
      "n_batch['targets']=tensor([[100264,  30854,  91507,  ..., 100266, 100266, 100265],\n",
      "        [100264,   1951,   5636,  ..., 100266, 100266, 100265],\n",
      "        [100264,   2244,     11,  ..., 100266, 100266, 100265],\n",
      "        ...,\n",
      "        [100264,     34,  17771,  ..., 100266, 100266, 100265],\n",
      "        [100264,   3976,   4150,  ..., 100266, 100266, 100265],\n",
      "        [100264,  30854,    348,  ..., 100266, 100266, 100265]])\n",
      "n_batch['translation']={'en': ['I will say very precisely why.', 'In addition, a ceiling ensures that eventually the fairly closed markets for classification and statutory tasks will become more open and a more level playing field will be created.', \"We are forced to note, however, that, with regard to Iraq, voices are currently divided between those who are opposed to any kind of military action or threat; those who, alongside the Americans, seem determined to take military action in the next few days and those, myself included, who are a majority within the Group of the European People's Party (Christian Democrats) and European Democrats, who ask that we only resort to war when we have exhausted all diplomatic means of persuading Saddam Hussein to give in and of forcing him to abandon all his projects for the development and production of weapons of mass destruction.\", 'I think it is a proposal which fits in perfectly with the Barcelona summit and should be presented before that summit.', 'Many of the problems apparent in the legislation at first reading have been substantially resolved by the Council at second reading.', 'At that time we also expressed regret - at least my group did - that the Council had decided not to table a human rights motion on Iran at the next meeting of the UN Commission on Human Rights.', 'I am delighted that the Commissioner has been able to inform us that a positive solution to the problem of creating a definition has now emerged.', 'There is another important point in this budget.', 'The Council puts that there and that has to be financed anyway.', 'That is something new and we will have to get used to it.', 'Finally, I should like to stress that the essential plank of the Commission’s strategy is to work with Member States to promote global solutions through the International Maritime Organisation - the IMO.', 'I would nonetheless like, as others have done, to touch upon the procedure, which is unclear and, above all, unwanted.'], 'fr': ['Je vais vous dire très précisément pourquoi.', \"De plus, l'existence d'un tel plafond entraînera à terme une plus grande ouverture des marchés assez fermés de la classification des tâches statutaires et l'apparition d'une base de concurrence plus égale.\", \"Or, force est bien de constater que, concernant l'Irak, les voix demeurent aujourd'hui discordantes entre ceux qui sont opposés à toute forme de menace ou d'action militaire ; ceux qui, aux côtés des Américains, semblent résolus à passer dans les jours à venir à cette action militaire et ceux, dont je suis, majoritaires au sein du groupe du parti populaire européen, qui demandent que l'on ne se résolve à la guerre que lorsque l'on aura épuisé tous les moyens diplomatiques pour faire céder Saddam Hussein et le contraindre à abandonner tous ses projets de développement et de production d'armes de destruction massive.\", 'À mon sens, cette proposition cadre parfaitement avec le Sommet de Barcelone et devrait être présentée avant la tenue de celui-ci.', 'Nombreux sont les problèmes de la législation apparus en première lecture à avoir été en grande partie résolus par le Conseil en deuxième lecture.', \"À l'époque, nous avons également - du moins mon groupe politique - regretté la décision du Conseil de ne pas présenter de résolution sur la situation des droits de l'homme en Iran lors de la prochaine réunion de la Commission des droits de l'homme des Nations Unies.\", 'Je suis ravi et juge extrêmement positif le fait que la commissaire ait pu nous confirmer que le problème de la définition est sur le point de trouver une issue positive.', 'Il est encore un point qui nous importe dans ce budget.', 'Le Conseil décide qu’il en va ainsi et il faut de toute façon assurer le financement.', \"C'est nouveau et nous allons devoir nous y habituer.\", 'Permettez-moi enfin de souligner le fait que l’un des principaux objectifs de la stratégie de la Commission consiste à collaborer avec les États membres en vue de promouvoir des solutions à l’échelon international par le biais de l’OMI, l’Organisation maritime internationale.', \"Je voudrais pourtant m'arrêter quelques instants, comme d'autres l'ont fait, sur la procédure peu claire et aussi - et surtout - peu souhaitable.\"]}\n"
     ]
    }
   ],
   "source": [
    "n_batch = next(batch)\n",
    "\n",
    "print(f\"{n_batch['inputs']=}\")\n",
    "print(f\"{n_batch['targets']=}\")\n",
    "print(f\"{n_batch['translation']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d09d529-2f21-4495-bf0f-36cc30640d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m\u001b[4mNumber of data by datasets splits\u001b[0m\n",
      "Train\t\t: 500000\n",
      "Validation\t: 3000\n",
      "Test\t\t: 3003\n",
      "Total\t\t: 506003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "506003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce6e629-fa89-4931-83d0-e62727dac927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"A Layer Normalization module with optional bias.\n",
    "\n",
    "    This implementation of Layer Normalization allows turning off the bias term,\n",
    "    which is not directly supported by PyTorch's layer normalization function.\n",
    "\n",
    "    Attributes:\n",
    "        weight: Learnable weights for the layer normalization. Initialized as an all ones tensor.\n",
    "        bias: Learnable biases for the layer normalization. Initialized as an all zeros tensor \n",
    "              if bias argument in constructor is True, otherwise it's set to None.\n",
    "\n",
    "    Args:\n",
    "        ndim: An integer for the dimension of the input vectors.\n",
    "        bias: A boolean which, if True, adds a learnable bias to the output.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, ndim: int, bias: bool):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            input (tensor): The input tensor to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            tensor: The normalized input tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357d6e13-1c75-485f-9625-29a174b44d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module.\n",
    "    \n",
    "    This module applies multi-head attention mechanism on the input sequence. This implementation doesn't apply mask over the attention scores.\n",
    "    \n",
    "    Attributes:\n",
    "        - n_head (int): Number of attention heads.\n",
    "        - n_embd (int): Embedding dimensionality.\n",
    "        - dropout (float): Dropout rate.\n",
    "        - q_attn (nn.Linear): Linear layer for the query projection.\n",
    "        - k_attn (nn.Linear): Linear layer for the key projection.\n",
    "        - v_attn (nn.Linear): Linear layer for the value projection.\n",
    "        - c_proj (nn.Linear): Linear layer for the output projection.\n",
    "        - attn_dropout (nn.Dropout): Dropout layer for the attention scores.\n",
    "        - resid_dropout (nn.Dropout): Dropout layer for the residual connection.\n",
    "        - flash (bool): Flag indicating if flash attention is available.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Constructor for the MultiHeadAttention class.\n",
    "        \n",
    "        Args:\n",
    "            - config: The configuration object containing model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # Params\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.visualize = config.visualize\n",
    "        \n",
    "        # INPUTS: query, key, value projections for all heads, but in a batch\n",
    "        self.q_attn = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.k_attn = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.v_attn = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # OUTPUT: output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # flash attention make GPU go br but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.tril(\n",
    "                    torch.ones(config.block_size, config.block_size)\n",
    "                ).view(1, 1, config.block_size, config.block_size)\n",
    "            )\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask: bool = None):\n",
    "        \"\"\"\n",
    "        Computes the scaled dot product attention.\n",
    "        \n",
    "        Args:\n",
    "            - q (Tensor): Query tensor of shape (batch_size, num_heads, seq_length, emb_dim).\n",
    "            - k (Tensor): Key tensor of shape (batch_size, num_heads, seq_length, emb_dim).\n",
    "            - v (Tensor): Value tensor of shape (batch_size, num_heads, seq_length, emb_dim).\n",
    "            - mask (bool, optional): Flag indicating whether to apply mask on the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            - y (Tensor): Output tensor after applying attention.\n",
    "        \"\"\"\n",
    "        # manual implementation of attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # Step 1 & 2: (MatMul) and (Scale)\n",
    "        if mask:\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1) # Step 3: Softmax\n",
    "        att_weights = att  # Save attention weights for visualization\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs) # Step 4: MatMul\n",
    "        return y, att_weights\n",
    "\n",
    "    def forward(self, q_x, k_x, v_x, mask = None):\n",
    "        \"\"\"\n",
    "        Forward pass for the MultiHeadAttention module.\n",
    "        \n",
    "        Args:\n",
    "            - q_x (Tensor): Input query tensor of shape (batch_size, seq_length, emb_dim).\n",
    "            - k_x (Tensor): Input key tensor of shape (batch_size, seq_length, emb_dim).\n",
    "            - v_x (Tensor): Input value tensor of shape (batch_size, seq_length, emb_dim).\n",
    "            - mask (bool, optional): Flag indicating whether to apply mask on the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            - y (Tensor): Output tensor after applying multi-head attention.\n",
    "        \"\"\"\n",
    "        B_q, T_q, C_q = q_x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        B_kv, T_kv, C_kv = k_x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.q_attn(q_x), self.k_attn(k_x), self.v_attn(v_x)\n",
    "        k = k.view(B_kv, T_kv, self.n_head, C_kv // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B_q, T_q, self.n_head, C_q // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B_kv, T_kv, self.n_head, C_kv // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash and not self.visualize:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=mask == True)\n",
    "            attn_weights = None\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            y, attn_weights = self.scaled_dot_product_attention(q, k, v)\n",
    "        y = y.transpose(1, 2).contiguous().view(B_q, T_q, C_q) # re-assemble all head outputs side by side # Step 5: Concatenate\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y)) # Step 6 : Linear\n",
    "        return y, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb156ccb-b9fd-4b62-b2a4-d2762cb71a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A position-wise Feed Forward Neural Network (FFNN) class for transformer models.\n",
    "    \n",
    "    The class implementing a position-wise FFNN.\n",
    "    The FFNN consists of two linear transformations with a GELU activation in between, \n",
    "    followed by a dropout for regularization.\n",
    "\n",
    "    Attributes:\n",
    "        - c_fc (nn.Linear): First fully connected layer.\n",
    "        - gelu (nn.GELU): GELU activation function layer.\n",
    "        - c_proj (nn.Linear): Second fully connected layer.\n",
    "        - dropout (nn.Dropout): Dropout layer for regularization.\n",
    "\n",
    "    Args:\n",
    "        - config (Config): A configuration object with attribute `n_embd`, `bias`, and `dropout`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): The input tensor to the forward pass.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output of the FFNN.\n",
    "        \"\"\"\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d648bfa6-2687-4d48-b9b1-be60061f48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements a single encoder block in the Transformer model.\n",
    "    \n",
    "    Each block consists of two sub-layers: a multi-head self-attention mechanism,\n",
    "    and a position-wise fully connected feed-forward network. There is a residual \n",
    "    connection around each of the two sub-layers, followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        - ln_1 (LayerNorm): Layer normalization before the multi-head attention layer.\n",
    "        - attn (MultiHeadAttention): Multi-head attention layer.\n",
    "        - ln_2 (LayerNorm): Layer normalization before the feed-forward network.\n",
    "        - ffw (FeedForward): Position-wise feed-forward network.\n",
    "\n",
    "    Args:\n",
    "        - config (Config): A configuration object with attribute `n_embd` and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.ffw = FeedForward(config)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): The input tensor to the forward pass.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor of the block.\n",
    "        \"\"\"\n",
    "        # MultiHeadAttention\n",
    "        x = self.ln_1(x)\n",
    "        x_attn, decoder_attn = self.attn(x, x, x)\n",
    "        x = x + x_attn\n",
    "        # FeedForward\n",
    "        x = x + self.ffw(self.ln_2(x))\n",
    "        return x, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b0c31f8-35a6-4f31-a0ca-39943bff97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements a single decoder block in the Transformer model.\n",
    "\n",
    "    Each block consists of three sub-layers: a multi-head self-attention mechanism,\n",
    "    a multi-head attention mechanism over the encoder's output, and a position-wise \n",
    "    fully connected feed-forward network. There is a residual connection around \n",
    "    each of the three sub-layers, followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        - ln_1 (LayerNorm): Layer normalization before the first multi-head attention layer.\n",
    "        - attn1 (MultiHeadAttention): First multi-head attention layer, with self-attention.\n",
    "        - ln_2 (LayerNorm): Layer normalization before the second multi-head attention layer.\n",
    "        - attn2 (MultiHeadAttention): Second multi-head attention layer, attends to encoder outputs.\n",
    "        - ln_3 (LayerNorm): Layer normalization before the feed-forward network.\n",
    "        - ffw (FeedForward): Position-wise feed-forward network.\n",
    "\n",
    "    Args:\n",
    "        config (Config): A configuration object with attribute `n_embd` and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn1 = MultiHeadAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn2 = MultiHeadAttention(config)\n",
    "        self.ln_3 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.ffw = FeedForward(config)\n",
    "\n",
    "    def forward(self, x, encoder_output) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): The input tensor to the forward pass.\n",
    "            - encoder_output (torch.Tensor): The output tensor from the last encoder block.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor of the block.\n",
    "        \"\"\"\n",
    "        # Masked MultiHeadAttention\n",
    "        x = self.ln_1(x)\n",
    "        x_attn, cross_attn = self.attn1(x, x, x, True)\n",
    "        x = x + x_attn\n",
    "        # MultiHeadAttention with q, k from encoder and x from decoder\n",
    "        x = self.ln_2(x)\n",
    "        x_attn, encoder_attn = self.attn2(x, encoder_output, encoder_output)\n",
    "        x = x + x_attn\n",
    "        # FeedForward\n",
    "        x = x + self.ffw(self.ln_3(x))\n",
    "        return x, encoder_attn, cross_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d415dfae-bc35-4f10-9fa0-cf182bf30a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements the encoder part of the Transformer model.\n",
    "\n",
    "    The encoder consists of several EncoderBlocks arranged in sequence.\n",
    "    The input first goes through an embedding layer followed by a positional encoding layer.\n",
    "    The output of this is then passed through each EncoderBlock in sequence.\n",
    "\n",
    "    Attributes:\n",
    "        - encoder (nn.ModuleDict): A dictionary of modules making up the transformer encoder.\n",
    "\n",
    "    Args:\n",
    "        - config (Config): A configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # Learned positional encoding:\n",
    "            # In this case, instead of using a fixed function to determine positional encoding,\n",
    "            # we initialize a tensor of positional encodings which gets updated during training via backpropagation.\n",
    "            # This method may potentially capture more complex position-related patterns than fixed positional encoding,\n",
    "            # but it also introduces additional parameters to the model.\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([EncoderBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        \n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # apply special scaled init to the residual projections, based on GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                # This initialization is used to preventing the variance of the outputs of each layer from exploding or vanishing\n",
    "                # during the forward pass through the network.\n",
    "                # Preventing \"vanishing/exploding gradients\" problem\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"Number of Encoder parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding: bool = True):\n",
    "        \"\"\"\n",
    "        Returns the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "\n",
    "        Args:\n",
    "            -non_embedding (bool, optional): If True, excludes the position embeddings count from the total (Default is True).\n",
    "\n",
    "        Returns:\n",
    "            - int: The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.encoder.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model. Proper weight initialization can help speed up the training process and improve model performance.\n",
    "\n",
    "        Args:\n",
    "            - module (nn.Module): The module of the model to be initialized.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # init Linear layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # bias initialization if necessary\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # init Embedding layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - idx (torch.Tensor): The input tensor to the forward pass.\n",
    "            - targets (torch.Tensor, optional): The target tensor against which the loss will be calculated.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "            - torch.Tensor: The loss tensor, if targets were provided. Otherwise, None.\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # pre-encoder block\n",
    "        tok_emb = self.encoder.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.encoder.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.encoder.drop(tok_emb + pos_emb) # Addition of input embd + positional encoding\n",
    "\n",
    "        # encoders block\n",
    "        encoder_attn_all = []\n",
    "        for block in self.encoder.h:\n",
    "            x, encoder_attn = block(x)\n",
    "            encoder_attn_all.append(encoder_attn)\n",
    "\n",
    "        return  self.encoder.ln_f(x), encoder_attn_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a0264d2-09df-4222-9cc3-945f5b4272af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the decoder part of the Transformer model.\n",
    "\n",
    "    The Decoder consists of several DecoderBlocks arranged in sequence. The input first goes through an embedding \n",
    "    layer followed by a positional encoding layer. The output of this is then passed through each DecoderBlock in \n",
    "    sequence.\n",
    "\n",
    "    Attributes:\n",
    "        - decoder (nn.ModuleDict): A dictionary of modules making up the transformer decoder.\n",
    "        - lm_head (nn.Linear): The final linear layer mapping from the embedding dimension to the vocabulary size.\n",
    "        - config (:obj:`Config`): The configuration object for the transformer model.\n",
    "\n",
    "    .. note:: The weight of the embedding layer and the linear layer are shared.\n",
    "\n",
    "    Args:\n",
    "        - config (:obj:`Config`): The configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.decoder = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # Learned positional encoding:\n",
    "            # In this case, instead of using a fixed function to determine positional encoding,\n",
    "            # we initialize a tensor of positional encodings which gets updated during training via backpropagation.\n",
    "            # This method may potentially capture more complex position-related patterns than fixed positional encoding,\n",
    "            # but it also introduces additional parameters to the model.\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.decoder.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "\n",
    "        Args:\n",
    "            - non_embedding (bool): If True, excludes the position embeddings count from the total. Default is True.\n",
    "\n",
    "        Returns:\n",
    "            - int: The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.decoder.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model.\n",
    "\n",
    "        Args:\n",
    "            - module (torch.nn.Module): The module of the model to be initialized.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # init Linear layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # bias initialization if necessary\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # init Embedding layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, enc_output = None):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - idx (torch.Tensor): The input tensor to the forward pass.\n",
    "            - enc_output (torch.Tensor): The output tensor from the encoder.\n",
    "            - targets (torch.Tensor, optional): The target tensor against which the loss will be calculated.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "        tok_emb = self.decoder.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.decoder.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.decoder.drop(tok_emb + pos_emb) # Addition of input embd + positional encoding\n",
    "\n",
    "        cross_attn_all = []\n",
    "        decoder_attn_all = []\n",
    "        for block in self.decoder.h:\n",
    "            x, encoder_attn, cross_attn = block(x, enc_output)\n",
    "            decoder_attn_all.append(encoder_attn)\n",
    "            cross_attn_all.append(cross_attn)\n",
    "            \n",
    "        x = self.decoder.ln_f(x)\n",
    "        return self.lm_head(x), decoder_attn_all, cross_attn_all\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d62cda5-5bc1-4bd9-a3e8-134bab8da831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the Transformer model, which includes both the encoder and decoder.\n",
    "\n",
    "    The Transformer is a sequence transduction model that uses attention mechanisms.\n",
    "    It is primarily used in tasks that require understanding of context or relationships among words in a text.\n",
    "\n",
    "    Attributes:\n",
    "        - encoder (Encoder): The transformer encoder.\n",
    "        - decoder (Decoder): The transformer decoder.\n",
    "        - config (:obj:`Config`): The configuration object for the transformer model.\n",
    "\n",
    "    Args:\n",
    "        - config (:obj:`Config`): The configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - src (torch.Tensor): The input tensor to the encoder.\n",
    "            - tgt (torch.Tensor): The input tensor to the decoder.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "            - torch.Tensor: The loss tensor calculated on the basis of the decoder's output and target tensor.\n",
    "        \"\"\"\n",
    "        enc_output, _ = self.encoder(src)\n",
    "        output, _, _ = self.decoder(tgt, enc_output)\n",
    "\n",
    "        # Calculate the loss, using both the output and the target\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        # The targets for the loss function are the input sequences shifted\n",
    "        tgt_tgt = tgt[:, 1:].contiguous()\n",
    "        loss = loss_fct(output.view(-1, output.size(-1)), tgt_tgt.view(-1))\n",
    "        return output, loss\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def translate_beam_search(self, src, beam_size=5):\n",
    "        enc_output, encoder_attn = self.encoder(src)\n",
    "        # initialize beam with start token\n",
    "        start_token = torch.zeros((src.size(0), 1)).long().to(src.device)  # initialize target tensor with zeros\n",
    "        start_score = torch.zeros((src.size(0), 1)).float().to(src.device)  # initialize target tensor with zeros\n",
    "        beams = [(start_token, start_score)]  # initialize beams\n",
    "        decoder_attentions = []\n",
    "        cross_attentions = []\n",
    "        for iter in range(self.config.block_size):\n",
    "            print(f\"\\r{iter+1}/{self.config.block_size}\", end=\"\")\n",
    "            new_beams = []\n",
    "            new_decoder_attentions = []\n",
    "            new_cross_attentions = []\n",
    "            for beam, score in beams:\n",
    "                output, dec_attention, cross_attention = self.decoder(beam, enc_output)\n",
    "                output = F.log_softmax(output[:, -1, :], dim=-1)  # use log_softmax for numerical stability\n",
    "\n",
    "                top_scores, top_indices = output.topk(beam_size, dim=-1)  # select topk predictions\n",
    "                \n",
    "                for i in range(beam_size):\n",
    "                    next_token = top_indices[:, i].unsqueeze(1)  # get next token\n",
    "                    next_score = top_scores[:, i].unsqueeze(1)  # get next score\n",
    "                    new_beam = torch.cat((beam, next_token), dim=-1)  # generate new beam\n",
    "                    new_score = score + next_score  # calculate new score\n",
    "                    new_beams.append((new_beam, new_score))  # append new beam to new beams\n",
    "                    new_decoder_attentions.append(torch.stack(dec_attention, dim=0))\n",
    "                    new_cross_attentions.append(torch.stack(cross_attention, dim=0))\n",
    "            # sort all candidates by score\n",
    "            beams = sorted(new_beams, key=lambda tup: tup[1].sum(), reverse=True)[:beam_size]  # keep top performing beams\n",
    "            decoder_attentions = sorted(new_decoder_attentions, key=lambda tup: tup.sum(), reverse=True)[:beam_size]\n",
    "            cross_attentions = sorted(new_cross_attentions, key=lambda tup: tup.sum(), reverse=True)[:beam_size]\n",
    "        return beams[0][0], dict(encoder_attn=encoder_attn, cross_attn=cross_attentions[0][0], decoder_attn=decoder_attentions[0][0])  # return the best sequence\n",
    "\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        model_state = {\n",
    "            'encoder_state': self.encoder.state_dict(),\n",
    "            'decoder_state': self.decoder.state_dict(),\n",
    "            'config': self.config\n",
    "        }\n",
    "        torch.save(model_state, path)\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError(f\"{path} does not exist.\")\n",
    "        model_state = torch.load(path)\n",
    "        \n",
    "        # Sanity check for configuration consistency\n",
    "        assert self.config == model_state['config'], \"Trying to loasequence_cleand model with different configuration\"\n",
    "\n",
    "        self.encoder.load_state_dict(model_state['encoder_state'])\n",
    "        self.decoder.load_state_dict(model_state['decoder_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e3d7e-cb08-4dce-aae1-41fc9b5aceca",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "882759cf-c512-4701-acce-0da8efdc6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(dataset, config):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(config.eval_iters)\n",
    "        batch = dataset.get_batch(split)\n",
    "        for k in range(config.eval_iters):\n",
    "            print(f\"\\rEvaluation - {split} : {k+1}/{config.eval_iters}\", end=\"\")\n",
    "            n_batch = next(batch)\n",
    "            X = n_batch['inputs']\n",
    "            Y = n_batch['targets']\n",
    "            print(f\"{X.size()=}\")\n",
    "            print(f\"{Y.size()=}\")\n",
    "            logits, loss = model(X, Y[:, :-1])\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fdf6a48-2441-4e6b-9070-fcf67b8751ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encoder parameters: 13.03M\n",
      "number of parameters: 13.10M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m\u001b[4mNumber of data by datasets splits\u001b[0m\n",
      "Train\t\t: 500000\n",
      "Validation\t: 3000\n",
      "Test\t\t: 3003\n",
      "Total\t\t: 506003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "506003"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = TransformerConfig(tokenizer)\n",
    "model = Transformer(config)\n",
    "dataset = DataLoaderFactory(config.block_size, config.batch_size, tokenizer, config.device)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd10ccbf-da9f-4704-a8dc-37e1b64218b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "model.train()\n",
    "model = model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "664a4db6-01e5-49ce-bff7-449598c5722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29636df1-1fa6-4b0e-b3fb-ed5f6fad567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - train : 1/20X.size()=torch.Size([12, 512])\n",
      "Y.size()=torch.Size([12, 512])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (6132) to match target batch_size (6120).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m config\u001b[38;5;241m.\u001b[39meval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m         batch \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 16\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m(dataset, config)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     18\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[19], line 43\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# The targets for the loss function are the input sequences shifted\u001b[39;00m\n\u001b[0;32m     42\u001b[0m tgt_tgt \u001b[38;5;241m=\u001b[39m tgt[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m---> 43\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_tgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, loss\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\transformer-LWcVpt7F\\Lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (6132) to match target batch_size (6120)."
     ]
    }
   ],
   "source": [
    "batch = dataset.get_batch('train')\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % config.eval_interval == 0:\n",
    "        losses = estimate_loss(dataset, config)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        batch = dataset.get_batch('train')\n",
    "        \n",
    "    # sample a batch of data\n",
    "    n_batch = next(batch)\n",
    "    xb = n_batch['inputs']\n",
    "    yb = n_batch['targets']\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0a083-cf00-4d87-99be-a7b11d457883",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"./out/transformer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897c151-f04d-4ba5-a19e-a32c12818f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(tokenizer, visualize=True)\n",
    "model = Transformer(config)\n",
    "# model.load_model(\"./out/transformer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538ea60-8b86-4a98-9c86-15cd7c4a9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentences, tokenizer, model):\n",
    "    # Tokenize sentences\n",
    "    tknzr = tokenizer.encoder\n",
    "    sequences = []\n",
    "    for sentence in sentences:\n",
    "        sequence = tokenizer.sequence_padding(tknzr.encode(sentence)).unsqueeze(dim=0)\n",
    "        sequences.append(sequence)\n",
    "    sequences = torch.cat(sequences, dim=0)\n",
    "\n",
    "    # Translate sentences\n",
    "    model_compiled.eval()\n",
    "    outputs, attn = model.translate_beam_search(sequences)\n",
    "\n",
    "    # Decode tokenized sentences\n",
    "    decode_output = []\n",
    "    for output in outputs:\n",
    "        output = tokenizer.sequence_cleaner(output)\n",
    "        decode_output += [tknzr.decode(output)]\n",
    "    \n",
    "    return decode_output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8b57a-1410-4f39-a560-fd1668fe7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, attentions = translate(['I am a teacher.'], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d8520-80e1-431a-bc3c-025dc8b7b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb611a-50a7-4a3c-98e0-b3d33dd871d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attentions['encoder_attn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4cf5e-578f-44b1-80cd-1b7348e871ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions['cross_attn'].unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d00f3e9-4cb4-4a96-b5e9-fbe185f6679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_encoder_attn = torch.stack(attentions['encoder_attn'], dim=0)\n",
    "tensor_cross_attn = attentions['cross_attn'].unsqueeze(0)\n",
    "tensor_decoder_attn = attentions['decoder_attn'].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8325fc44-755e-4c75-93a9-03e4bdbbb142",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cross_attn.size() # layer batch heads block_size block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cbcf5-6887-4fe4-ab9b-8aec70ff927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2a5fc-0740-46fa-b11c-45a02d3f5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a = tokenizer.tokenize_from_str('I am a teacher.')\n",
    "tokens_b = tokenizer.tokenize_from_str('Je suis un professeur.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a96e38-3403-4ad4-bbee-bd1ece6ea737",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a = tokens_a + [''] * 2\n",
    "tokens_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4609874-5273-4773-a570-3d559721e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd2f12-8cb1-400e-bc1a-76ff03fab43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_token = max(len(tokens_a), len(tokens_b)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c9bf7c0-b143-4dad-8077-ec1c42ba0622",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor_encoder_attn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tensor_encoder_attn \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_encoder_attn\u001b[49m[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m, :, \u001b[38;5;241m1\u001b[39m:max_len_token, \u001b[38;5;241m1\u001b[39m:max_len_token] \u001b[38;5;66;03m# layers, batch, heads, seq_len, seq_len\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tensor_encoder_attn\u001b[38;5;241m.\u001b[39msize()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor_encoder_attn' is not defined"
     ]
    }
   ],
   "source": [
    "tensor_encoder_attn = tensor_encoder_attn[:, 0:1, :, 1:max_len_token, 1:max_len_token] # layers, batch, heads, seq_len, seq_len\n",
    "tensor_encoder_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13656f-d642-470d-968c-f2626c031e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cross_attn = tensor_cross_attn[:, 0:1, :, 1:max_len_token, 1:max_len_token] # layers, batch, heads, seq_len, seq_len\n",
    "tensor_cross_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b4070-4d84-4da4-b1a2-1143342806c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_decoder_attn = tensor_decoder_attn[:, 0:1, :, 1:max_len_token, 1:max_len_token] # layers, batch, heads, seq_len, seq_len\n",
    "tensor_decoder_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49a167-b8f9-4a06-82ca-5b3ec493e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set your dimensions\n",
    "# num_layers = 1\n",
    "# batch_size = 1\n",
    "# num_heads = 2\n",
    "# seq_len = 512\n",
    "# num_tokens = 5\n",
    "# tensor_attn_weights2 = torch.rand(num_layers, batch_size, num_heads, seq_len, seq_len)\n",
    "# tensor_attn_weights2 = tensor_attn_weights2[:, :, :, :num_tokens, :num_tokens]\n",
    "# tensor_attn_weights2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b6d9c-02e6-4e50-a610-1ea24492c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_view(\n",
    "    encoder_attention=tensor_encoder_attn,\n",
    "    decoder_attention=tensor_decoder_attn,\n",
    "    cross_attention=tensor_cross_attn,\n",
    "    encoder_tokens=tokens_a,\n",
    "    decoder_tokens=tokens_b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14a8b2-fd6e-43b9-981b-eebae0dbc987",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(tensor_encoder_attn, tokens_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb925c-45fa-4d8c-a096-6a8c695ff730",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'a' == 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866f811-48a3-40f0-b509-f2a1a308c60e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee0a6b-7dbe-45ad-be47-74987676addc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db74cc-9562-47d3-bd70-05a2f9a57eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b67d3b-bc43-4685-a096-a0cba40cc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'a' == 'b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de34b5-094e-446c-8b08-97a18e2635d1",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136de36-6000-4aef-a7e1-c50b62ea9719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
