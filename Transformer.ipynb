{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012f19fa-d88b-4356-b1cb-fd2dce3f7bbb",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359954e4-ec88-4389-ad02-998016bb0311",
   "metadata": {},
   "source": [
    "## Config\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4be0056-d6ee-4416-aa13-69ef8593a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import tiktoken\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e811e4-75c2-4774-ba77-5b5347c6313d",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997eff3a-807b-4131-8848-26ae66f1854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887222e1-02e2-457e-af0b-5616dcbef351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    \"\"\"A tokenizer class for encoding/decoding text sequences.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor method to initialize special token indices and tokenizer encoding. \n",
    "        \"\"\"\n",
    "        # Initialize special token indices\n",
    "        self.BOS_IDX: int = 100264  # Index for the Beginning of Sentence token\n",
    "        self.EOS_IDX: int = 100265  # Index for the End of Sentence token\n",
    "        self.PAD_IDX: int = 100266  # Index for the Padding token\n",
    "\n",
    "        # Initialize base encoding from tiktoken\n",
    "        cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "        # Initialize the tokenizer's encoding with special tokens added\n",
    "        self.encoder = tiktoken.Encoding(\n",
    "            name=\"cl100k_bep\", # Name for the encoder with BOS, EOS, and PAD tokens added\n",
    "            pat_str=cl100k_base._pat_str, # Pattern string from the base encoding\n",
    "            mergeable_ranks=cl100k_base._mergeable_ranks, # Mergeable ranks from the base encoding\n",
    "            special_tokens={\n",
    "                **cl100k_base._special_tokens, # Special tokens from the base encoding\n",
    "                \"<|bos|>\": self.BOS_IDX,  # BOS token\n",
    "                \"<|eos|>\": self.EOS_IDX,  # EOS token\n",
    "                \"<|pad|>\": self.PAD_IDX,  # PAD token\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Method to return the size of the vocabulary in the tokenizer's encoding.\n",
    "\n",
    "        Returns:\n",
    "            int: The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        return self.encoder.n_vocab\n",
    "\n",
    "\n",
    "    def sequence_padding(self, sequence, max_size: int = 512, device: str = \"cpu\") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Method to add BOS/PAD/EOS special tokens and ensure the sequence length is within the maximum size.\n",
    "\n",
    "        Args:\n",
    "            sequence (torch.Tensor or list): The input sequence.\n",
    "            max_size (int, optional): The maximum allowed size for the sequence. Defaults to 512.\n",
    "            device (str, optional): The device where the tensors will be allocated. Defaults to \"cpu\".\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The processed sequence with special tokens added and length limited.\n",
    "        \"\"\"\n",
    "        assert max_size > 2, f\"[max_size]: {max_size} should be greater than 2\"\n",
    "        # Ensure the sequence is a torch tensor\n",
    "        tensor_sequence = torch.tensor(sequence, dtype=torch.long).to(device) if not torch.is_tensor(sequence) else sequence.to(device)\n",
    "\n",
    "        # Limit the sequence length within (max_size - 2) where 2 corresponding to bos and eos tags\n",
    "        cutted_sequence_size = max(0, min(max_size - 2, tensor_sequence.size()[0]))\n",
    "        tensor_sequence = tensor_sequence[:cutted_sequence_size]\n",
    "        \n",
    "        # Add BOS token\n",
    "        tensor_sequence = torch.cat([torch.tensor([self.BOS_IDX], dtype=torch.long, device=device), tensor_sequence], dim=0)\n",
    "\n",
    "        # Calculate the padding size\n",
    "        padding_size = max_size - tensor_sequence.size()[0] - 1 # expected size - current size - EOS tag\n",
    "\n",
    "        # Create PAD tensor\n",
    "        pad_tensor = torch.full((padding_size,), self.PAD_IDX, dtype=torch.long, device=device)\n",
    "\n",
    "        # Add PAD and EOS tokens\n",
    "        tensor_sequence = torch.cat([tensor_sequence, pad_tensor, torch.tensor([self.EOS_IDX], dtype=torch.long, device=device)], dim=0)\n",
    "        \n",
    "        return tensor_sequence\n",
    "    \n",
    "    def sequence_cleaner(self, sequence):\n",
    "        \"\"\" Method used to remove BOS/PAD/EOS special tokens \"\"\"\n",
    "        # Checking tensor format\n",
    "        list_sequence = sequence.tolist() if torch.is_tensor(sequence) else sequence\n",
    "        def check_special(number):\n",
    "            return number not in [self.BOS_IDX, self.EOS_IDX, self.PAD_IDX]\n",
    "        return list(filter(check_special, list_sequence))\n",
    "\n",
    "    def tokenize(self, sequence, device=\"cpu\") -> list:\n",
    "        \"\"\"\n",
    "        Method to generate a str list of separated tokens token.\n",
    "\n",
    "        Args:\n",
    "            sequence (torch.Tensor or list): The input sequence.\n",
    "            device (str, optional): The device where the tensors will be allocated. Defaults to \"cpu\".\n",
    "\n",
    "        Returns:\n",
    "            list: The processed sequence converted in a list of tokens in string format.\n",
    "        \"\"\"\n",
    "        # Ensure the sequence is a torch tensor\n",
    "        tensor_sequence = torch.tensor(sequence, dtype=torch.long).to(device) if not torch.is_tensor(sequence) else sequence.to(device)\n",
    "        # create batch of idx tokens\n",
    "        tensor_sequence = tensor_sequence.unsqueeze(0).T\n",
    "        # decode all batch to recreate list of separated tokens \n",
    "        tensor_sequence = self.encoder.decode_batch(tensor_sequence.detach().tolist())\n",
    "        return tensor_sequence\n",
    "\n",
    "    def tokenize_from_str(self, sequence, device=\"cpu\") -> list:\n",
    "        return self.tokenize(self.encoder.encode(sequence), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321a98a8-144b-4815-956a-b866c679fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7d7325-63c2-4c89-887d-1d28c6288293",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    \"\"\"Data class that stores the configuration for a Transformer model.\n",
    "\n",
    "    Attributes:\n",
    "        - tokenizer: An instance of the Tokenizer class.\n",
    "        - block_size (int): Number of tokens in each sequence. Defaults to 512.\n",
    "        - batch_size (int): Number of sequences in each batch. Defaults to 12.\n",
    "        - vocab_size (int): Total size of the tokenizer vocabulary. It is set to the size of the tokenizer vocabulary.\n",
    "        - n_layer (int): Number of transformer encoder and decoder blocks (N). Defaults to 1.\n",
    "        - n_head (int): Number of heads in each attention block. Defaults to 2.\n",
    "        - n_embd (int): Token embedding size. This is from the original Transformer paper. Defaults to 128.\n",
    "        - dropout (float): Dropout rate to use in the Transformer model. Defaults to 0.1.\n",
    "        - bias (bool): Indicates whether to use bias in Linears and LayerNorms.\n",
    "            If True, bias is used similar to GPT-2.\n",
    "            If False, it is a bit better and faster. Defaults to False.\n",
    "        - device (str): The device to run the model on. Defaults to 'cpu'. 'cuda' is used if a GPU is available.\n",
    "        - learning_rate (float): Learning rate for the model optimization. Defaults to 3e-4.\n",
    "        - eval_interval (int): Number of steps between each validation dataset. Defaults to 1.\n",
    "        - eval_iters (int): Number of validation epochs. Defaults to 20.\n",
    "        - visualize (bool): Define if we want to get the attention scores.\n",
    "    \"\"\"\n",
    "    tokenizer: any\n",
    "    block_size: int = 512\n",
    "    batch_size: int = 12\n",
    "    n_layer: int = 1 # 6\n",
    "    n_head: int = 2 # 8\n",
    "    n_embd: int = 128 # 512\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = False # True:\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = 'cpu'\n",
    "    learning_rate = 3e-4\n",
    "    eval_interval: int = 1\n",
    "    eval_iters: int = 20 # 200\n",
    "    visualize: bool = False\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Returns the total size of the tokenizer vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            int: The size of the tokenizer vocabulary.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a4b719-f507-46bc-b389-892b9efd52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb57475-96fe-42c1-9568-06926c2c8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "\n",
    "    Args:\n",
    "        - dataset (Dataset): a dataset from HuggingFace datasets library.\n",
    "        - tokenizer (Tokenizer): The custom tiktoken tokenizer used to encode sequences.\n",
    "        - block_size (int): The maximum sequence length for tokenization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer, block_size):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a tokenized example from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            - index (int): the index of the example to fetch.\n",
    "\n",
    "        Returns:\n",
    "            - Dict: dictionary with keys 'inputs', 'targets' and 'translation', containing tokenized input,\n",
    "            target sequences and original translation.\n",
    "        \"\"\"\n",
    "        translation = self.dataset[index]['translation']\n",
    "        encode = self.tokenizer.encoder.encode\n",
    "        inputs = self.tokenizer.sequence_padding(encode(translation['en']), self.block_size) # source language\n",
    "        targets = self.tokenizer.sequence_padding(encode(translation['fr']), self.block_size) # target language\n",
    "        return {'inputs': inputs, 'targets': targets, 'translation': translation}\n",
    "\n",
    "    def __len__(self) -> int :\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            - int: the length of the dataset.\n",
    "        \"\"\"\n",
    "        return self.dataset.num_rows\n",
    "\n",
    "\n",
    "class DataLoaderFactory():\n",
    "    \"\"\"\n",
    "    A class to instantiate PyTorch DataLoaders for different splits of a HuggingFace Dataset.\n",
    "\n",
    "    Args:\n",
    "        - block_size (int): The maximum sequence length for tokenization.\n",
    "        - batch_size (int): The batch size for DataLoader.\n",
    "        - tokenizer (Tokenizer): a tokenizer that has an encode method.\n",
    "        - device (str): 'cpu' or 'cuda', depending on whether we use CPU or GPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_size, batch_size, tokenizer, device):\n",
    "        self.train_data = TranslationDataset(load_dataset(\"wmt14\", \"fr-en\", split=\"train[:500000]\"), tokenizer, block_size)\n",
    "        self.val_data = TranslationDataset(load_dataset(\"wmt14\", \"fr-en\", split=\"validation\"), tokenizer, block_size)\n",
    "        self.test_data = TranslationDataset(load_dataset(\"wmt14\", \"fr-en\", split=\"test\"), tokenizer, block_size)\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.dataloader_train = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "        self.dataloader_val = DataLoader(self.val_data, batch_size=batch_size, shuffle=True)\n",
    "        self.dataloader_test = DataLoader(self.test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int :\n",
    "        \"\"\"\n",
    "        Print the length of each dataset and returns the length of all datasets.\n",
    "\n",
    "        Returns:\n",
    "            - int: the length of all dataset (train + val + test).\n",
    "        \"\"\"\n",
    "        print(\"\\033[95m\\033[1m\\033[4mNumber of data by datasets splits\\033[0m\")\n",
    "        print(f\"Train\\t\\t: {len(self.train_data)}\")\n",
    "        print(f\"Validation\\t: {len(self.val_data)}\")\n",
    "        print(f\"Test\\t\\t: {len(self.test_data)}\")\n",
    "        total = len(self.train_data) + len(self.val_data) + len(self.test_data)\n",
    "        print(f\"Total\\t\\t: {total}\")\n",
    "        return total\n",
    "\n",
    "    def get_batch(self, split):\n",
    "        \"\"\"\n",
    "        Choose the correct DataLoader and yield batches from it.\n",
    "\n",
    "        Args:\n",
    "            - split (str): 'train', 'val' or 'test'.\n",
    "\n",
    "        Yields:\n",
    "            - Dict: a dictionary with keys 'inputs', 'targets' and 'translation', containing a batch of tokenized input,\n",
    "            target sequences and original translation.\n",
    "        \"\"\"\n",
    "        # choose the correct dataloader\n",
    "        if split == 'train':\n",
    "            dataloader = self.dataloader_train\n",
    "        elif split == 'val':\n",
    "            dataloader = self.dataloader_val\n",
    "        else:\n",
    "            dataloader = self.dataloader_test\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # Separate the 'translation' from the rest of the batch\n",
    "            translation = batch.pop('translation')\n",
    "    \n",
    "            # Move tensors to device\n",
    "            batch_on_device = {k: v.to(self.device) for k, v in batch.items()}\n",
    "    \n",
    "            # Add 'translation' back into the batch\n",
    "            batch_on_device['translation'] = translation\n",
    "    \n",
    "            yield batch_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b64c8f-7fd6-456f-b6ce-afa6c25f27fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    }
   ],
   "source": [
    "config = TransformerConfig(tokenizer)\n",
    "dataset = DataLoaderFactory(config.block_size, config.batch_size, tokenizer, config.device)\n",
    "# dataset.DataSize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d0e6a1-4b17-4955-9600-b5cac119e801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataLoaderFactory.get_batch at 0x0000019C2C76A840>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = dataset.get_batch('train')\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020363b6-98ed-4d32-a522-41d78b804600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': tensor([[100264,   5451,     11,  ..., 100266, 100266, 100265],\n",
       "         [100264,   4897,    374,  ..., 100266, 100266, 100265],\n",
       "         [100264,  55915,     11,  ..., 100266, 100266, 100265],\n",
       "         ...,\n",
       "         [100264,  12805,    814,  ..., 100266, 100266, 100265],\n",
       "         [100264,   2028,    374,  ..., 100266, 100266, 100265],\n",
       "         [100264,   1737,  27995,  ..., 100266, 100266, 100265]]),\n",
       " 'targets': tensor([[100264,  42562,  67882,  ..., 100266, 100266, 100265],\n",
       "         [100264,     34,  17771,  ..., 100266, 100266, 100265],\n",
       "         [100264,   6719,    273,  ..., 100266, 100266, 100265],\n",
       "         ...,\n",
       "         [100264,  56948,  37291,  ..., 100266, 100266, 100265],\n",
       "         [100264,     34,   8458,  ..., 100266, 100266, 100265],\n",
       "         [100264,     43,      6,  ..., 100266, 100266, 100265]]),\n",
       " 'translation': {'en': ['First, the debate has made it plain that citizens do not want the Union to withdraw from areas where it is present.',\n",
       "   'That is a significant and important start.',\n",
       "   'Therefore, Mr President, after much debate, the Committee on Legal Affairs and the Internal Market has adopted exactly the same approach as the European Commission, without going so far: for there still to be a national Court of First Instance, albeit with many elements that are already communitarised, and I feel that this is a possibility that the Council should follow if we are to have the patent we need.',\n",
       "   'I still remember an example that was renowned in Portugal ten years ago: that of Thierry Rousell, who successfully prospected - or so he said - in the Brejão region, sowed hope and then stole many millions of escudos.',\n",
       "   'Finally, the escalation in the Middle East has meanwhile led to extremely shameful expressions of blatant anti-Semitism in Member States of the European Union.',\n",
       "   'However, I share the view expressed in paragraph 32.',\n",
       "   'These have surfaced following research carried out within industry, which shows that the practical problems would really be enormous if we were not to distribute euro notes beforehand.',\n",
       "   'In the first amendment, it proposed extending the date until which Member States may authorise buses with dimensions not complying with the requirements of the new directive to circulate on their territory from December 2009, as proposed by the Commission, to 2015, in order to allow a normal economic life for those buses.',\n",
       "   'I should like to add that I very much agree with my fellow MEP, Mr Blak, that we must also look at another problem for drivers.',\n",
       "   'Once they are in force, Member States will no longer need to notify aid projects complying with the conditions of the exemption regulations but can grant the aid immediately.',\n",
       "   'This is, of course, because it is a waste of resources for us not to ensure that materials in cars are recycled.',\n",
       "   'Enlargement specifically embodies the magnanimous and open vision of the original European project.'],\n",
       "  'fr': [\"Premièrement, ce débat a mis en évidence que les citoyens ne veulent pas que l'Union se retire des domaines d'activité dans lesquels elle est présente.\",\n",
       "   \"C'est un bon début non négligeable.\",\n",
       "   \"Elle se fonderait sur une prévision du traité de Nice qui n'est pas encore entré en vigueur, et qui consiste à instaurer une juridiction communautaire centralisée dès la première instance. Donc, Monsieur le Président, après un long débat, la Commission juridique a totalement adhéré au point de vue de la commission européenne, mais sans aller aussi loin : elle propose encore une première instance nationale, bien qu'avec beaucoup d'éléments déjà communautarisés ; et je crois que c'est une piste de réflexion que le Conseil devra suivre si nous voulons obtenir notre brevet comme nous le demandons.\",\n",
       "   \"J'ai encore en mémoire un exemple qui a fait la une au Portugal il y a une dizaine d'années : celui de M. Thierry Rousell qui a prospecté avec succès - disait-il - dans la région de Brejão en semant des illusions et en volant des millions.\",\n",
       "   \"Enfin, Monsieur le Président, l'escalade au Moyen-Orient a entre-temps donné lieu, dans les États membres de l'Union européenne, à des expressions extrêmement honteuses d'antisémitisme ouvert.\",\n",
       "   'En revanche, le point 32 correspond bien à mon opinion.',\n",
       "   'Il ressort de celles-ci que les problèmes pratiques prendront une envergure majeure si nous ne distribuons pas de billets en euros au préalable.',\n",
       "   \"Par le premier amendement, nous avons proposé de prolonger la période au cours de laquelle les États membres peuvent autoriser, sur leur territoire, la circulation d' autobus dont les dimensions ne sont pas conformes aux exigences de la nouvelle directive, de décembre 2009, comme la Commission l' avait proposé, à 2015, de manière à ce que ces autobus puissent avoir une vie économique normale.\",\n",
       "   \"J'ajouterai que je suis tout à fait d'accord avec mon collègue, M. Blak, lorsqu'il affirme que nous devons examiner un autre problème qui se pose aux chauffeurs.\",\n",
       "   \"Une fois en vigueur, les États membres ne devront plus notifier leurs projets d'octroi d'aides conformes aux dispositions des règlements instituant une exemption de groupe, mais pourront octroyer ces aides directement.\",\n",
       "   \"Cela tient au fait qu'il y a un gaspillage des ressources dû à l'absence de recyclage du matériel se trouvant dans les voitures.\",\n",
       "   \"L' élargissement concrétise la vision magnanime et ouverte du projet européen original.\"]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d09d529-2f21-4495-bf0f-36cc30640d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m\u001b[4mNumber of data by datasets splits\u001b[0m\n",
      "Train\t\t: 500000\n",
      "Validation\t: 3000\n",
      "Test\t\t: 3003\n",
      "Total\t\t: 506003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "506003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce6e629-fa89-4931-83d0-e62727dac927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"A Layer Normalization module with optional bias.\n",
    "\n",
    "    This implementation of Layer Normalization allows turning off the bias term,\n",
    "    which is not directly supported by PyTorch's layer normalization function.\n",
    "\n",
    "    Attributes:\n",
    "        weight: Learnable weights for the layer normalization. Initialized as an all ones tensor.\n",
    "        bias: Learnable biases for the layer normalization. Initialized as an all zeros tensor \n",
    "              if bias argument in constructor is True, otherwise it's set to None.\n",
    "\n",
    "    Args:\n",
    "        ndim: An integer for the dimension of the input vectors.\n",
    "        bias: A boolean which, if True, adds a learnable bias to the output.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, ndim: int, bias: bool):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            input (tensor): The input tensor to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            tensor: The normalized input tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357d6e13-1c75-485f-9625-29a174b44d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module.\n",
    "    \n",
    "    This module applies multi-head attention mechanism on the input sequence. This implementation doesn't apply mask over the attention scores.\n",
    "    \n",
    "    Attributes:\n",
    "        - n_head (int): Number of attention heads.\n",
    "        - n_embd (int): Embedding dimensionality.\n",
    "        - dropout (float): Dropout rate.\n",
    "        - q_attn (nn.Linear): Linear layer for the query projection.\n",
    "        - k_attn (nn.Linear): Linear layer for the key projection.\n",
    "        - v_attn (nn.Linear): Linear layer for the value projection.\n",
    "        - c_proj (nn.Linear): Linear layer for the output projection.\n",
    "        - attn_dropout (nn.Dropout): Dropout layer for the attention scores.\n",
    "        - resid_dropout (nn.Dropout): Dropout layer for the residual connection.\n",
    "        - flash (bool): Flag indicating if flash attention is available.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Constructor for the MultiHeadAttention class.\n",
    "        \n",
    "        Args:\n",
    "            - config: The configuration object containing model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # Params\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.visualize = config.visualize\n",
    "        \n",
    "        # INPUTS: query, key, value projections for all heads, but in a batch\n",
    "        self.q_attn = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.k_attn = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.v_attn = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # OUTPUT: output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # flash attention make GPU go br but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.tril(\n",
    "                    torch.ones(config.block_size, config.block_size)\n",
    "                ).view(1, 1, config.block_size, config.block_size)\n",
    "            )\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask: bool = None):\n",
    "        \"\"\"\n",
    "        Computes the scaled dot product attention.\n",
    "        \n",
    "        Args:\n",
    "            - q (Tensor): Query tensor of shape (batch_size, num_heads, seq_length, emb_dim).\n",
    "            - k (Tensor): Key tensor of shape (batch_size, num_heads, seq_length, emb_dim).\n",
    "            - v (Tensor): Value tensor of shape (batch_size, num_heads, seq_length, emb_dim).\n",
    "            - mask (bool, optional): Flag indicating whether to apply mask on the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            - y (Tensor): Output tensor after applying attention.\n",
    "        \"\"\"\n",
    "        # manual implementation of attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # Step 1 & 2: (MatMul) and (Scale)\n",
    "        if mask:\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1) # Step 3: Softmax\n",
    "        att_weights = att  # Save attention weights for visualization\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs) # Step 4: MatMul\n",
    "        return y, att_weights\n",
    "\n",
    "    def forward(self, q_x, k_x, v_x, mask = None):\n",
    "        \"\"\"\n",
    "        Forward pass for the MultiHeadAttention module.\n",
    "        \n",
    "        Args:\n",
    "            - q_x (Tensor): Input query tensor of shape (batch_size, seq_length, emb_dim).\n",
    "            - k_x (Tensor): Input key tensor of shape (batch_size, seq_length, emb_dim).\n",
    "            - v_x (Tensor): Input value tensor of shape (batch_size, seq_length, emb_dim).\n",
    "            - mask (bool, optional): Flag indicating whether to apply mask on the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            - y (Tensor): Output tensor after applying multi-head attention.\n",
    "        \"\"\"\n",
    "        B_q, T_q, C_q = q_x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        B_kv, T_kv, C_kv = k_x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.q_attn(q_x), self.k_attn(k_x), self.v_attn(v_x)\n",
    "        k = k.view(B_kv, T_kv, self.n_head, C_kv // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B_q, T_q, self.n_head, C_q // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B_kv, T_kv, self.n_head, C_kv // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash and not self.visualize:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=mask == True)\n",
    "            attn_weights = None\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            y, attn_weights = self.scaled_dot_product_attention(q, k, v)\n",
    "        y = y.transpose(1, 2).contiguous().view(B_q, T_q, C_q) # re-assemble all head outputs side by side # Step 5: Concatenate\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y)) # Step 6 : Linear\n",
    "        return y, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb156ccb-b9fd-4b62-b2a4-d2762cb71a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A position-wise Feed Forward Neural Network (FFNN) class for transformer models.\n",
    "    \n",
    "    The class implementing a position-wise FFNN.\n",
    "    The FFNN consists of two linear transformations with a GELU activation in between, \n",
    "    followed by a dropout for regularization.\n",
    "\n",
    "    Attributes:\n",
    "        - c_fc (nn.Linear): First fully connected layer.\n",
    "        - gelu (nn.GELU): GELU activation function layer.\n",
    "        - c_proj (nn.Linear): Second fully connected layer.\n",
    "        - dropout (nn.Dropout): Dropout layer for regularization.\n",
    "\n",
    "    Args:\n",
    "        - config (Config): A configuration object with attribute `n_embd`, `bias`, and `dropout`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): The input tensor to the forward pass.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output of the FFNN.\n",
    "        \"\"\"\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d648bfa6-2687-4d48-b9b1-be60061f48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements a single encoder block in the Transformer model.\n",
    "    \n",
    "    Each block consists of two sub-layers: a multi-head self-attention mechanism,\n",
    "    and a position-wise fully connected feed-forward network. There is a residual \n",
    "    connection around each of the two sub-layers, followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        - ln_1 (LayerNorm): Layer normalization before the multi-head attention layer.\n",
    "        - attn (MultiHeadAttention): Multi-head attention layer.\n",
    "        - ln_2 (LayerNorm): Layer normalization before the feed-forward network.\n",
    "        - ffw (FeedForward): Position-wise feed-forward network.\n",
    "\n",
    "    Args:\n",
    "        - config (Config): A configuration object with attribute `n_embd` and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.ffw = FeedForward(config)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): The input tensor to the forward pass.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor of the block.\n",
    "        \"\"\"\n",
    "        # MultiHeadAttention\n",
    "        x = self.ln_1(x)\n",
    "        x_attn, decoder_attn = self.attn(x, x, x)\n",
    "        x = x + x_attn\n",
    "        # FeedForward\n",
    "        x = x + self.ffw(self.ln_2(x))\n",
    "        return x, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b0c31f8-35a6-4f31-a0ca-39943bff97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements a single decoder block in the Transformer model.\n",
    "\n",
    "    Each block consists of three sub-layers: a multi-head self-attention mechanism,\n",
    "    a multi-head attention mechanism over the encoder's output, and a position-wise \n",
    "    fully connected feed-forward network. There is a residual connection around \n",
    "    each of the three sub-layers, followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        - ln_1 (LayerNorm): Layer normalization before the first multi-head attention layer.\n",
    "        - attn1 (MultiHeadAttention): First multi-head attention layer, with self-attention.\n",
    "        - ln_2 (LayerNorm): Layer normalization before the second multi-head attention layer.\n",
    "        - attn2 (MultiHeadAttention): Second multi-head attention layer, attends to encoder outputs.\n",
    "        - ln_3 (LayerNorm): Layer normalization before the feed-forward network.\n",
    "        - ffw (FeedForward): Position-wise feed-forward network.\n",
    "\n",
    "    Args:\n",
    "        config (Config): A configuration object with attribute `n_embd` and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn1 = MultiHeadAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn2 = MultiHeadAttention(config)\n",
    "        self.ln_3 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.ffw = FeedForward(config)\n",
    "\n",
    "    def forward(self, x, encoder_output) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): The input tensor to the forward pass.\n",
    "            - encoder_output (torch.Tensor): The output tensor from the last encoder block.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor of the block.\n",
    "        \"\"\"\n",
    "        # Masked MultiHeadAttention\n",
    "        x = self.ln_1(x)\n",
    "        x_attn, cross_attn = self.attn1(x, x, x, True)\n",
    "        x = x + x_attn\n",
    "        # MultiHeadAttention with q, k from encoder and x from decoder\n",
    "        x = self.ln_2(x)\n",
    "        x_attn, encoder_attn = self.attn2(x, encoder_output, encoder_output)\n",
    "        x = x + x_attn\n",
    "        # FeedForward\n",
    "        x = x + self.ffw(self.ln_3(x))\n",
    "        return x, encoder_attn, cross_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d415dfae-bc35-4f10-9fa0-cf182bf30a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements the encoder part of the Transformer model.\n",
    "\n",
    "    The encoder consists of several EncoderBlocks arranged in sequence.\n",
    "    The input first goes through an embedding layer followed by a positional encoding layer.\n",
    "    The output of this is then passed through each EncoderBlock in sequence.\n",
    "\n",
    "    Attributes:\n",
    "        - encoder (nn.ModuleDict): A dictionary of modules making up the transformer encoder.\n",
    "\n",
    "    Args:\n",
    "        - config (Config): A configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # Learned positional encoding:\n",
    "            # In this case, instead of using a fixed function to determine positional encoding,\n",
    "            # we initialize a tensor of positional encodings which gets updated during training via backpropagation.\n",
    "            # This method may potentially capture more complex position-related patterns than fixed positional encoding,\n",
    "            # but it also introduces additional parameters to the model.\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([EncoderBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        \n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # apply special scaled init to the residual projections, based on GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                # This initialization is used to preventing the variance of the outputs of each layer from exploding or vanishing\n",
    "                # during the forward pass through the network.\n",
    "                # Preventing \"vanishing/exploding gradients\" problem\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"Number of Encoder parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding: bool = True):\n",
    "        \"\"\"\n",
    "        Returns the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "\n",
    "        Args:\n",
    "            -non_embedding (bool, optional): If True, excludes the position embeddings count from the total (Default is True).\n",
    "\n",
    "        Returns:\n",
    "            - int: The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.encoder.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model. Proper weight initialization can help speed up the training process and improve model performance.\n",
    "\n",
    "        Args:\n",
    "            - module (nn.Module): The module of the model to be initialized.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # init Linear layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # bias initialization if necessary\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # init Embedding layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - idx (torch.Tensor): The input tensor to the forward pass.\n",
    "            - targets (torch.Tensor, optional): The target tensor against which the loss will be calculated.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "            - torch.Tensor: The loss tensor, if targets were provided. Otherwise, None.\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # pre-encoder block\n",
    "        tok_emb = self.encoder.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.encoder.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.encoder.drop(tok_emb + pos_emb) # Addition of input embd + positional encoding\n",
    "\n",
    "        # encoders block\n",
    "        encoder_attn_all = []\n",
    "        for block in self.encoder.h:\n",
    "            x, encoder_attn = block(x)\n",
    "            encoder_attn_all.append(encoder_attn)\n",
    "\n",
    "        return  self.encoder.ln_f(x), encoder_attn_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a0264d2-09df-4222-9cc3-945f5b4272af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the decoder part of the Transformer model.\n",
    "\n",
    "    The Decoder consists of several DecoderBlocks arranged in sequence. The input first goes through an embedding \n",
    "    layer followed by a positional encoding layer. The output of this is then passed through each DecoderBlock in \n",
    "    sequence.\n",
    "\n",
    "    Attributes:\n",
    "        - decoder (nn.ModuleDict): A dictionary of modules making up the transformer decoder.\n",
    "        - lm_head (nn.Linear): The final linear layer mapping from the embedding dimension to the vocabulary size.\n",
    "        - config (:obj:`Config`): The configuration object for the transformer model.\n",
    "\n",
    "    .. note:: The weight of the embedding layer and the linear layer are shared.\n",
    "\n",
    "    Args:\n",
    "        - config (:obj:`Config`): The configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.decoder = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # Learned positional encoding:\n",
    "            # In this case, instead of using a fixed function to determine positional encoding,\n",
    "            # we initialize a tensor of positional encodings which gets updated during training via backpropagation.\n",
    "            # This method may potentially capture more complex position-related patterns than fixed positional encoding,\n",
    "            # but it also introduces additional parameters to the model.\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.decoder.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "\n",
    "        Args:\n",
    "            - non_embedding (bool): If True, excludes the position embeddings count from the total. Default is True.\n",
    "\n",
    "        Returns:\n",
    "            - int: The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.decoder.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model.\n",
    "\n",
    "        Args:\n",
    "            - module (torch.nn.Module): The module of the model to be initialized.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # init Linear layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # bias initialization if necessary\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # init Embedding layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, enc_output = None):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - idx (torch.Tensor): The input tensor to the forward pass.\n",
    "            - enc_output (torch.Tensor): The output tensor from the encoder.\n",
    "            - targets (torch.Tensor, optional): The target tensor against which the loss will be calculated.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "        tok_emb = self.decoder.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.decoder.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.decoder.drop(tok_emb + pos_emb) # Addition of input embd + positional encoding\n",
    "\n",
    "        cross_attn_all = []\n",
    "        decoder_attn_all = []\n",
    "        for block in self.decoder.h:\n",
    "            x, encoder_attn, cross_attn = block(x, enc_output)\n",
    "            decoder_attn_all.append(encoder_attn)\n",
    "            cross_attn_all.append(cross_attn)\n",
    "            \n",
    "        x = self.decoder.ln_f(x)\n",
    "        return self.lm_head(x), decoder_attn_all, cross_attn_all\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d62cda5-5bc1-4bd9-a3e8-134bab8da831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the Transformer model, which includes both the encoder and decoder.\n",
    "\n",
    "    The Transformer is a sequence transduction model that uses attention mechanisms.\n",
    "    It is primarily used in tasks that require understanding of context or relationships among words in a text.\n",
    "\n",
    "    Attributes:\n",
    "        - encoder (Encoder): The transformer encoder.\n",
    "        - decoder (Decoder): The transformer decoder.\n",
    "        - config (:obj:`Config`): The configuration object for the transformer model.\n",
    "\n",
    "    Args:\n",
    "        - config (:obj:`Config`): The configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - src (torch.Tensor): The input tensor to the encoder.\n",
    "            - tgt (torch.Tensor): The input tensor to the decoder.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "            - torch.Tensor: The loss tensor calculated on the basis of the decoder's output and target tensor.\n",
    "        \"\"\"\n",
    "        enc_output, _ = self.encoder(src)\n",
    "        output, _, _ = self.decoder(tgt, enc_output)\n",
    "\n",
    "        # Calculate the loss, using both the output and the target\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        # The targets for the loss function are the input sequences shifted\n",
    "        tgt_tgt = tgt[:, 1:].contiguous()\n",
    "        loss = loss_fct(output.view(-1, output.size(-1)), tgt_tgt.view(-1))\n",
    "        return output, loss\n",
    "\n",
    "    @torch.no_grad() # deactivate autograd\n",
    "    def translate_old(self, src):\n",
    "        enc_output, encoder_attn = self.encoder(src)\n",
    "        tgt = torch.zeros_like(src).long().to(src.device)  # initialize target tensor with zeros\n",
    "        assert 'a' == 'b', \"stop\"\n",
    "        for i in range(self.config.block_size):\n",
    "            print(f\"\\r{i+1}/{self.config.block_size}\", end=\"\")\n",
    "            output, decoder_attn, cross_attn = self.decoder(tgt, enc_output)  # obtain output probabilities\n",
    "            next_word = output.argmax(2)[:, -1]  # select the token with the highest probability\n",
    "            tgt[:, i] = next_word # append the new token to the target sequence\n",
    "        return tgt, dict(encoder_attn = encoder_attn, cross_attn = cross_attn, decoder_attn = decoder_attn)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def translate_beam_search(self, src, beam_size=5):\n",
    "        enc_output, encoder_attn = self.encoder(src)\n",
    "        # initialize beam with start token\n",
    "        start_token = torch.zeros((src.size(0), 1)).long().to(src.device)  # initialize target tensor with zeros\n",
    "        start_score = torch.zeros((src.size(0), 1)).float().to(src.device)  # initialize target tensor with zeros\n",
    "        beams = [(start_token, start_score)]  # initialize beams\n",
    "        decoder_attentions = []\n",
    "        cross_attentions = []\n",
    "        for iter in range(self.config.block_size):\n",
    "            print(f\"\\r{iter+1}/{self.config.block_size}\", end=\"\")\n",
    "            new_beams = []\n",
    "            new_decoder_attentions = []\n",
    "            new_cross_attentions = []\n",
    "            for beam, score in beams:\n",
    "                output, dec_attention, cross_attention = self.decoder(beam, enc_output)\n",
    "                output = F.log_softmax(output[:, -1, :], dim=-1)  # use log_softmax for numerical stability\n",
    "\n",
    "                top_scores, top_indices = output.topk(beam_size, dim=-1)  # select topk predictions\n",
    "                \n",
    "                for i in range(beam_size):\n",
    "                    next_token = top_indices[:, i].unsqueeze(1)  # get next token\n",
    "                    next_score = top_scores[:, i].unsqueeze(1)  # get next score\n",
    "                    new_beam = torch.cat((beam, next_token), dim=-1)  # generate new beam\n",
    "                    new_score = score + next_score  # calculate new score\n",
    "                    new_beams.append((new_beam, new_score))  # append new beam to new beams\n",
    "                    new_decoder_attentions.append(torch.stack(dec_attention, dim=0))\n",
    "                    new_cross_attentions.append(torch.stack(cross_attention, dim=0))\n",
    "            # sort all candidates by score\n",
    "            beams = sorted(new_beams, key=lambda tup: tup[1].sum(), reverse=True)[:beam_size]  # keep top performing beams\n",
    "            decoder_attentions = sorted(new_decoder_attentions, key=lambda tup: tup.sum(), reverse=True)[:beam_size]\n",
    "            cross_attentions = sorted(new_cross_attentions, key=lambda tup: tup.sum(), reverse=True)[:beam_size]\n",
    "        return beams[0][0], dict(encoder_attn=encoder_attn, cross_attn=cross_attentions[0][0], decoder_attn=decoder_attentions[0][0])  # return the best sequence\n",
    "\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        model_state = {\n",
    "            'encoder_state': self.encoder.state_dict(),\n",
    "            'decoder_state': self.decoder.state_dict(),\n",
    "            'config': self.config\n",
    "        }\n",
    "        torch.save(model_state, path)\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError(f\"{path} does not exist.\")\n",
    "        model_state = torch.load(path)\n",
    "        \n",
    "        # Sanity check for configuration consistency\n",
    "        assert self.config == model_state['config'], \"Trying to loasequence_cleand model with different configuration\"\n",
    "\n",
    "        self.encoder.load_state_dict(model_state['encoder_state'])\n",
    "        self.decoder.load_state_dict(model_state['decoder_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7897c151-f04d-4ba5-a19e-a32c12818f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encoder parameters: 13.03M\n",
      "number of parameters: 13.10M\n"
     ]
    }
   ],
   "source": [
    "config = TransformerConfig(tokenizer, visualize=True)\n",
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e538ea60-8b86-4a98-9c86-15cd7c4a9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentences, tokenizer, model):\n",
    "    # Tokenize sentences\n",
    "    tknzr = tokenizer.encoder\n",
    "    sequences = []\n",
    "    for sentence in sentences:\n",
    "        sequence = tokenizer.sequence_padding(tknzr.encode(sentence)).unsqueeze(dim=0)\n",
    "        sequences.append(sequence)\n",
    "    sequences = torch.cat(sequences, dim=0)\n",
    "\n",
    "    # Translate sentences\n",
    "    model.eval()\n",
    "    outputs, attn = model.translate_beam_search(sequences)\n",
    "\n",
    "    # Decode tokenized sentences\n",
    "    decode_output = []\n",
    "    for output in outputs:\n",
    "        output = tokenizer.sequence_cleaner(output)\n",
    "        decode_output += [tknzr.decode(output)]\n",
    "    \n",
    "    return decode_output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35d8b57a-1410-4f39-a560-fd1668fe7af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512"
     ]
    }
   ],
   "source": [
    "outputs, attentions = translate(['I am a teacher.'], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a7d8520-80e1-431a-bc3c-025dc8b7b965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!illesillesillesillesillesillesillesillesillesillesilles-grid-grid-grid-grid-grid-grid-grid-grid-grid-grid-grid-grid-grid-grid-grid-grid-grid-grid_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl_acl Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay Gameplay_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc_doc']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60eb611a-50a7-4a3c-98e0-b3d33dd871d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attentions['encoder_attn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3eb4cf5e-578f-44b1-80cd-1b7348e871ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 512, 512])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions['cross_attn'].unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d00f3e9-4cb4-4a96-b5e9-fbe185f6679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_encoder_attn = torch.stack(attentions['encoder_attn'], dim=0)\n",
    "tensor_cross_attn = attentions['cross_attn'].unsqueeze(0)\n",
    "tensor_decoder_attn = attentions['decoder_attn'].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8325fc44-755e-4c75-93a9-03e4bdbbb142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 512, 512])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cross_attn.size() # layer batch heads block_size block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "100cbcf5-6887-4fe4-ab9b-8aec70ff927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5d2a5fc-0740-46fa-b11c-45a02d3f5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a = tokenizer.tokenize_from_str('I am a teacher.')\n",
    "tokens_b = tokenizer.tokenize_from_str('Je suis un professeur.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37a96e38-3403-4ad4-bbee-bd1ece6ea737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', ' am', ' a', ' teacher', '.', '', '']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = tokens_a + [''] * 2\n",
    "tokens_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4609874-5273-4773-a570-3d559721e46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Je', ' suis', ' un', ' prof', 'esse', 'ur', '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fcfd2f12-8cb1-400e-bc1a-76ff03fab43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_token = max(len(tokens_a), len(tokens_b)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c9bf7c0-b143-4dad-8077-ec1c42ba0622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 7, 7])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_encoder_attn = tensor_encoder_attn[:, 0:1, :, 1:max_len_token, 1:max_len_token] # layers, batch, heads, seq_len, seq_len\n",
    "tensor_encoder_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d13656f-d642-470d-968c-f2626c031e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 7, 7])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cross_attn = tensor_cross_attn[:, 0:1, :, 1:max_len_token, 1:max_len_token] # layers, batch, heads, seq_len, seq_len\n",
    "tensor_cross_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "555b4070-4d84-4da4-b1a2-1143342806c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 7, 7])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_decoder_attn = tensor_decoder_attn[:, 0:1, :, 1:max_len_token, 1:max_len_token] # layers, batch, heads, seq_len, seq_len\n",
    "tensor_decoder_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe49a167-b8f9-4a06-82ca-5b3ec493e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set your dimensions\n",
    "# num_layers = 1\n",
    "# batch_size = 1\n",
    "# num_heads = 2\n",
    "# seq_len = 512\n",
    "# num_tokens = 5\n",
    "# tensor_attn_weights2 = torch.rand(num_layers, batch_size, num_heads, seq_len, seq_len)\n",
    "# tensor_attn_weights2 = tensor_attn_weights2[:, :, :, :num_tokens, :num_tokens]\n",
    "# tensor_attn_weights2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e62b6d9c-02e6-4e50-a610-1ea24492c19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-19f2fd06dd98456dafdfb40bedcfff28\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                Attention: <select id=\"filter\"><option value=\"0\">Encoder</option>\n",
       "<option value=\"1\">Decoder</option>\n",
       "<option value=\"2\">Cross</option></select>\n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/**\n",
       " * @fileoverview Transformer Visualization D3 javascript code.\n",
       " *\n",
       " * Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n",
       " *\n",
       " * Change log:\n",
       " *\n",
       " * 02/01/19  Jesse Vig   Initial implementation\n",
       " * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n",
       " * 01/19/21  Jesse Vig   Support light/dark modes\n",
       " * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n",
       " * 05/03/21  Jesse Vig   Adjust visualization height dynamically\n",
       " * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n",
       " **/\n",
       "\n",
       "require.config({\n",
       "  paths: {\n",
       "      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
       "    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
       "  }\n",
       "});\n",
       "\n",
       "requirejs(['jquery', 'd3'], function($, d3) {\n",
       "\n",
       "        const params = {\"attention\": [{\"name\": \"Encoder\", \"attn\": [[[[0.0018464146414771676, 0.0018001589924097061, 0.0017344491789117455, 0.0017507843440398574, 0.0017416394548490644, 0.002001393586397171, 0.0019082187209278345], [0.001972303492948413, 0.0019028690876439214, 0.0019574479665607214, 0.001994171878322959, 0.002041721483692527, 0.001843519159592688, 0.0020233222749084234], [0.001910420716740191, 0.0019798160064965487, 0.0018404732691124082, 0.0018303972901776433, 0.001803202903829515, 0.001991175813600421, 0.002007290255278349], [0.00193652359303087, 0.0019385053310543299, 0.0017446685815230012, 0.001843749894760549, 0.0019058147445321083, 0.001891041873022914, 0.0019511611899361014], [0.0018211458809673786, 0.0022737483959645033, 0.0019070394337177277, 0.001960636116564274, 0.0019712240900844336, 0.0018031225772574544, 0.0019608011934906244], [0.0020816256292164326, 0.0019198638619855046, 0.0020577104296535254, 0.002013441873714328, 0.002247920725494623, 0.0019339921418577433, 0.0019799477886408567], [0.0020214286632835865, 0.001893710228614509, 0.001992067787796259, 0.002000984037294984, 0.0021547002252191305, 0.0019538195338100195, 0.0019273399375379086]], [[0.0018187944078817964, 0.001985579263418913, 0.0019260745029896498, 0.00180947151966393, 0.0019689216278493404, 0.0020192016381770372, 0.002064069965854287], [0.0017711446853354573, 0.0018826526356860995, 0.0018535854760557413, 0.0019155882764607668, 0.0018425106536597013, 0.002035740530118346, 0.0019385528285056353], [0.002136766677722335, 0.0018885541940107942, 0.002194999484345317, 0.0020226400811225176, 0.00204380601644516, 0.0019325066823512316, 0.001852415967732668], [0.001960150897502899, 0.0018585346406325698, 0.002058140467852354, 0.002054718090221286, 0.001816323958337307, 0.0019814076367765665, 0.001978799467906356], [0.0020187837071716785, 0.0021511209197342396, 0.001890729647129774, 0.0019266529707238078, 0.001968749798834324, 0.0019305276218801737, 0.0018384315771982074], [0.00207484420388937, 0.0020477715879678726, 0.0021526317577809095, 0.0020190495997667313, 0.002208051737397909, 0.0019488539546728134, 0.0020111138001084328], [0.0020642303861677647, 0.00188267242629081, 0.002075662836432457, 0.0017805399838835, 0.0019012733828276396, 0.001902154297567904, 0.0019424210768193007]]]], \"left_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}, {\"name\": \"Decoder\", \"attn\": [[[[0.0018792246701195836, 0.0018335148924961686, 0.001820812001824379, 0.0017954667564481497, 0.0021011088974773884, 0.0020436758641153574, 0.0019277126993983984], [0.0017988268518820405, 0.0017897641519084573, 0.0018982256297022104, 0.0017840112559497356, 0.0019622761756181717, 0.0020553539507091045, 0.001958946930244565], [0.0016729023773223162, 0.0017019616207107902, 0.0018107111100107431, 0.0017706295475363731, 0.001895493478514254, 0.0019761521834880114, 0.0019466831581667066], [0.0018859697738662362, 0.0018127233488485217, 0.0018587384838610888, 0.0019052211428061128, 0.002037316095083952, 0.0019663453567773104, 0.0018807054730132222], [0.0018968258518725634, 0.0017964169383049011, 0.0020061549730598927, 0.0017624899046495557, 0.001969343051314354, 0.0020240950398147106, 0.0019146257545799017], [0.0018185381777584553, 0.0018189316615462303, 0.001893619541078806, 0.001837545190937817, 0.0019073606235906482, 0.0020435380283743143, 0.001931778620928526], [0.0017854087054729462, 0.0016933033475652337, 0.0018706403207033873, 0.0017497813096269965, 0.0018687675474211574, 0.0019071969436481595, 0.001976992003619671]], [[0.0018866317113861442, 0.001884565339423716, 0.0018271380104124546, 0.002000353531911969, 0.0018941479502245784, 0.00190870207734406, 0.0019586512353271246], [0.0019458665046840906, 0.001955003011971712, 0.001763446838594973, 0.002025987720116973, 0.0020241239108145237, 0.002011059084907174, 0.0019334204262122512], [0.0019150797743350267, 0.00201409007422626, 0.0018144450150430202, 0.001973160309717059, 0.0018947036005556583, 0.0019690273329615593, 0.001973600359633565], [0.0020460551604628563, 0.001991517608985305, 0.0019401645986363292, 0.001956542255356908, 0.001962150912731886, 0.001956462860107422, 0.0019380557350814342], [0.001826889580115676, 0.00197908584959805, 0.0019266741583123803, 0.0021761287935078144, 0.0020787979010492563, 0.0019404739141464233, 0.0019756497349590063], [0.0020150511991232634, 0.001968350261449814, 0.0019453623099252582, 0.0022021098993718624, 0.0018916501430794597, 0.0018871808424592018, 0.0020191394723951817], [0.0020470020826905966, 0.001941175083629787, 0.0018237376352772117, 0.0021195411682128906, 0.002066523302346468, 0.0020058455411344767, 0.0019738110713660717]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"]}, {\"name\": \"Cross\", \"attn\": [[[[0.0018928833305835724, 0.001979365013539791, 0.0020256545394659042, 0.0019819485023617744, 0.0019882870838046074, 0.0019005568465217948, 0.0019277624087408185], [0.002064631786197424, 0.0020102073904126883, 0.0020302797202020884, 0.0019594456534832716, 0.0020429627038538456, 0.0020231925882399082, 0.0019938661716878414], [0.0020769962575286627, 0.0019761205185204744, 0.0019977958872914314, 0.001938159461133182, 0.001975645311176777, 0.001930567785166204, 0.002015835838392377], [0.0019813713151961565, 0.0020163939334452152, 0.002016756683588028, 0.001959118526428938, 0.00204226141795516, 0.002010411350056529, 0.0020092870108783245], [0.0021182207856327295, 0.0020402008667588234, 0.002106484491378069, 0.0019699756521731615, 0.0020870943553745747, 0.00196444196626544, 0.002070276765152812], [0.002075962023809552, 0.002053468720987439, 0.0021034383680671453, 0.0019978636410087347, 0.0020043132826685905, 0.0020100504625588655, 0.0019626449793577194], [0.0019920789636671543, 0.0019896121229976416, 0.00203309184871614, 0.0019026115769520402, 0.001997563987970352, 0.0019834409467875957, 0.001937332795932889]], [[0.0020585348829627037, 0.0022093539591878653, 0.002054172568023205, 0.002214851789176464, 0.001976896310225129, 0.0022126103285700083, 0.001982291927561164], [0.0019996606279164553, 0.002100819954648614, 0.002011501230299473, 0.00222789472900331, 0.0020315046422183514, 0.0020209692884236574, 0.0018972812686115503], [0.0019404401537030935, 0.001998294610530138, 0.002023017266765237, 0.0021029955241829157, 0.00195328495465219, 0.0020936334040015936, 0.0020305835641920567], [0.0018462236039340496, 0.002105464693158865, 0.001960600959137082, 0.0020630822982639074, 0.002010447671636939, 0.001861116150394082, 0.0019247423624619842], [0.0018972725374624133, 0.0020896100904792547, 0.002025584690272808, 0.0021661592181771994, 0.0020235241390764713, 0.0019547005649656057, 0.001829158398322761], [0.0019221949623897672, 0.002155434340238571, 0.0020251492969691753, 0.0021360444370657206, 0.0019315291428938508, 0.001935217878781259, 0.0020276906434446573], [0.0020533825736492872, 0.002221213886514306, 0.0021053319796919823, 0.00226296903565526, 0.001921007176861167, 0.002011587144806981, 0.0018843512516468763]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-19f2fd06dd98456dafdfb40bedcfff28\", \"include_layers\": [0], \"include_heads\": [0, 1], \"total_heads\": 2}; // HACK: {\"attention\": [{\"name\": \"Encoder\", \"attn\": [[[[0.0018464146414771676, 0.0018001589924097061, 0.0017344491789117455, 0.0017507843440398574, 0.0017416394548490644, 0.002001393586397171, 0.0019082187209278345], [0.001972303492948413, 0.0019028690876439214, 0.0019574479665607214, 0.001994171878322959, 0.002041721483692527, 0.001843519159592688, 0.0020233222749084234], [0.001910420716740191, 0.0019798160064965487, 0.0018404732691124082, 0.0018303972901776433, 0.001803202903829515, 0.001991175813600421, 0.002007290255278349], [0.00193652359303087, 0.0019385053310543299, 0.0017446685815230012, 0.001843749894760549, 0.0019058147445321083, 0.001891041873022914, 0.0019511611899361014], [0.0018211458809673786, 0.0022737483959645033, 0.0019070394337177277, 0.001960636116564274, 0.0019712240900844336, 0.0018031225772574544, 0.0019608011934906244], [0.0020816256292164326, 0.0019198638619855046, 0.0020577104296535254, 0.002013441873714328, 0.002247920725494623, 0.0019339921418577433, 0.0019799477886408567], [0.0020214286632835865, 0.001893710228614509, 0.001992067787796259, 0.002000984037294984, 0.0021547002252191305, 0.0019538195338100195, 0.0019273399375379086]], [[0.0018187944078817964, 0.001985579263418913, 0.0019260745029896498, 0.00180947151966393, 0.0019689216278493404, 0.0020192016381770372, 0.002064069965854287], [0.0017711446853354573, 0.0018826526356860995, 0.0018535854760557413, 0.0019155882764607668, 0.0018425106536597013, 0.002035740530118346, 0.0019385528285056353], [0.002136766677722335, 0.0018885541940107942, 0.002194999484345317, 0.0020226400811225176, 0.00204380601644516, 0.0019325066823512316, 0.001852415967732668], [0.001960150897502899, 0.0018585346406325698, 0.002058140467852354, 0.002054718090221286, 0.001816323958337307, 0.0019814076367765665, 0.001978799467906356], [0.0020187837071716785, 0.0021511209197342396, 0.001890729647129774, 0.0019266529707238078, 0.001968749798834324, 0.0019305276218801737, 0.0018384315771982074], [0.00207484420388937, 0.0020477715879678726, 0.0021526317577809095, 0.0020190495997667313, 0.002208051737397909, 0.0019488539546728134, 0.0020111138001084328], [0.0020642303861677647, 0.00188267242629081, 0.002075662836432457, 0.0017805399838835, 0.0019012733828276396, 0.001902154297567904, 0.0019424210768193007]]]], \"left_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}, {\"name\": \"Decoder\", \"attn\": [[[[0.0018792246701195836, 0.0018335148924961686, 0.001820812001824379, 0.0017954667564481497, 0.0021011088974773884, 0.0020436758641153574, 0.0019277126993983984], [0.0017988268518820405, 0.0017897641519084573, 0.0018982256297022104, 0.0017840112559497356, 0.0019622761756181717, 0.0020553539507091045, 0.001958946930244565], [0.0016729023773223162, 0.0017019616207107902, 0.0018107111100107431, 0.0017706295475363731, 0.001895493478514254, 0.0019761521834880114, 0.0019466831581667066], [0.0018859697738662362, 0.0018127233488485217, 0.0018587384838610888, 0.0019052211428061128, 0.002037316095083952, 0.0019663453567773104, 0.0018807054730132222], [0.0018968258518725634, 0.0017964169383049011, 0.0020061549730598927, 0.0017624899046495557, 0.001969343051314354, 0.0020240950398147106, 0.0019146257545799017], [0.0018185381777584553, 0.0018189316615462303, 0.001893619541078806, 0.001837545190937817, 0.0019073606235906482, 0.0020435380283743143, 0.001931778620928526], [0.0017854087054729462, 0.0016933033475652337, 0.0018706403207033873, 0.0017497813096269965, 0.0018687675474211574, 0.0019071969436481595, 0.001976992003619671]], [[0.0018866317113861442, 0.001884565339423716, 0.0018271380104124546, 0.002000353531911969, 0.0018941479502245784, 0.00190870207734406, 0.0019586512353271246], [0.0019458665046840906, 0.001955003011971712, 0.001763446838594973, 0.002025987720116973, 0.0020241239108145237, 0.002011059084907174, 0.0019334204262122512], [0.0019150797743350267, 0.00201409007422626, 0.0018144450150430202, 0.001973160309717059, 0.0018947036005556583, 0.0019690273329615593, 0.001973600359633565], [0.0020460551604628563, 0.001991517608985305, 0.0019401645986363292, 0.001956542255356908, 0.001962150912731886, 0.001956462860107422, 0.0019380557350814342], [0.001826889580115676, 0.00197908584959805, 0.0019266741583123803, 0.0021761287935078144, 0.0020787979010492563, 0.0019404739141464233, 0.0019756497349590063], [0.0020150511991232634, 0.001968350261449814, 0.0019453623099252582, 0.0022021098993718624, 0.0018916501430794597, 0.0018871808424592018, 0.0020191394723951817], [0.0020470020826905966, 0.001941175083629787, 0.0018237376352772117, 0.0021195411682128906, 0.002066523302346468, 0.0020058455411344767, 0.0019738110713660717]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"]}, {\"name\": \"Cross\", \"attn\": [[[[0.0018928833305835724, 0.001979365013539791, 0.0020256545394659042, 0.0019819485023617744, 0.0019882870838046074, 0.0019005568465217948, 0.0019277624087408185], [0.002064631786197424, 0.0020102073904126883, 0.0020302797202020884, 0.0019594456534832716, 0.0020429627038538456, 0.0020231925882399082, 0.0019938661716878414], [0.0020769962575286627, 0.0019761205185204744, 0.0019977958872914314, 0.001938159461133182, 0.001975645311176777, 0.001930567785166204, 0.002015835838392377], [0.0019813713151961565, 0.0020163939334452152, 0.002016756683588028, 0.001959118526428938, 0.00204226141795516, 0.002010411350056529, 0.0020092870108783245], [0.0021182207856327295, 0.0020402008667588234, 0.002106484491378069, 0.0019699756521731615, 0.0020870943553745747, 0.00196444196626544, 0.002070276765152812], [0.002075962023809552, 0.002053468720987439, 0.0021034383680671453, 0.0019978636410087347, 0.0020043132826685905, 0.0020100504625588655, 0.0019626449793577194], [0.0019920789636671543, 0.0019896121229976416, 0.00203309184871614, 0.0019026115769520402, 0.001997563987970352, 0.0019834409467875957, 0.001937332795932889]], [[0.0020585348829627037, 0.0022093539591878653, 0.002054172568023205, 0.002214851789176464, 0.001976896310225129, 0.0022126103285700083, 0.001982291927561164], [0.0019996606279164553, 0.002100819954648614, 0.002011501230299473, 0.00222789472900331, 0.0020315046422183514, 0.0020209692884236574, 0.0018972812686115503], [0.0019404401537030935, 0.001998294610530138, 0.002023017266765237, 0.0021029955241829157, 0.00195328495465219, 0.0020936334040015936, 0.0020305835641920567], [0.0018462236039340496, 0.002105464693158865, 0.001960600959137082, 0.0020630822982639074, 0.002010447671636939, 0.001861116150394082, 0.0019247423624619842], [0.0018972725374624133, 0.0020896100904792547, 0.002025584690272808, 0.0021661592181771994, 0.0020235241390764713, 0.0019547005649656057, 0.001829158398322761], [0.0019221949623897672, 0.002155434340238571, 0.0020251492969691753, 0.0021360444370657206, 0.0019315291428938508, 0.001935217878781259, 0.0020276906434446573], [0.0020533825736492872, 0.002221213886514306, 0.0021053319796919823, 0.00226296903565526, 0.001921007176861167, 0.002011587144806981, 0.0018843512516468763]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-19f2fd06dd98456dafdfb40bedcfff28\", \"include_layers\": [0], \"include_heads\": [0, 1], \"total_heads\": 2} is a template marker that is replaced by actual params.\n",
       "        const config = {};\n",
       "\n",
       "        const MIN_X = 0;\n",
       "        const MIN_Y = 0;\n",
       "        const DIV_WIDTH = 970;\n",
       "        const THUMBNAIL_PADDING = 5;\n",
       "        const DETAIL_WIDTH = 300;\n",
       "        const DETAIL_ATTENTION_WIDTH = 140;\n",
       "        const DETAIL_BOX_WIDTH = 80;\n",
       "        const DETAIL_BOX_HEIGHT = 18;\n",
       "        const DETAIL_PADDING = 15;\n",
       "        const ATTN_PADDING = 0;\n",
       "        const DETAIL_HEADING_HEIGHT = 25;\n",
       "        const HEADING_TEXT_SIZE = 15;\n",
       "        const HEADING_PADDING = 5;\n",
       "        const TEXT_SIZE = 13;\n",
       "        const TEXT_PADDING = 5;\n",
       "        const LAYER_COLORS = d3.schemeCategory10;\n",
       "        const PALETTE = {\n",
       "            'light': {\n",
       "                'text': 'black',\n",
       "                'background': 'white',\n",
       "                'highlight': '#F5F5F5'\n",
       "            },\n",
       "            'dark': {\n",
       "                'text': '#ccc',\n",
       "                'background': 'black',\n",
       "                'highlight': '#222'\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function render() {\n",
       "\n",
       "            // Set global state variables\n",
       "\n",
       "            var attData = config.attention[config.filter];\n",
       "            config.leftText = attData.left_text;\n",
       "            config.rightText = attData.right_text;\n",
       "            config.attn = attData.attn;\n",
       "            config.numLayers = config.attn.length;\n",
       "            config.numHeads = config.attn[0].length;\n",
       "            config.thumbnailBoxHeight = 7 * (12 / config.totalHeads);\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            config.thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n",
       "            config.thumbnailWidth = (DIV_WIDTH - axisSize) / config.totalHeads;\n",
       "            config.detailHeight = Math.max(config.leftText.length, config.rightText.length) * DETAIL_BOX_HEIGHT + 2 * DETAIL_PADDING + DETAIL_HEADING_HEIGHT;\n",
       "            config.divHeight = Math.max(config.numLayers * config.thumbnailHeight + axisSize, config.detailHeight);\n",
       "\n",
       "            const vis = $(`#${config.rootDivId} #vis`)\n",
       "            vis.empty();\n",
       "            vis.attr(\"height\", config.divHeight);\n",
       "            config.svg = d3.select(`#${config.rootDivId} #vis`)\n",
       "                .append('svg')\n",
       "                .attr(\"width\", DIV_WIDTH)\n",
       "                .attr(\"height\", config.divHeight)\n",
       "                .attr(\"fill\", getBackgroundColor());\n",
       "\n",
       "            renderAxisLabels();\n",
       "\n",
       "            var i;\n",
       "            var j;\n",
       "            for (i = 0; i < config.numLayers; i++) {\n",
       "                for (j = 0; j < config.numHeads; j++) {\n",
       "                    renderThumbnail(i, j);\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function renderAxisLabels() {\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            const tableWidth = config.thumbnailWidth * config.heads.length;\n",
       "            config.svg.append(\"text\")\n",
       "                .text(\"Heads\")\n",
       "                .attr(\"fill\", \"black\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n",
       "                .attr(\"x\", axisSize + tableWidth / 2)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", 0)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "            for (let i = 0; i < config.numHeads; i++) {\n",
       "                config.svg.append(\"text\")\n",
       "                    .text(config.heads[i])\n",
       "                    .attr(\"fill\", \"black\")\n",
       "                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                    .attr(\"x\", axisSize + (i + .5) * config.thumbnailWidth)\n",
       "                    .attr(\"text-anchor\", \"middle\")\n",
       "                    .attr(\"y\", HEADING_TEXT_SIZE + HEADING_PADDING)\n",
       "                    .attr(\"dy\", TEXT_SIZE);\n",
       "            }\n",
       "            let x = 0;\n",
       "            let y = axisSize + config.thumbnailHeight * config.layers.length / 2;\n",
       "            console.log(\"x\", x, y)\n",
       "            config.svg.append(\"text\")\n",
       "                .text(\"Layers\")\n",
       "                .attr(\"fill\", \"black\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .attr(\"transform\", \"rotate(270, \" + x  + \", \" + y + \")\")\n",
       "                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "            for (let i = 0; i < config.numLayers; i++) {\n",
       "                x = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE; // HACK\n",
       "                y = axisSize + (i + .5) * config.thumbnailHeight;\n",
       "                config.svg.append(\"text\")\n",
       "                    .text(config.layers[i])\n",
       "                    .attr(\"fill\", \"black\")\n",
       "                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                    .attr(\"x\", x)\n",
       "                    .attr(\"text-anchor\", \"end\")\n",
       "                    .attr(\"y\", y)\n",
       "                    .attr(\"dy\", TEXT_SIZE / 2);\n",
       "            }\n",
       "        }\n",
       "\n",
       "\n",
       "        function renderThumbnail(layerIndex, headIndex) {\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING\n",
       "            const x = headIndex * config.thumbnailWidth + axisSize;\n",
       "            const y = layerIndex * config.thumbnailHeight + axisSize;\n",
       "            renderThumbnailAttn(x, y, config.attn[layerIndex][headIndex], layerIndex, headIndex);\n",
       "        }\n",
       "\n",
       "        function renderDetail(att, layerIndex, headIndex) {\n",
       "            const axisSize = TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            var xOffset = .8 * config.thumbnailWidth;\n",
       "            var maxX = DIV_WIDTH;\n",
       "            var maxY = config.divHeight - 3;\n",
       "            var leftPos = axisSize + headIndex * config.thumbnailWidth;\n",
       "            var x = leftPos + THUMBNAIL_PADDING + xOffset;\n",
       "            if (x < MIN_X) {\n",
       "                x = MIN_X;\n",
       "            } else if (x + DETAIL_WIDTH > maxX) {\n",
       "                x = leftPos + THUMBNAIL_PADDING - DETAIL_WIDTH + 8;\n",
       "            }\n",
       "            var posLeftText = x;\n",
       "            var posAttention = posLeftText + DETAIL_BOX_WIDTH;\n",
       "            var posRightText = posAttention + DETAIL_ATTENTION_WIDTH;\n",
       "            var thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n",
       "            var yOffset = 20;\n",
       "            var y = layerIndex * thumbnailHeight + THUMBNAIL_PADDING + yOffset;\n",
       "            if (y < MIN_Y) {\n",
       "                y = MIN_Y;\n",
       "            } else if (y + config.detailHeight > maxY) {\n",
       "                y = maxY - config.detailHeight;\n",
       "            }\n",
       "            renderDetailFrame(x, y, layerIndex);\n",
       "            y = y + DETAIL_PADDING;\n",
       "            renderDetailHeading(x, y, layerIndex, headIndex);\n",
       "            y = y + DETAIL_HEADING_HEIGHT;\n",
       "            renderDetailText(config.leftText, \"leftText\", posLeftText, y , layerIndex);\n",
       "            renderDetailAttn(posAttention, y, att, layerIndex, headIndex);\n",
       "            renderDetailText(config.rightText, \"rightText\", posRightText, y, layerIndex);\n",
       "        }\n",
       "\n",
       "        function renderDetailHeading(x, y, layerIndex, headIndex) {\n",
       "            var fillColor = getTextColor();\n",
       "            config.svg.append(\"text\")\n",
       "                .classed(\"detail\", true)\n",
       "                .text('Layer ' + config.layers[layerIndex] + \", Head \" + config.heads[headIndex])\n",
       "                .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .style(\"cursor\", \"default\")\n",
       "                .style(\"-webkit-user-select\", \"none\")\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .attr(\"x\", x + DETAIL_WIDTH / 2)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", DETAIL_HEADING_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_WIDTH)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "        }\n",
       "\n",
       "        function renderDetailText(text, id, x, y, layerIndex) {\n",
       "            var tokenContainer = config.svg.append(\"svg:g\")\n",
       "                .classed(\"detail\", true)\n",
       "                .selectAll(\"g\")\n",
       "                .data(text)\n",
       "                .enter()\n",
       "                .append(\"g\");\n",
       "\n",
       "            var fillColor = getTextColor();\n",
       "\n",
       "            tokenContainer.append(\"rect\")\n",
       "                .classed(\"highlight\", true)\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .style(\"opacity\", 0.0)\n",
       "                .attr(\"height\", DETAIL_BOX_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_BOX_WIDTH)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", function (d, i) {\n",
       "                    return y + i * DETAIL_BOX_HEIGHT;\n",
       "                });\n",
       "\n",
       "            var textContainer = tokenContainer.append(\"text\")\n",
       "                .classed(\"token\", true)\n",
       "                .text(function (d) {\n",
       "                    return d;\n",
       "                })\n",
       "                .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                .style(\"cursor\", \"default\")\n",
       "                .style(\"-webkit-user-select\", \"none\")\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", function (d, i) {\n",
       "                    return i * DETAIL_BOX_HEIGHT + y;\n",
       "                })\n",
       "                .attr(\"height\", DETAIL_BOX_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_BOX_WIDTH)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "\n",
       "            if (id == \"leftText\") {\n",
       "                textContainer.style(\"text-anchor\", \"end\")\n",
       "                    .attr(\"dx\", DETAIL_BOX_WIDTH - 2);\n",
       "                tokenContainer.on(\"mouseover\", function (d, index) {\n",
       "                    highlightSelection(index);\n",
       "                });\n",
       "                tokenContainer.on(\"mouseleave\", function () {\n",
       "                    unhighlightSelection();\n",
       "                });\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function highlightSelection(index) {\n",
       "            config.svg.select(\"#leftText\")\n",
       "                .selectAll(\".highlight\")\n",
       "                .style(\"opacity\", function (d, i) {\n",
       "                    return i == index ? 1.0 : 0.0;\n",
       "                });\n",
       "            config.svg.selectAll(\".attn-line-group\")\n",
       "                .style(\"opacity\", function (d, i) {\n",
       "                    return i == index ? 1.0 : 0.0;\n",
       "                });\n",
       "        }\n",
       "\n",
       "        function unhighlightSelection() {\n",
       "            config.svg.select(\"#leftText\")\n",
       "                .selectAll(\".highlight\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "            config.svg.selectAll(\".attn-line-group\")\n",
       "                .style(\"opacity\", 1);\n",
       "        }\n",
       "\n",
       "        function renderThumbnailAttn(x, y, att, layerIndex, headIndex) {\n",
       "\n",
       "            var attnContainer = config.svg.append(\"svg:g\");\n",
       "\n",
       "            var attnBackground = attnContainer.append(\"rect\")\n",
       "                .attr(\"id\", 'attn_background_' + layerIndex + \"_\" + headIndex)\n",
       "                .classed(\"attn_background\", true)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.thumbnailHeight)\n",
       "                .attr(\"width\", config.thumbnailWidth)\n",
       "                .attr(\"stroke-width\", 2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", 0)\n",
       "                .attr(\"fill\", getBackgroundColor());\n",
       "            var x1 = x + THUMBNAIL_PADDING;\n",
       "            var x2 = x1 + config.thumbnailWidth - 14;\n",
       "            var y1 = y + THUMBNAIL_PADDING;\n",
       "\n",
       "            attnContainer.selectAll(\"g\")\n",
       "                .data(att)\n",
       "                .enter()\n",
       "                .append(\"g\") // Add group for each source token\n",
       "                .attr(\"source-index\", function (d, i) { // Save index of source token\n",
       "                    return i;\n",
       "                })\n",
       "                .selectAll(\"line\")\n",
       "                .data(function (d) { // Loop over all target tokens\n",
       "                    return d;\n",
       "                })\n",
       "                .enter() // When entering\n",
       "                .append(\"line\")\n",
       "                .attr(\"x1\", x1)\n",
       "                .attr(\"y1\", function (d) {\n",
       "                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n",
       "                    return y1 + (sourceIndex + .5) * config.thumbnailBoxHeight;\n",
       "                })\n",
       "                .attr(\"x2\", x2)\n",
       "                .attr(\"y2\", function (d, targetIndex) {\n",
       "                    return y1 + (targetIndex + .5) * config.thumbnailBoxHeight;\n",
       "                })\n",
       "                .attr(\"stroke-width\", 2.2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", function (d) {\n",
       "                    return d;\n",
       "                });\n",
       "\n",
       "            var clickRegion = attnContainer.append(\"rect\")\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.thumbnailHeight)\n",
       "                .attr(\"width\", config.thumbnailWidth)\n",
       "                .style(\"opacity\", 0);\n",
       "\n",
       "            clickRegion.on(\"click\", function (d, index) {\n",
       "                var attnBackgroundOther = config.svg.selectAll(\".attn_background\");\n",
       "                attnBackgroundOther.attr(\"fill\", getBackgroundColor());\n",
       "                attnBackgroundOther.attr(\"stroke-opacity\", 0);\n",
       "\n",
       "                config.svg.selectAll(\".detail\").remove();\n",
       "                if (config.detail_layer != layerIndex || config.detail_head != headIndex) {\n",
       "                    renderDetail(att, layerIndex, headIndex);\n",
       "                    config.detail_layer = layerIndex;\n",
       "                    config.detail_head = headIndex;\n",
       "                    attnBackground.attr(\"fill\", getHighlightColor());\n",
       "                    attnBackground.attr(\"stroke-opacity\", .8);\n",
       "                } else {\n",
       "                    config.detail_layer = null;\n",
       "                    config.detail_head = null;\n",
       "                    attnBackground.attr(\"fill\", getBackgroundColor());\n",
       "                    attnBackground.attr(\"stroke-opacity\", 0);\n",
       "                }\n",
       "            });\n",
       "\n",
       "            clickRegion.on(\"mouseover\", function (d) {\n",
       "                d3.select(this).style(\"cursor\", \"pointer\");\n",
       "            });\n",
       "        }\n",
       "\n",
       "        function renderDetailFrame(x, y, layerIndex) {\n",
       "            var detailFrame = config.svg.append(\"rect\")\n",
       "                .classed(\"detail\", true)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.detailHeight)\n",
       "                .attr(\"width\", DETAIL_WIDTH)\n",
       "                .style(\"opacity\", 1)\n",
       "                .attr(\"stroke-width\", 1.5)\n",
       "                .attr(\"stroke-opacity\", 0.7)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex));\n",
       "        }\n",
       "\n",
       "        function renderDetailAttn(x, y, att, layerIndex) {\n",
       "            var attnContainer = config.svg.append(\"svg:g\")\n",
       "                .classed(\"detail\", true)\n",
       "                .attr(\"pointer-events\", \"none\");\n",
       "            attnContainer.selectAll(\"g\")\n",
       "                .data(att)\n",
       "                .enter()\n",
       "                .append(\"g\") // Add group for each source token\n",
       "                .classed('attn-line-group', true)\n",
       "                .attr(\"source-index\", function (d, i) { // Save index of source token\n",
       "                    return i;\n",
       "                })\n",
       "                .selectAll(\"line\")\n",
       "                .data(function (d) { // Loop over all target tokens\n",
       "                    return d;\n",
       "                })\n",
       "                .enter()\n",
       "                .append(\"line\")\n",
       "                .attr(\"x1\", x + ATTN_PADDING)\n",
       "                .attr(\"y1\", function (d) {\n",
       "                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n",
       "                    return y + (sourceIndex + .5) * DETAIL_BOX_HEIGHT;\n",
       "                })\n",
       "                .attr(\"x2\", x + DETAIL_ATTENTION_WIDTH - ATTN_PADDING)\n",
       "                .attr(\"y2\", function (d, targetIndex) {\n",
       "                    return y + (targetIndex + .5) * DETAIL_BOX_HEIGHT;\n",
       "                })\n",
       "                .attr(\"stroke-width\", 2.2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", function (d) {\n",
       "                    return d;\n",
       "                });\n",
       "        }\n",
       "\n",
       "        function getLayerColor(layer) {\n",
       "          return LAYER_COLORS[config.layers[layer] % 10];\n",
       "        }\n",
       "\n",
       "        function getTextColor() {\n",
       "            return PALETTE[config.mode]['text']\n",
       "        }\n",
       "\n",
       "        function getBackgroundColor() {\n",
       "           return PALETTE[config.mode]['background']\n",
       "        }\n",
       "\n",
       "        function getHighlightColor() {\n",
       "           return PALETTE[config.mode]['highlight']\n",
       "        }\n",
       "\n",
       "        function initialize() {\n",
       "            config.attention = params['attention'];\n",
       "            config.filter = params['default_filter'];\n",
       "            config.mode = params['display_mode'];\n",
       "            config.layers = params['include_layers']\n",
       "            config.heads = params['include_heads']\n",
       "            config.totalHeads = params['total_heads']\n",
       "            config.rootDivId = params['root_div_id'];\n",
       "            $(`#${config.rootDivId} #filter`).on('change', function (e) {\n",
       "                config.filter = e.currentTarget.value;\n",
       "                render();\n",
       "            });\n",
       "        }\n",
       "\n",
       "        initialize();\n",
       "        render();\n",
       "\n",
       "    });"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_view(\n",
    "    encoder_attention=tensor_encoder_attn,\n",
    "    decoder_attention=tensor_decoder_attn,\n",
    "    cross_attention=tensor_cross_attn,\n",
    "    encoder_tokens=tokens_a,\n",
    "    decoder_tokens=tokens_b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a14a8b2-fd6e-43b9-981b-eebae0dbc987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-4d244a09cc2c49f49ed8d0ab816d9bac\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                Layer: <select id=\"layer\"></select>\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/**\n",
       " * @fileoverview Transformer Visualization D3 javascript code.\n",
       " *\n",
       " *\n",
       " *  Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n",
       " *\n",
       " * Change log:\n",
       " *\n",
       " * 12/19/18  Jesse Vig   Assorted cleanup. Changed orientation of attention matrices.\n",
       " * 12/29/20  Jesse Vig   Significant refactor.\n",
       " * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n",
       " * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n",
       " * 05/03/21  Jesse Vig   Adjust height of visualization dynamically\n",
       " * 07/25/21  Jesse Vig   Support layer filtering\n",
       " * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n",
       " **/\n",
       "\n",
       "require.config({\n",
       "  paths: {\n",
       "      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
       "    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
       "  }\n",
       "});\n",
       "\n",
       "requirejs(['jquery', 'd3'], function ($, d3) {\n",
       "\n",
       "    const params = {\"attention\": [{\"name\": null, \"attn\": [[[[0.0018464146414771676, 0.0018001589924097061, 0.0017344491789117455, 0.0017507843440398574, 0.0017416394548490644, 0.002001393586397171, 0.0019082187209278345], [0.001972303492948413, 0.0019028690876439214, 0.0019574479665607214, 0.001994171878322959, 0.002041721483692527, 0.001843519159592688, 0.0020233222749084234], [0.001910420716740191, 0.0019798160064965487, 0.0018404732691124082, 0.0018303972901776433, 0.001803202903829515, 0.001991175813600421, 0.002007290255278349], [0.00193652359303087, 0.0019385053310543299, 0.0017446685815230012, 0.001843749894760549, 0.0019058147445321083, 0.001891041873022914, 0.0019511611899361014], [0.0018211458809673786, 0.0022737483959645033, 0.0019070394337177277, 0.001960636116564274, 0.0019712240900844336, 0.0018031225772574544, 0.0019608011934906244], [0.0020816256292164326, 0.0019198638619855046, 0.0020577104296535254, 0.002013441873714328, 0.002247920725494623, 0.0019339921418577433, 0.0019799477886408567], [0.0020214286632835865, 0.001893710228614509, 0.001992067787796259, 0.002000984037294984, 0.0021547002252191305, 0.0019538195338100195, 0.0019273399375379086]], [[0.0018187944078817964, 0.001985579263418913, 0.0019260745029896498, 0.00180947151966393, 0.0019689216278493404, 0.0020192016381770372, 0.002064069965854287], [0.0017711446853354573, 0.0018826526356860995, 0.0018535854760557413, 0.0019155882764607668, 0.0018425106536597013, 0.002035740530118346, 0.0019385528285056353], [0.002136766677722335, 0.0018885541940107942, 0.002194999484345317, 0.0020226400811225176, 0.00204380601644516, 0.0019325066823512316, 0.001852415967732668], [0.001960150897502899, 0.0018585346406325698, 0.002058140467852354, 0.002054718090221286, 0.001816323958337307, 0.0019814076367765665, 0.001978799467906356], [0.0020187837071716785, 0.0021511209197342396, 0.001890729647129774, 0.0019266529707238078, 0.001968749798834324, 0.0019305276218801737, 0.0018384315771982074], [0.00207484420388937, 0.0020477715879678726, 0.0021526317577809095, 0.0020190495997667313, 0.002208051737397909, 0.0019488539546728134, 0.0020111138001084328], [0.0020642303861677647, 0.00188267242629081, 0.002075662836432457, 0.0017805399838835, 0.0019012733828276396, 0.001902154297567904, 0.0019424210768193007]]]], \"left_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}], \"default_filter\": \"0\", \"root_div_id\": \"bertviz-4d244a09cc2c49f49ed8d0ab816d9bac\", \"layer\": null, \"heads\": null, \"include_layers\": [0]}; // HACK: {\"attention\": [{\"name\": null, \"attn\": [[[[0.0018464146414771676, 0.0018001589924097061, 0.0017344491789117455, 0.0017507843440398574, 0.0017416394548490644, 0.002001393586397171, 0.0019082187209278345], [0.001972303492948413, 0.0019028690876439214, 0.0019574479665607214, 0.001994171878322959, 0.002041721483692527, 0.001843519159592688, 0.0020233222749084234], [0.001910420716740191, 0.0019798160064965487, 0.0018404732691124082, 0.0018303972901776433, 0.001803202903829515, 0.001991175813600421, 0.002007290255278349], [0.00193652359303087, 0.0019385053310543299, 0.0017446685815230012, 0.001843749894760549, 0.0019058147445321083, 0.001891041873022914, 0.0019511611899361014], [0.0018211458809673786, 0.0022737483959645033, 0.0019070394337177277, 0.001960636116564274, 0.0019712240900844336, 0.0018031225772574544, 0.0019608011934906244], [0.0020816256292164326, 0.0019198638619855046, 0.0020577104296535254, 0.002013441873714328, 0.002247920725494623, 0.0019339921418577433, 0.0019799477886408567], [0.0020214286632835865, 0.001893710228614509, 0.001992067787796259, 0.002000984037294984, 0.0021547002252191305, 0.0019538195338100195, 0.0019273399375379086]], [[0.0018187944078817964, 0.001985579263418913, 0.0019260745029896498, 0.00180947151966393, 0.0019689216278493404, 0.0020192016381770372, 0.002064069965854287], [0.0017711446853354573, 0.0018826526356860995, 0.0018535854760557413, 0.0019155882764607668, 0.0018425106536597013, 0.002035740530118346, 0.0019385528285056353], [0.002136766677722335, 0.0018885541940107942, 0.002194999484345317, 0.0020226400811225176, 0.00204380601644516, 0.0019325066823512316, 0.001852415967732668], [0.001960150897502899, 0.0018585346406325698, 0.002058140467852354, 0.002054718090221286, 0.001816323958337307, 0.0019814076367765665, 0.001978799467906356], [0.0020187837071716785, 0.0021511209197342396, 0.001890729647129774, 0.0019266529707238078, 0.001968749798834324, 0.0019305276218801737, 0.0018384315771982074], [0.00207484420388937, 0.0020477715879678726, 0.0021526317577809095, 0.0020190495997667313, 0.002208051737397909, 0.0019488539546728134, 0.0020111138001084328], [0.0020642303861677647, 0.00188267242629081, 0.002075662836432457, 0.0017805399838835, 0.0019012733828276396, 0.001902154297567904, 0.0019424210768193007]]]], \"left_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}], \"default_filter\": \"0\", \"root_div_id\": \"bertviz-4d244a09cc2c49f49ed8d0ab816d9bac\", \"layer\": null, \"heads\": null, \"include_layers\": [0]} is a template marker that is replaced by actual params.\n",
       "    const TEXT_SIZE = 15;\n",
       "    const BOXWIDTH = 110;\n",
       "    const BOXHEIGHT = 22.5;\n",
       "    const MATRIX_WIDTH = 115;\n",
       "    const CHECKBOX_SIZE = 20;\n",
       "    const TEXT_TOP = 30;\n",
       "\n",
       "    console.log(\"d3 version\", d3.version)\n",
       "    let headColors;\n",
       "    try {\n",
       "        headColors = d3.scaleOrdinal(d3.schemeCategory10);\n",
       "    } catch (err) {\n",
       "        console.log('Older d3 version')\n",
       "        headColors = d3.scale.category10();\n",
       "    }\n",
       "    let config = {};\n",
       "    initialize();\n",
       "    renderVis();\n",
       "\n",
       "    function initialize() {\n",
       "        config.attention = params['attention'];\n",
       "        config.filter = params['default_filter'];\n",
       "        config.rootDivId = params['root_div_id'];\n",
       "        config.nLayers = config.attention[config.filter]['attn'].length;\n",
       "        config.nHeads = config.attention[config.filter]['attn'][0].length;\n",
       "        config.layers = params['include_layers']\n",
       "\n",
       "        if (params['heads']) {\n",
       "            config.headVis = new Array(config.nHeads).fill(false);\n",
       "            params['heads'].forEach(x => config.headVis[x] = true);\n",
       "        } else {\n",
       "            config.headVis = new Array(config.nHeads).fill(true);\n",
       "        }\n",
       "        config.initialTextLength = config.attention[config.filter].right_text.length;\n",
       "        config.layer_seq = (params['layer'] == null ? 0 : config.layers.findIndex(layer => params['layer'] === layer));\n",
       "        config.layer = config.layers[config.layer_seq]\n",
       "\n",
       "        let layerEl = $(`#${config.rootDivId} #layer`);\n",
       "        for (const layer of config.layers) {\n",
       "            layerEl.append($(\"<option />\").val(layer).text(layer));\n",
       "        }\n",
       "        layerEl.val(config.layer).change();\n",
       "        layerEl.on('change', function (e) {\n",
       "            config.layer = +e.currentTarget.value;\n",
       "            config.layer_seq = config.layers.findIndex(layer => config.layer === layer);\n",
       "            renderVis();\n",
       "        });\n",
       "\n",
       "        $(`#${config.rootDivId} #filter`).on('change', function (e) {\n",
       "            config.filter = e.currentTarget.value;\n",
       "            renderVis();\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function renderVis() {\n",
       "\n",
       "        // Load parameters\n",
       "        const attnData = config.attention[config.filter];\n",
       "        const leftText = attnData.left_text;\n",
       "        const rightText = attnData.right_text;\n",
       "\n",
       "        // Select attention for given layer\n",
       "        const layerAttention = attnData.attn[config.layer_seq];\n",
       "\n",
       "        // Clear vis\n",
       "        $(`#${config.rootDivId} #vis`).empty();\n",
       "\n",
       "        // Determine size of visualization\n",
       "        const height = Math.max(leftText.length, rightText.length) * BOXHEIGHT + TEXT_TOP;\n",
       "        const svg = d3.select(`#${config.rootDivId} #vis`)\n",
       "            .append('svg')\n",
       "            .attr(\"width\", \"100%\")\n",
       "            .attr(\"height\", height + \"px\");\n",
       "\n",
       "        // Display tokens on left and right side of visualization\n",
       "        renderText(svg, leftText, true, layerAttention, 0);\n",
       "        renderText(svg, rightText, false, layerAttention, MATRIX_WIDTH + BOXWIDTH);\n",
       "\n",
       "        // Render attention arcs\n",
       "        renderAttention(svg, layerAttention);\n",
       "\n",
       "        // Draw squares at top of visualization, one for each head\n",
       "        drawCheckboxes(0, svg, layerAttention);\n",
       "    }\n",
       "\n",
       "    function renderText(svg, text, isLeft, attention, leftPos) {\n",
       "\n",
       "        const textContainer = svg.append(\"svg:g\")\n",
       "            .attr(\"id\", isLeft ? \"left\" : \"right\");\n",
       "\n",
       "        // Add attention highlights superimposed over words\n",
       "        textContainer.append(\"g\")\n",
       "            .classed(\"attentionBoxes\", true)\n",
       "            .selectAll(\"g\")\n",
       "            .data(attention)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .attr(\"head-index\", (d, i) => i)\n",
       "            .selectAll(\"rect\")\n",
       "            .data(d => isLeft ? d : transpose(d)) // if right text, transpose attention to get right-to-left weights\n",
       "            .enter()\n",
       "            .append(\"rect\")\n",
       "            .attr(\"x\", function () {\n",
       "                var headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                return leftPos + boxOffsets(headIndex);\n",
       "            })\n",
       "            .attr(\"y\", (+1) * BOXHEIGHT)\n",
       "            .attr(\"width\", BOXWIDTH / activeHeads())\n",
       "            .attr(\"height\", BOXHEIGHT)\n",
       "            .attr(\"fill\", function () {\n",
       "                return headColors(+this.parentNode.getAttribute(\"head-index\"))\n",
       "            })\n",
       "            .style(\"opacity\", 0.0);\n",
       "\n",
       "        const tokenContainer = textContainer.append(\"g\").selectAll(\"g\")\n",
       "            .data(text)\n",
       "            .enter()\n",
       "            .append(\"g\");\n",
       "\n",
       "        // Add gray background that appears when hovering over text\n",
       "        tokenContainer.append(\"rect\")\n",
       "            .classed(\"background\", true)\n",
       "            .style(\"opacity\", 0.0)\n",
       "            .attr(\"fill\", \"lightgray\")\n",
       "            .attr(\"x\", leftPos)\n",
       "            .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT)\n",
       "            .attr(\"width\", BOXWIDTH)\n",
       "            .attr(\"height\", BOXHEIGHT);\n",
       "\n",
       "        // Add token text\n",
       "        const textEl = tokenContainer.append(\"text\")\n",
       "            .text(d => d)\n",
       "            .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "            .style(\"cursor\", \"default\")\n",
       "            .style(\"-webkit-user-select\", \"none\")\n",
       "            .attr(\"x\", leftPos)\n",
       "            .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT);\n",
       "\n",
       "        if (isLeft) {\n",
       "            textEl.style(\"text-anchor\", \"end\")\n",
       "                .attr(\"dx\", BOXWIDTH - 0.5 * TEXT_SIZE)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "        } else {\n",
       "            textEl.style(\"text-anchor\", \"start\")\n",
       "                .attr(\"dx\", +0.5 * TEXT_SIZE)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "        }\n",
       "\n",
       "        tokenContainer.on(\"mouseover\", function (d, index) {\n",
       "\n",
       "            // Show gray background for moused-over token\n",
       "            textContainer.selectAll(\".background\")\n",
       "                .style(\"opacity\", (d, i) => i === index ? 1.0 : 0.0)\n",
       "\n",
       "            // Reset visibility attribute for any previously highlighted attention arcs\n",
       "            svg.select(\"#attention\")\n",
       "                .selectAll(\"line[visibility='visible']\")\n",
       "                .attr(\"visibility\", null)\n",
       "\n",
       "            // Hide group containing attention arcs\n",
       "            svg.select(\"#attention\").attr(\"visibility\", \"hidden\");\n",
       "\n",
       "            // Set to visible appropriate attention arcs to be highlighted\n",
       "            if (isLeft) {\n",
       "                svg.select(\"#attention\").selectAll(\"line[left-token-index='\" + index + \"']\").attr(\"visibility\", \"visible\");\n",
       "            } else {\n",
       "                svg.select(\"#attention\").selectAll(\"line[right-token-index='\" + index + \"']\").attr(\"visibility\", \"visible\");\n",
       "            }\n",
       "\n",
       "            // Update color boxes superimposed over tokens\n",
       "            const id = isLeft ? \"right\" : \"left\";\n",
       "            const leftPos = isLeft ? MATRIX_WIDTH + BOXWIDTH : 0;\n",
       "            svg.select(\"#\" + id)\n",
       "                .selectAll(\".attentionBoxes\")\n",
       "                .selectAll(\"g\")\n",
       "                .attr(\"head-index\", (d, i) => i)\n",
       "                .selectAll(\"rect\")\n",
       "                .attr(\"x\", function () {\n",
       "                    const headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                    return leftPos + boxOffsets(headIndex);\n",
       "                })\n",
       "                .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT)\n",
       "                .attr(\"width\", BOXWIDTH / activeHeads())\n",
       "                .attr(\"height\", BOXHEIGHT)\n",
       "                .style(\"opacity\", function (d) {\n",
       "                    const headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                    if (config.headVis[headIndex])\n",
       "                        if (d) {\n",
       "                            return d[index];\n",
       "                        } else {\n",
       "                            return 0.0;\n",
       "                        }\n",
       "                    else\n",
       "                        return 0.0;\n",
       "                });\n",
       "        });\n",
       "\n",
       "        textContainer.on(\"mouseleave\", function () {\n",
       "\n",
       "            // Unhighlight selected token\n",
       "            d3.select(this).selectAll(\".background\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "\n",
       "            // Reset visibility attributes for previously selected lines\n",
       "            svg.select(\"#attention\")\n",
       "                .selectAll(\"line[visibility='visible']\")\n",
       "                .attr(\"visibility\", null) ;\n",
       "            svg.select(\"#attention\").attr(\"visibility\", \"visible\");\n",
       "\n",
       "            // Reset highlights superimposed over tokens\n",
       "            svg.selectAll(\".attentionBoxes\")\n",
       "                .selectAll(\"g\")\n",
       "                .selectAll(\"rect\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function renderAttention(svg, attention) {\n",
       "\n",
       "        // Remove previous dom elements\n",
       "        svg.select(\"#attention\").remove();\n",
       "\n",
       "        // Add new elements\n",
       "        svg.append(\"g\")\n",
       "            .attr(\"id\", \"attention\") // Container for all attention arcs\n",
       "            .selectAll(\".headAttention\")\n",
       "            .data(attention)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .classed(\"headAttention\", true) // Group attention arcs by head\n",
       "            .attr(\"head-index\", (d, i) => i)\n",
       "            .selectAll(\".tokenAttention\")\n",
       "            .data(d => d)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .classed(\"tokenAttention\", true) // Group attention arcs by left token\n",
       "            .attr(\"left-token-index\", (d, i) => i)\n",
       "            .selectAll(\"line\")\n",
       "            .data(d => d)\n",
       "            .enter()\n",
       "            .append(\"line\")\n",
       "            .attr(\"x1\", BOXWIDTH)\n",
       "            .attr(\"y1\", function () {\n",
       "                const leftTokenIndex = +this.parentNode.getAttribute(\"left-token-index\")\n",
       "                return TEXT_TOP + leftTokenIndex * BOXHEIGHT + (BOXHEIGHT / 2)\n",
       "            })\n",
       "            .attr(\"x2\", BOXWIDTH + MATRIX_WIDTH)\n",
       "            .attr(\"y2\", (d, rightTokenIndex) => TEXT_TOP + rightTokenIndex * BOXHEIGHT + (BOXHEIGHT / 2))\n",
       "            .attr(\"stroke-width\", 2)\n",
       "            .attr(\"stroke\", function () {\n",
       "                const headIndex = +this.parentNode.parentNode.getAttribute(\"head-index\");\n",
       "                return headColors(headIndex)\n",
       "            })\n",
       "            .attr(\"left-token-index\", function () {\n",
       "                return +this.parentNode.getAttribute(\"left-token-index\")\n",
       "            })\n",
       "            .attr(\"right-token-index\", (d, i) => i)\n",
       "        ;\n",
       "        updateAttention(svg)\n",
       "    }\n",
       "\n",
       "    function updateAttention(svg) {\n",
       "        svg.select(\"#attention\")\n",
       "            .selectAll(\"line\")\n",
       "            .attr(\"stroke-opacity\", function (d) {\n",
       "                const headIndex = +this.parentNode.parentNode.getAttribute(\"head-index\");\n",
       "                // If head is selected\n",
       "                if (config.headVis[headIndex]) {\n",
       "                    // Set opacity to attention weight divided by number of active heads\n",
       "                    return d / activeHeads()\n",
       "                } else {\n",
       "                    return 0.0;\n",
       "                }\n",
       "            })\n",
       "    }\n",
       "\n",
       "    function boxOffsets(i) {\n",
       "        const numHeadsAbove = config.headVis.reduce(\n",
       "            function (acc, val, cur) {\n",
       "                return val && cur < i ? acc + 1 : acc;\n",
       "            }, 0);\n",
       "        return numHeadsAbove * (BOXWIDTH / activeHeads());\n",
       "    }\n",
       "\n",
       "    function activeHeads() {\n",
       "        return config.headVis.reduce(function (acc, val) {\n",
       "            return val ? acc + 1 : acc;\n",
       "        }, 0);\n",
       "    }\n",
       "\n",
       "    function drawCheckboxes(top, svg) {\n",
       "        const checkboxContainer = svg.append(\"g\");\n",
       "        const checkbox = checkboxContainer.selectAll(\"rect\")\n",
       "            .data(config.headVis)\n",
       "            .enter()\n",
       "            .append(\"rect\")\n",
       "            .attr(\"fill\", (d, i) => headColors(i))\n",
       "            .attr(\"x\", (d, i) => i * CHECKBOX_SIZE)\n",
       "            .attr(\"y\", top)\n",
       "            .attr(\"width\", CHECKBOX_SIZE)\n",
       "            .attr(\"height\", CHECKBOX_SIZE);\n",
       "\n",
       "        function updateCheckboxes() {\n",
       "            checkboxContainer.selectAll(\"rect\")\n",
       "                .data(config.headVis)\n",
       "                .attr(\"fill\", (d, i) => d ? headColors(i): lighten(headColors(i)));\n",
       "        }\n",
       "\n",
       "        updateCheckboxes();\n",
       "\n",
       "        checkbox.on(\"click\", function (d, i) {\n",
       "            if (config.headVis[i] && activeHeads() === 1) return;\n",
       "            config.headVis[i] = !config.headVis[i];\n",
       "            updateCheckboxes();\n",
       "            updateAttention(svg);\n",
       "        });\n",
       "\n",
       "        checkbox.on(\"dblclick\", function (d, i) {\n",
       "            // If we double click on the only active head then reset\n",
       "            if (config.headVis[i] && activeHeads() === 1) {\n",
       "                config.headVis = new Array(config.nHeads).fill(true);\n",
       "            } else {\n",
       "                config.headVis = new Array(config.nHeads).fill(false);\n",
       "                config.headVis[i] = true;\n",
       "            }\n",
       "            updateCheckboxes();\n",
       "            updateAttention(svg);\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function lighten(color) {\n",
       "        const c = d3.hsl(color);\n",
       "        const increment = (1 - c.l) * 0.6;\n",
       "        c.l += increment;\n",
       "        c.s -= increment;\n",
       "        return c;\n",
       "    }\n",
       "\n",
       "    function transpose(mat) {\n",
       "        return mat[0].map(function (col, i) {\n",
       "            return mat.map(function (row) {\n",
       "                return row[i];\n",
       "            });\n",
       "        });\n",
       "    }\n",
       "\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head_view(tensor_encoder_attn, tokens_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bcfb925c-45fa-4d8c-a096-6a8c695ff730",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 'a' == 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d8306-63ec-4928-814b-7f04ba5b4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_data = []\n",
    "print(f\"{len(attn_weights)=}\")\n",
    "print(f\"{attn_weights[0].size()=}\")\n",
    "for layer in attn_weights:\n",
    "    print(f\"{layer.size()=}\")\n",
    "    attn_data_layer = []\n",
    "    for head in layer:\n",
    "        attn_data_layer.append(head.detach().cpu().numpy())\n",
    "    attn_data.append(attn_data_layer)\n",
    "\n",
    "    # Create dictionary for attention visualization\n",
    "\n",
    "print(len(attn_data))\n",
    "print(len(attn_data[0]))\n",
    "print(len(attn_data[0][0]))\n",
    "print(len(attn_data[0][0][0]))\n",
    "print(len(attn_data[0][0][0][0]))\n",
    "attn_dict = {\n",
    "    'all': {\n",
    "        'att': attn_data, # Attention weights [num_layers, num_heads, seq_len, seq_len]\n",
    "        'left_text': tokens_a, # List of source tokens\n",
    "        'right_text': tokens_b # List of target tokens\n",
    "    }\n",
    "}\n",
    "# Call BertViz\n",
    "model_view(attn_data, tokens_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866f811-48a3-40f0-b509-f2a1a308c60e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee0a6b-7dbe-45ad-be47-74987676addc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db74cc-9562-47d3-bd70-05a2f9a57eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c37a32-76ad-412a-9d99-dd388b96017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12901d06-4354-4f65-a76d-6ec11704226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(input_words, output_words, attention):\n",
    "    # attention: (input_len, output_len)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    seaborn.heatmap(attention, xticklabels=input_words, yticklabels=output_words, square=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd48ef-285f-4da9-ad6e-f0e48b1c35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = [\"I\", \"am\", \"a\", \"teacher\", \".\"]\n",
    "output_words = [\"Je\", \"suis\", \"un\", \"professeur\", \".\"]\n",
    "attention = attn_weights.mean(dim=1)[0].detach().cpu().numpy()  # shape (input_len, output_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378810d-bf43-4701-882d-f126a24d81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention(input_words, output_words, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6c0af-09fd-4f08-9456-6375f3df0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00d991-7cd8-4d1f-ac00-235c09ae08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(input_sentence, output_sentence, attentions):\n",
    "    # Ensure the sentence is a list of words\n",
    "    if type(input_sentence) == str:\n",
    "        input_sentence = input_sentence.split(' ')\n",
    "    if type(output_sentence) == str:\n",
    "        output_sentence = output_sentence.split(' ')\n",
    "\n",
    "    # Set up figure with colorbar\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_sentence)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58c171-3212-4ce4-a901-3458c4666728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'eng_sentence' is your English sentence and 'french_sentence' is your French sentence\n",
    "# and 'attn_weights' is the attention weights you got from your model\n",
    "visualize_attention(input_words, output_words, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b67d3b-bc43-4685-a096-a0cba40cc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'a' == 'b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de34b5-094e-446c-8b08-97a18e2635d1",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882759cf-c512-4701-acce-0da8efdc6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(dataset, config):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(config.eval_iters)\n",
    "        for k in range(config.eval_iters):\n",
    "            X, Y, _ = dataset.GetBatch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf6a48-2441-4e6b-9070-fcf67b8751ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(tokenizer)\n",
    "model = Transformer(config)\n",
    "dataset = Data(config.block_size, config.batch_size, tokenizer, config.device)\n",
    "dataset.DataSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10ccbf-da9f-4704-a8dc-37e1b64218b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "model.train()\n",
    "model = model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a4db6-01e5-49ce-bff7-449598c5722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29636df1-1fa6-4b0e-b3fb-ed5f6fad567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % config.eval_interval == 0:\n",
    "        losses = estimate_loss(dataset, config)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb, _ = dataset.GetBatch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136de36-6000-4aef-a7e1-c50b62ea9719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
