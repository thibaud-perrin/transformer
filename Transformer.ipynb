{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012f19fa-d88b-4356-b1cb-fd2dce3f7bbb",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359954e4-ec88-4389-ad02-998016bb0311",
   "metadata": {},
   "source": [
    "## Config\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4be0056-d6ee-4416-aa13-69ef8593a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os\n",
    "import math\n",
    "\n",
    "# display\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data\n",
    "from dataclasses import dataclass\n",
    "import tiktoken\n",
    "\n",
    "# Machine leaning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e811e4-75c2-4774-ba77-5b5347c6313d",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997eff3a-807b-4131-8848-26ae66f1854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887222e1-02e2-457e-af0b-5616dcbef351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    \"\"\"A tokenizer class for encoding/decoding text sequences.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor method to initialize special token indices and tokenizer encoding. \n",
    "        \"\"\"\n",
    "        # Initialize special token indices\n",
    "        self.BOS_IDX: int = 100264  # Index for the Beginning of Sentence token\n",
    "        self.EOS_IDX: int = 100265  # Index for the End of Sentence token\n",
    "        self.PAD_IDX: int = 100266  # Index for the Padding token\n",
    "\n",
    "        # Initialize base encoding from tiktoken\n",
    "        cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "        # Initialize the tokenizer's encoding with special tokens added\n",
    "        self.encoder = tiktoken.Encoding(\n",
    "            name=\"cl100k_bep\", # Name for the encoder with BOS, EOS, and PAD tokens added\n",
    "            pat_str=cl100k_base._pat_str, # Pattern string from the base encoding\n",
    "            mergeable_ranks=cl100k_base._mergeable_ranks, # Mergeable ranks from the base encoding\n",
    "            special_tokens={\n",
    "                **cl100k_base._special_tokens, # Special tokens from the base encoding\n",
    "                \"<|bos|>\": self.BOS_IDX,  # BOS token\n",
    "                \"<|eos|>\": self.EOS_IDX,  # EOS token\n",
    "                \"<|pad|>\": self.PAD_IDX,  # PAD token\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Method to return the size of the vocabulary in the tokenizer's encoding.\n",
    "\n",
    "        Returns:\n",
    "            int: The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        return self.encoder.n_vocab\n",
    "\n",
    "\n",
    "    def sequence_padding(self, sequence, max_size: int = 512, device: str = \"cpu\") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Method to add BOS/PAD/EOS special tokens and ensure the sequence length is within the maximum size.\n",
    "\n",
    "        Args:\n",
    "            sequence (torch.Tensor or list): The input sequence.\n",
    "            max_size (int, optional): The maximum allowed size for the sequence. Defaults to 512.\n",
    "            device (str, optional): The device where the tensors will be allocated. Defaults to \"cpu\".\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The processed sequence with special tokens added and length limited.\n",
    "        \"\"\"\n",
    "        assert max_size > 2, f\"[max_size]: {max_size} should be greater than 2\"\n",
    "        # Ensure the sequence is a torch tensor\n",
    "        tensor_sequence = torch.tensor(sequence, dtype=torch.long).to(device) if not torch.is_tensor(sequence) else sequence.to(device)\n",
    "\n",
    "        # Limit the sequence length within (max_size - 2) where 2 corresponding to bos and eos tags\n",
    "        cutted_sequence_size = max(0, min(max_size - 2, tensor_sequence.size()[0]))\n",
    "        tensor_sequence = tensor_sequence[:cutted_sequence_size]\n",
    "        \n",
    "        # Add BOS token\n",
    "        tensor_sequence = torch.cat([torch.tensor([self.BOS_IDX], dtype=torch.long, device=device), tensor_sequence], dim=0)\n",
    "\n",
    "        # Calculate the padding size\n",
    "        padding_size = max_size - tensor_sequence.size()[0] - 1 # expected size - current size - EOS tag\n",
    "\n",
    "        # Create PAD tensor\n",
    "        pad_tensor = torch.full((padding_size,), self.PAD_IDX, dtype=torch.long, device=device)\n",
    "\n",
    "        # Add PAD and EOS tokens\n",
    "        tensor_sequence = torch.cat([tensor_sequence, pad_tensor, torch.tensor([self.EOS_IDX], dtype=torch.long, device=device)], dim=0)\n",
    "        \n",
    "        return tensor_sequence\n",
    "    \n",
    "    def sequence_cleaner(self, sequence):\n",
    "        \"\"\" Method used to remove BOS/PAD/EOS special tokens \"\"\"\n",
    "        # Checking tensor format\n",
    "        list_sequence = sequence.tolist() if torch.is_tensor(sequence) else sequence\n",
    "        def check_special(number):\n",
    "            return number not in [self.BOS_IDX, self.EOS_IDX, self.PAD_IDX]\n",
    "        return list(filter(check_special, list_sequence))\n",
    "\n",
    "    def tokenize(self, sequence, device=\"cpu\") -> list:\n",
    "        \"\"\"\n",
    "        Method to generate a str list of separated tokens token.\n",
    "\n",
    "        Args:\n",
    "            sequence (torch.Tensor or list): The input sequence.\n",
    "            device (str, optional): The device where the tensors will be allocated. Defaults to \"cpu\".\n",
    "\n",
    "        Returns:\n",
    "            list: The processed sequence converted in a list of tokens in string format.\n",
    "        \"\"\"\n",
    "        # Ensure the sequence is a torch tensor\n",
    "        tensor_sequence = torch.tensor(sequence, dtype=torch.long).to(device) if not torch.is_tensor(sequence) else sequence.to(device)\n",
    "        # create batch of idx tokens\n",
    "        tensor_sequence = tensor_sequence.unsqueeze(0).T\n",
    "        # decode all batch to recreate list of separated tokens \n",
    "        tensor_sequence = self.encoder.decode_batch(tensor_sequence.detach().tolist())\n",
    "        return tensor_sequence\n",
    "\n",
    "    def tokenize_from_str(self, sequence, device=\"cpu\") -> list:\n",
    "        return self.tokenize(self.encoder.encode(sequence), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321a98a8-144b-4815-956a-b866c679fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7d7325-63c2-4c89-887d-1d28c6288293",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    \"\"\"Data class that stores the configuration for a Transformer model.\n",
    "\n",
    "    Attributes:\n",
    "        - tokenizer: An instance of the Tokenizer class.\n",
    "        - block_size (int): Number of tokens in each sequence. Defaults to 512.\n",
    "        - batch_size (int): Number of sequences in each batch. Defaults to 12.\n",
    "        - vocab_size (int): Total size of the tokenizer vocabulary. It is set to the size of the tokenizer vocabulary.\n",
    "        - n_layer (int): Number of transformer encoder and decoder blocks (N). Defaults to 1.\n",
    "        - n_head (int): Number of heads in each attention block. Defaults to 2.\n",
    "        - n_embd (int): Token embedding size. This is from the original Transformer paper. Defaults to 128.\n",
    "        - dropout (float): Dropout rate to use in the Transformer model. Defaults to 0.1.\n",
    "        - bias (bool): Indicates whether to use bias in Linears and LayerNorms.\n",
    "            If True, bias is used similar to GPT-2.\n",
    "            If False, it is a bit better and faster. Defaults to False.\n",
    "        - device (str): The device to run the model on. Defaults to 'cpu'. 'cuda' is used if a GPU is available.\n",
    "        - learning_rate (float): Learning rate for the model optimization. Defaults to 3e-4.\n",
    "        - max_iters (int): Number of training steps. Defaults to 20.\n",
    "        - eval_interval (int): Number of steps between each validation dataset. Defaults to 5.\n",
    "        - eval_iters (int): Number of validation epochs. Defaults to 20.\n",
    "        - visualize (bool): Define if we want to get the attention scores.\n",
    "    \"\"\"\n",
    "    tokenizer: any\n",
    "    block_size: int = 512\n",
    "    batch_size: int = 12\n",
    "    n_layer: int = 2 # 6\n",
    "    n_head: int = 4 # 8\n",
    "    n_embd: int = 512 # 512\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = False # True:\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = 'cpu'\n",
    "    learning_rate = 3e-4\n",
    "    max_iters: int = 100\n",
    "    eval_interval: int = 5\n",
    "    eval_iters: int = 20 # 200\n",
    "    visualize: bool = False\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Returns the total size of the tokenizer vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            int: The size of the tokenizer vocabulary.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a4b719-f507-46bc-b389-892b9efd52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb57475-96fe-42c1-9568-06926c2c8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "\n",
    "    Args:\n",
    "        - dataset (Dataset): a dataset from HuggingFace datasets library.\n",
    "        - tokenizer (Tokenizer): The custom tiktoken tokenizer used to encode sequences.\n",
    "        - block_size (int): The maximum sequence length for tokenization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer, block_size):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a tokenized example from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            - index (int): the index of the example to fetch.\n",
    "\n",
    "        Returns:\n",
    "            - Dict: dictionary with keys 'inputs', 'targets' and 'translation', containing tokenized input,\n",
    "            target sequences and original translation.\n",
    "        \"\"\"\n",
    "        translation = self.dataset[index]['translation']\n",
    "        encode = self.tokenizer.encoder.encode\n",
    "        inputs = self.tokenizer.sequence_padding(encode(translation['en']), self.block_size) # source language\n",
    "        targets = self.tokenizer.sequence_padding(encode(translation['fr']), self.block_size) # target language\n",
    "        return {'inputs': inputs, 'targets': targets, 'translation': translation}\n",
    "\n",
    "    def __len__(self) -> int :\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            - int: the length of the dataset.\n",
    "        \"\"\"\n",
    "        return self.dataset.num_rows\n",
    "\n",
    "\n",
    "class DataLoaderFactory():\n",
    "    \"\"\"\n",
    "    A class to instantiate PyTorch DataLoaders for different splits of a HuggingFace Dataset.\n",
    "\n",
    "    Args:\n",
    "        - block_size (int): The maximum sequence length for tokenization.\n",
    "        - batch_size (int): The batch size for DataLoader.\n",
    "        - tokenizer (Tokenizer): a tokenizer that has an encode method.\n",
    "        - device (str): 'cpu' or 'cuda', depending on whether we use CPU or GPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_size, batch_size, tokenizer, device):\n",
    "        self.train_data = TranslationDataset(load_dataset(\"wmt14\", \"fr-en\", split=\"train[:500000]\"), tokenizer, block_size)\n",
    "        self.val_data = TranslationDataset(load_dataset(\"wmt14\", \"fr-en\", split=\"validation\"), tokenizer, block_size)\n",
    "        self.test_data = TranslationDataset(load_dataset(\"wmt14\", \"fr-en\", split=\"test\"), tokenizer, block_size)\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.dataloader_train = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "        self.dataloader_val = DataLoader(self.val_data, batch_size=batch_size, shuffle=True)\n",
    "        self.dataloader_test = DataLoader(self.test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int :\n",
    "        \"\"\"\n",
    "        Print the length of each dataset and returns the length of all datasets.\n",
    "\n",
    "        Returns:\n",
    "            - int: the length of all dataset (train + val + test).\n",
    "        \"\"\"\n",
    "        print(\"\\033[95m\\033[1m\\033[4mNumber of data by datasets splits\\033[0m\")\n",
    "        print(f\"Train\\t\\t: {len(self.train_data)}\")\n",
    "        print(f\"Validation\\t: {len(self.val_data)}\")\n",
    "        print(f\"Test\\t\\t: {len(self.test_data)}\")\n",
    "        total = len(self.train_data) + len(self.val_data) + len(self.test_data)\n",
    "        print(f\"Total\\t\\t: {total}\")\n",
    "        return total\n",
    "\n",
    "    def get_batch(self, split):\n",
    "        \"\"\"\n",
    "        Choose the correct DataLoader and yield batches from it.\n",
    "\n",
    "        Args:\n",
    "            - split (str): 'train', 'val' or 'test'.\n",
    "\n",
    "        Yields:\n",
    "            - Dict: a dictionary with keys 'inputs', 'targets' and 'translation', containing a batch of tokenized input,\n",
    "            target sequences and original translation.\n",
    "        \"\"\"\n",
    "        # choose the correct dataloader\n",
    "        if split == 'train':\n",
    "            dataloader = self.dataloader_train\n",
    "        elif split == 'val':\n",
    "            dataloader = self.dataloader_val\n",
    "        else:\n",
    "            dataloader = self.dataloader_test\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # Separate the 'translation' from the rest of the batch\n",
    "            translation = batch.pop('translation')\n",
    "    \n",
    "            # Move tensors to device\n",
    "            batch_on_device = {k: v.to(self.device) for k, v in batch.items()}\n",
    "    \n",
    "            # Add 'translation' back into the batch\n",
    "            batch_on_device['translation'] = translation\n",
    "    \n",
    "            yield batch_on_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b64c8f-7fd6-456f-b6ce-afa6c25f27fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    }
   ],
   "source": [
    "config = TransformerConfig(tokenizer)\n",
    "dataset = DataLoaderFactory(config.block_size, config.batch_size, tokenizer, config.device)\n",
    "# dataset.DataSize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d0e6a1-4b17-4955-9600-b5cac119e801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataLoaderFactory.get_batch at 0x000001F783C82840>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = dataset.get_batch('train')\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020363b6-98ed-4d32-a522-41d78b804600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_batch['inputs']=tensor([[100264,   2028,    374,  ..., 100266, 100266, 100265],\n",
      "        [100264,    791,  44780,  ..., 100266, 100266, 100265],\n",
      "        [100264,     40,    617,  ..., 100266, 100266, 100265],\n",
      "        ...,\n",
      "        [100264,    644,    420,  ..., 100266, 100266, 100265],\n",
      "        [100264,     40,    649,  ..., 100266, 100266, 100265],\n",
      "        [100264,   2746,    584,  ..., 100266, 100266, 100265]])\n",
      "n_batch['targets']=tensor([[100264,  28615,    321,  ..., 100266, 100266, 100265],\n",
      "        [100264,   8921,  30179,  ..., 100266, 100266, 100265],\n",
      "        [100264,  30854,   9189,  ..., 100266, 100266, 100265],\n",
      "        ...,\n",
      "        [100264,  74223,  20662,  ..., 100266, 100266, 100265],\n",
      "        [100264,  30854,  94297,  ..., 100266, 100266, 100265],\n",
      "        [100264,  22771,  17317,  ..., 100266, 100266, 100265]])\n",
      "n_batch['translation']={'en': ['This is the situation of alcoholic drink consumption in Sweden!', 'The Danish Presidency will keep a watch on this.', 'I have explained the spirit in which the Commission took this decision a few days ago.', 'Finally, I wish to thank the two rapporteurs for their excellent work and I hope that their reports will be adopted with the amendments tabled by the Committee on Fisheries.', 'A very interesting point in this communication is the question how we can develop the prevention culture.', 'If, in fact, they cannot comply with what they have agreed to, such countries are doing Green causes harm.', 'In the case of laying hens, over 25% of their feed is produced on the farm.', 'We have many slaughterhouses and meat processing plants that discharge effluent into domestic sewage systems in our major towns.', 'Finally, in discussing the question of the future application of tests, we should of course bear in mind that we have still not reached the point where tests are generally applicable.', 'In this particular issue there are problems of slender resources - we understand that - whether of the police or of security personnel at the areas where they are involved.', 'I can only take note of this supplementary remark and the information that the honourable Member has provided us with in order to investigate the matter and, if appropriate, also to give those who will have the task of leading the work during the next six months the chance to reply in more detail to this request too.', \"If we do not do so, a great opportunity to create a continent of free and peaceful cooperation may be lost.'\"], 'fr': ['Voilà la situation de la consommation des boissons alcooliques en Suède !', 'La présidence danoise veille à cela.', 'Je vous ai dit dans quel esprit la Commission avait pris sa décision il y a quelques jours.', \"Enfin, je remercie les deux rapporteurs pour leur excellent travail et j'espère que leurs rapports seront approuvés avec les amendements introduits par la commission de la pêche.\", 'Un des points très intéressants dans cette communication est la question de savoir comment nous pouvons développer une culture de prévention.', \"S'il s'avère qu'ils ne peuvent pas respecter ce à quoi ils se sont engagés, ces pays nuisent à la cause écologique.\", \"Pour les poules pondeuses, c'est plus de 25% de la nourriture qui est produite à la ferme.\", \"De nombreux abattoirs et installations de traitement de la viande déversent des effluents dans les systèmes d'égouts domestiques de nos grandes villes.\", \"Enfin, si nous abordons la question de l'application future des tests, nous devons naturellement savoir que nous n'en sommes pas encore au point que les tests soient exploitables d'une manière générale.\", 'Dans cette question particulière, il y a le problème des maigres ressources disponibles, nous le comprenons, que ce soit pour la police ou pour le personnel de sécurité présents dans les zones concernées.', 'Je peux uniquement prendre acte de cette remarque supplémentaire et de l’information que ce membre honorable vient de nous fournir de manière à étudier le sujet et, si nécessaire, à donner également à ceux ayant pour tâche de diriger le travail au cours des six prochains mois l’occasion de répondre plus en détail à cette question également.', 'Si nous ne le faisons pas, il se peut que nous manquions une occasion unique de créer un continent de coopération libre et pacifique.\"']}\n"
     ]
    }
   ],
   "source": [
    "n_batch = next(batch)\n",
    "\n",
    "print(f\"{n_batch['inputs']=}\")\n",
    "print(f\"{n_batch['targets']=}\")\n",
    "print(f\"{n_batch['translation']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d09d529-2f21-4495-bf0f-36cc30640d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m\u001b[4mNumber of data by datasets splits\u001b[0m\n",
      "Train\t\t: 500000\n",
      "Validation\t: 3000\n",
      "Test\t\t: 3003\n",
      "Total\t\t: 506003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "506003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce6e629-fa89-4931-83d0-e62727dac927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"A Layer Normalization module with optional bias.\n",
    "\n",
    "    This implementation of Layer Normalization allows turning off the bias term,\n",
    "    which is not directly supported by PyTorch's layer normalization function.\n",
    "\n",
    "    Attributes:\n",
    "        weight: Learnable weights for the layer normalization. Initialized as an all ones tensor.\n",
    "        bias: Learnable biases for the layer normalization. Initialized as an all zeros tensor \n",
    "              if bias argument in constructor is True, otherwise it's set to None.\n",
    "\n",
    "    Args:\n",
    "        ndim: An integer for the dimension of the input vectors.\n",
    "        bias: A boolean which, if True, adds a learnable bias to the output.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, ndim: int, bias: bool):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            input (tensor): The input tensor to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            tensor: The normalized input tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357d6e13-1c75-485f-9625-29a174b44d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module.\n",
    "    \n",
    "    This module applies multi-head attention mechanism on the input sequence. This implementation doesn't apply mask over the attention scores.\n",
    "    \n",
    "    Attributes:\n",
    "        - n_head (int): Number of attention heads.\n",
    "        - n_embd (int): Embedding dimensionality.\n",
    "        - dropout (float): Dropout rate.\n",
    "        - q_attn (nn.Linear): Linear layer for the query projection.\n",
    "        - k_attn (nn.Linear): Linear layer for the key projection.\n",
    "        - v_attn (nn.Linear): Linear layer for the value projection.\n",
    "        - c_proj (nn.Linear): Linear layer for the output projection.\n",
    "        - attn_dropout (nn.Dropout): Dropout layer for the attention scores.\n",
    "        - resid_dropout (nn.Dropout): Dropout layer for the residual connection.\n",
    "        - flash (bool): Flag indicating if flash attention is available.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Constructor for the MultiHeadAttention class.\n",
    "        \n",
    "        Args:\n",
    "            - config: The configuration object containing model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # Params\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.visualize = config.visualize\n",
    "        \n",
    "        # INPUTS: query, key, value projections for all heads, but in a batch\n",
    "        self.q_attn = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.k_attn = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.v_attn = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # OUTPUT: output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # flash attention make GPU go br but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.tril(\n",
    "                    torch.ones(config.block_size, config.block_size)\n",
    "                ).view(1, 1, config.block_size, config.block_size)\n",
    "            )\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask: bool = None):\n",
    "        \"\"\"\n",
    "        Computes the scaled dot product attention.\n",
    "        \n",
    "        Args:\n",
    "            - q (Tensor): Query tensor of shape (batch_size, num_heads, seq_length, emb_dim).\n",
    "            - k (Tensor): Key tensor of shape (batch_size, num_heads, seq_length, emb_dim).\n",
    "            - v (Tensor): Value tensor of shape (batch_size, num_heads, seq_length, emb_dim).\n",
    "            - mask (bool, optional): Flag indicating whether to apply mask on the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            - y (Tensor): Output tensor after applying attention.\n",
    "        \"\"\"\n",
    "        # manual implementation of attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # Step 1 & 2: (MatMul) and (Scale)\n",
    "        if mask:\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1) # Step 3: Softmax\n",
    "        att_weights = att  # Save attention weights for visualization\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs) # Step 4: MatMul\n",
    "        return y, att_weights\n",
    "\n",
    "    def forward(self, q_x, k_x, v_x, mask = None):\n",
    "        \"\"\"\n",
    "        Forward pass for the MultiHeadAttention module.\n",
    "        \n",
    "        Args:\n",
    "            - q_x (Tensor): Input query tensor of shape (batch_size, seq_length, emb_dim).\n",
    "            - k_x (Tensor): Input key tensor of shape (batch_size, seq_length, emb_dim).\n",
    "            - v_x (Tensor): Input value tensor of shape (batch_size, seq_length, emb_dim).\n",
    "            - mask (bool, optional): Flag indicating whether to apply mask on the attention scores.\n",
    "\n",
    "        Returns:\n",
    "            - y (Tensor): Output tensor after applying multi-head attention.\n",
    "        \"\"\"\n",
    "        B_q, T_q, C_q = q_x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        B_kv, T_kv, C_kv = k_x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.q_attn(q_x), self.k_attn(k_x), self.v_attn(v_x)\n",
    "        k = k.view(B_kv, T_kv, self.n_head, C_kv // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B_q, T_q, self.n_head, C_q // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B_kv, T_kv, self.n_head, C_kv // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash and not self.visualize:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=mask == True)\n",
    "            attn_weights = None\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            y, attn_weights = self.scaled_dot_product_attention(q, k, v)\n",
    "        y = y.transpose(1, 2).contiguous().view(B_q, T_q, C_q) # re-assemble all head outputs side by side # Step 5: Concatenate\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y)) # Step 6 : Linear\n",
    "        return y, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb156ccb-b9fd-4b62-b2a4-d2762cb71a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A position-wise Feed Forward Neural Network (FFNN) class for transformer models.\n",
    "    \n",
    "    The class implementing a position-wise FFNN.\n",
    "    The FFNN consists of two linear transformations with a GELU activation in between, \n",
    "    followed by a dropout for regularization.\n",
    "\n",
    "    Attributes:\n",
    "        - c_fc (nn.Linear): First fully connected layer.\n",
    "        - gelu (nn.GELU): GELU activation function layer.\n",
    "        - c_proj (nn.Linear): Second fully connected layer.\n",
    "        - dropout (nn.Dropout): Dropout layer for regularization.\n",
    "\n",
    "    Args:\n",
    "        - config (Config): A configuration object with attribute `n_embd`, `bias`, and `dropout`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): The input tensor to the forward pass.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output of the FFNN.\n",
    "        \"\"\"\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d648bfa6-2687-4d48-b9b1-be60061f48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements a single encoder block in the Transformer model.\n",
    "    \n",
    "    Each block consists of two sub-layers: a multi-head self-attention mechanism,\n",
    "    and a position-wise fully connected feed-forward network. There is a residual \n",
    "    connection around each of the two sub-layers, followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        - ln_1 (LayerNorm): Layer normalization before the multi-head attention layer.\n",
    "        - attn (MultiHeadAttention): Multi-head attention layer.\n",
    "        - ln_2 (LayerNorm): Layer normalization before the feed-forward network.\n",
    "        - ffw (FeedForward): Position-wise feed-forward network.\n",
    "\n",
    "    Args:\n",
    "        - config (Config): A configuration object with attribute `n_embd` and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.ffw = FeedForward(config)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): The input tensor to the forward pass.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor of the block.\n",
    "        \"\"\"\n",
    "        # MultiHeadAttention\n",
    "        x = self.ln_1(x)\n",
    "        x_attn, decoder_attn = self.attn(x, x, x)\n",
    "        x = x + x_attn\n",
    "        # FeedForward\n",
    "        x = x + self.ffw(self.ln_2(x))\n",
    "        return x, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b0c31f8-35a6-4f31-a0ca-39943bff97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements a single decoder block in the Transformer model.\n",
    "\n",
    "    Each block consists of three sub-layers: a multi-head self-attention mechanism,\n",
    "    a multi-head attention mechanism over the encoder's output, and a position-wise \n",
    "    fully connected feed-forward network. There is a residual connection around \n",
    "    each of the three sub-layers, followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        - ln_1 (LayerNorm): Layer normalization before the first multi-head attention layer.\n",
    "        - attn1 (MultiHeadAttention): First multi-head attention layer, with self-attention.\n",
    "        - ln_2 (LayerNorm): Layer normalization before the second multi-head attention layer.\n",
    "        - attn2 (MultiHeadAttention): Second multi-head attention layer, attends to encoder outputs.\n",
    "        - ln_3 (LayerNorm): Layer normalization before the feed-forward network.\n",
    "        - ffw (FeedForward): Position-wise feed-forward network.\n",
    "\n",
    "    Args:\n",
    "        config (Config): A configuration object with attribute `n_embd` and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn1 = MultiHeadAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn2 = MultiHeadAttention(config)\n",
    "        self.ln_3 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.ffw = FeedForward(config)\n",
    "\n",
    "    def forward(self, x, encoder_output) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): The input tensor to the forward pass.\n",
    "            - encoder_output (torch.Tensor): The output tensor from the last encoder block.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor of the block.\n",
    "        \"\"\"\n",
    "        # Masked MultiHeadAttention\n",
    "        x = self.ln_1(x)\n",
    "        x_attn, cross_attn = self.attn1(x, x, x, True)\n",
    "        x = x + x_attn\n",
    "        # MultiHeadAttention with q, k from encoder and x from decoder\n",
    "        x = self.ln_2(x)\n",
    "        x_attn, encoder_attn = self.attn2(x, encoder_output, encoder_output)\n",
    "        x = x + x_attn\n",
    "        # FeedForward\n",
    "        x = x + self.ffw(self.ln_3(x))\n",
    "        return x, encoder_attn, cross_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d415dfae-bc35-4f10-9fa0-cf182bf30a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A class that implements the encoder part of the Transformer model.\n",
    "\n",
    "    The encoder consists of several EncoderBlocks arranged in sequence.\n",
    "    The input first goes through an embedding layer followed by a positional encoding layer.\n",
    "    The output of this is then passed through each EncoderBlock in sequence.\n",
    "\n",
    "    Attributes:\n",
    "        - encoder (nn.ModuleDict): A dictionary of modules making up the transformer encoder.\n",
    "\n",
    "    Args:\n",
    "        - config (Config): A configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # Learned positional encoding:\n",
    "            # In this case, instead of using a fixed function to determine positional encoding,\n",
    "            # we initialize a tensor of positional encodings which gets updated during training via backpropagation.\n",
    "            # This method may potentially capture more complex position-related patterns than fixed positional encoding,\n",
    "            # but it also introduces additional parameters to the model.\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([EncoderBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        \n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # apply special scaled init to the residual projections, based on GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                # This initialization is used to preventing the variance of the outputs of each layer from exploding or vanishing\n",
    "                # during the forward pass through the network.\n",
    "                # Preventing \"vanishing/exploding gradients\" problem\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"Number of Encoder parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding: bool = True):\n",
    "        \"\"\"\n",
    "        Returns the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "\n",
    "        Args:\n",
    "            -non_embedding (bool, optional): If True, excludes the position embeddings count from the total (Default is True).\n",
    "\n",
    "        Returns:\n",
    "            - int: The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.encoder.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model. Proper weight initialization can help speed up the training process and improve model performance.\n",
    "\n",
    "        Args:\n",
    "            - module (nn.Module): The module of the model to be initialized.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # init Linear layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # bias initialization if necessary\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # init Embedding layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - idx (torch.Tensor): The input tensor to the forward pass.\n",
    "            - targets (torch.Tensor, optional): The target tensor against which the loss will be calculated.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "            - torch.Tensor: The loss tensor, if targets were provided. Otherwise, None.\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # pre-encoder block\n",
    "        tok_emb = self.encoder.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.encoder.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.encoder.drop(tok_emb + pos_emb) # Addition of input embd + positional encoding\n",
    "\n",
    "        # encoders block\n",
    "        encoder_attn_all = []\n",
    "        for block in self.encoder.h:\n",
    "            x, encoder_attn = block(x)\n",
    "            encoder_attn_all.append(encoder_attn)\n",
    "\n",
    "        return  self.encoder.ln_f(x), encoder_attn_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a0264d2-09df-4222-9cc3-945f5b4272af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the decoder part of the Transformer model.\n",
    "\n",
    "    The Decoder consists of several DecoderBlocks arranged in sequence. The input first goes through an embedding \n",
    "    layer followed by a positional encoding layer. The output of this is then passed through each DecoderBlock in \n",
    "    sequence.\n",
    "\n",
    "    Attributes:\n",
    "        - decoder (nn.ModuleDict): A dictionary of modules making up the transformer decoder.\n",
    "        - lm_head (nn.Linear): The final linear layer mapping from the embedding dimension to the vocabulary size.\n",
    "        - config (:obj:`Config`): The configuration object for the transformer model.\n",
    "\n",
    "    .. note:: The weight of the embedding layer and the linear layer are shared.\n",
    "\n",
    "    Args:\n",
    "        - config (:obj:`Config`): The configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.decoder = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # Learned positional encoding:\n",
    "            # In this case, instead of using a fixed function to determine positional encoding,\n",
    "            # we initialize a tensor of positional encodings which gets updated during training via backpropagation.\n",
    "            # This method may potentially capture more complex position-related patterns than fixed positional encoding,\n",
    "            # but it also introduces additional parameters to the model.\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.decoder.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "\n",
    "        Args:\n",
    "            - non_embedding (bool): If True, excludes the position embeddings count from the total. Default is True.\n",
    "\n",
    "        Returns:\n",
    "            - int: The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.decoder.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model.\n",
    "\n",
    "        Args:\n",
    "            - module (torch.nn.Module): The module of the model to be initialized.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # init Linear layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # bias initialization if necessary\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # init Embedding layers with normal distribution (Gaussian initialization)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, enc_output = None):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - idx (torch.Tensor): The input tensor to the forward pass.\n",
    "            - enc_output (torch.Tensor): The output tensor from the encoder.\n",
    "            - targets (torch.Tensor, optional): The target tensor against which the loss will be calculated.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "        tok_emb = self.decoder.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.decoder.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.decoder.drop(tok_emb + pos_emb) # Addition of input embd + positional encoding\n",
    "\n",
    "        cross_attn_all = []\n",
    "        decoder_attn_all = []\n",
    "        for block in self.decoder.h:\n",
    "            x, encoder_attn, cross_attn = block(x, enc_output)\n",
    "            decoder_attn_all.append(encoder_attn)\n",
    "            cross_attn_all.append(cross_attn)\n",
    "            \n",
    "        x = self.decoder.ln_f(x)\n",
    "        return self.lm_head(x), decoder_attn_all, cross_attn_all\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7d62cda5-5bc1-4bd9-a3e8-134bab8da831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the Transformer model, which includes both the encoder and decoder.\n",
    "\n",
    "    The Transformer is a sequence transduction model that uses attention mechanisms.\n",
    "    It is primarily used in tasks that require understanding of context or relationships among words in a text.\n",
    "\n",
    "    Attributes:\n",
    "        - encoder (Encoder): The transformer encoder.\n",
    "        - decoder (Decoder): The transformer decoder.\n",
    "        - config (:obj:`Config`): The configuration object for the transformer model.\n",
    "\n",
    "    Args:\n",
    "        - config (:obj:`Config`): The configuration object with attributes such as `vocab_size`, `block_size`, `n_embd`, `dropout`, `n_layer`, and `bias`.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            - src (torch.Tensor): The input tensor to the encoder.\n",
    "            - tgt (torch.Tensor): The input tensor to the decoder.\n",
    "\n",
    "        Returns:\n",
    "            - torch.Tensor: The output tensor (logits) of the model.\n",
    "            - torch.Tensor: The loss tensor calculated on the basis of the decoder's output and target tensor.\n",
    "        \"\"\"\n",
    "        enc_output, _ = self.encoder(src)\n",
    "        tgt_shifted = tgt[:, :-1] # Shifted target\n",
    "        output, _, _ = self.decoder(tgt_shifted, enc_output)\n",
    "\n",
    "        # Calculate the loss, using both the output and the target\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=self.config.tokenizer.PAD_IDX) # Ignore padding tokens\n",
    "        # The targets for the loss function are the input sequences shifted\n",
    "        tgt_tgt = tgt[:, 1:].contiguous()\n",
    "        loss = loss_fct(output.view(-1, output.size(-1)), tgt_tgt.view(-1))\n",
    "        return output, loss\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def translate_beam_search(self, src, beam_size=5):\n",
    "        enc_output, encoder_attn = self.encoder(src)\n",
    "        # initialize beam with start token\n",
    "        start_token = torch.full((src.size(0), 1), self.config.tokenizer.BOS_IDX).long().to(src.device)   # initialize target tensor with start token bos\n",
    "        start_score = torch.zeros((src.size(0), 1)).float().to(src.device)  # initialize target tensor with zeros\n",
    "        beams = [(start_token, start_score, [], [])]  # initialize beams\n",
    "        for iter in range(self.config.block_size):\n",
    "            print(f\"\\r{iter+1}/{self.config.block_size}\", end=\"\")\n",
    "            new_beams = []\n",
    "            for beam, score, decoder_attentions, cross_attentions in beams:\n",
    "                output, dec_attention, cross_attention = self.decoder(beam, enc_output)\n",
    "                output = F.log_softmax(output[:, -1, :], dim=-1)  # use log_softmax for numerical stability\n",
    "\n",
    "                top_scores, top_indices = output.topk(beam_size, dim=-1)  # select topk predictions\n",
    "                \n",
    "                for i in range(beam_size):\n",
    "                    next_token = top_indices[:, i].unsqueeze(1)  # get next token\n",
    "                    next_score = top_scores[:, i].unsqueeze(1)  # get next score\n",
    "                    new_beam = torch.cat((beam, next_token), dim=-1)  # generate new beam\n",
    "                    new_score = score + next_score  # calculate new score\n",
    "\n",
    "                    # new_decoder_attentions = decoder_attentions + [dec_attention]\n",
    "                    # new_cross_attentions = cross_attentions + [cross_attention]\n",
    "                    \n",
    "                    new_beams.append((new_beam, new_score, dec_attention, cross_attention))  # append new beam to new beams\n",
    "\n",
    "            # sort all candidates by score\n",
    "            beams = sorted(new_beams, key=lambda tup: tup[1].sum(), reverse=True)[:beam_size]  # keep top performing beams\n",
    "        return beams[0][0], dict(encoder_attn=encoder_attn, decoder_attn=beams[0][2], cross_attn=beams[0][3])  # return the best sequence\n",
    "\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError(f\"{path} does not exist.\")\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e3d7e-cb08-4dce-aae1-41fc9b5aceca",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "882759cf-c512-4701-acce-0da8efdc6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(dataset, config):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(config.eval_iters)\n",
    "        batch = dataset.get_batch(split)\n",
    "        # loop\n",
    "        inner_loop = tqdm(range(config.eval_iters), desc=f\"Evaluation - {split}\", leave=False)\n",
    "        for k in inner_loop:\n",
    "            n_batch = next(batch)\n",
    "            X = n_batch['inputs']\n",
    "            Y = n_batch['targets']\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fdf6a48-2441-4e6b-9070-fcf67b8751ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encoder parameters: 57.64M\n",
      "number of parameters: 59.73M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n",
      "Found cached dataset wmt14 (C:/Users/thiba/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m\u001b[4mNumber of data by datasets splits\u001b[0m\n",
      "Train\t\t: 500000\n",
      "Validation\t: 3000\n",
      "Test\t\t: 3003\n",
      "Total\t\t: 506003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "506003"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = TransformerConfig(tokenizer)\n",
    "model = Transformer(config)\n",
    "dataset = DataLoaderFactory(config.block_size, config.batch_size, tokenizer, config.device)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd10ccbf-da9f-4704-a8dc-37e1b64218b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "model.train()\n",
    "model = model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "664a4db6-01e5-49ce-bff7-449598c5722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29636df1-1fa6-4b0e-b3fb-ed5f6fad567a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02222bc756546579a7ca1f0cc80430e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train loss nan, Val loss nan:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - train:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation - val:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = dataset.get_batch('train')\n",
    "\n",
    "losses_list = {\n",
    "    'train': [],\n",
    "    'val': [],\n",
    "}\n",
    "\n",
    "outer_loop = tqdm(range(config.max_iters), desc=\"Train loss nan, Val loss nan\", leave=True)\n",
    "for iter in outer_loop:\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % config.eval_interval == 0:\n",
    "        losses = estimate_loss(dataset, config)\n",
    "        losses_list['train'].append(losses['train'])\n",
    "        losses_list['val'].append(losses['val'])\n",
    "        batch = dataset.get_batch('train')\n",
    "        last_loss_train = losses_list['train'][-1]\n",
    "        last_loss_val = losses_list['val'][-1]\n",
    "        outer_loop.set_description(f\"Train loss {last_loss_train:.4f}, Val loss {last_loss_val:.4f}\")\n",
    "        \n",
    "    # sample a batch of data\n",
    "    n_batch = next(batch)\n",
    "    xb = n_batch['inputs']\n",
    "    yb = n_batch['targets']\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26ae877b-15fe-491c-977d-8431556d2b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"./out/transformer_state_dict.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8f902cf9-6dc6-4c18-93f8-e1ee081128ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encoder parameters: 57.64M\n",
      "number of parameters: 59.73M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = TransformerConfig(tokenizer, visualize=True)\n",
    "# First, you need to initialize the model\n",
    "model2 = Transformer(config)\n",
    "# Then, load the state dict\n",
    "model2.load_model('./out/transformer_state_dict.pth')\n",
    "# If you are ready to perform inference (and not training), put the model in evaluation mode\n",
    "model2.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e538ea60-8b86-4a98-9c86-15cd7c4a9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentences, tokenizer, model):\n",
    "    # Tokenize sentences\n",
    "    tknzr = tokenizer.encoder\n",
    "    sequences = []\n",
    "    for sentence in sentences:\n",
    "        sequence = tokenizer.sequence_padding(tknzr.encode(sentence)).unsqueeze(dim=0)\n",
    "        sequences.append(sequence)\n",
    "    sequences = torch.cat(sequences, dim=0)\n",
    "\n",
    "    # Translate sentences\n",
    "    model.eval()\n",
    "    outputs, attn = model.translate_beam_search(sequences)\n",
    "\n",
    "    # Decode tokenized sentences\n",
    "    decode_output = []\n",
    "    for output in outputs:\n",
    "        output = tokenizer.sequence_cleaner(output)\n",
    "        decode_output += [tknzr.decode(output)]\n",
    "    \n",
    "    return decode_output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35d8b57a-1410-4f39-a560-fd1668fe7af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512"
     ]
    }
   ],
   "source": [
    "outputs, attentions = translate(['I am a teacher.'], tokenizer, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9a7d8520-80e1-431a-bc3c-025dc8b7b965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJeJe']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "60eb611a-50a7-4a3c-98e0-b3d33dd871d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attentions['encoder_attn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "02fcf313-e3db-4ec3-9638-815f3ad161e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attentions['cross_attn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3eb4cf5e-578f-44b1-80cd-1b7348e871ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4, 512, 512])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(attentions['cross_attn'], dim=0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d00f3e9-4cb4-4a96-b5e9-fbe185f6679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_encoder_attn = torch.stack(attentions['encoder_attn'], dim=0)\n",
    "tensor_cross_attn = torch.stack(attentions['cross_attn'], dim=0)\n",
    "tensor_decoder_attn = torch.stack(attentions['decoder_attn'], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "100cbcf5-6887-4fe4-ab9b-8aec70ff927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b5d2a5fc-0740-46fa-b11c-45a02d3f5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a = tokenizer.tokenize_from_str('I am a teacher.')\n",
    "tokens_b = tokenizer.tokenize_from_str('Je suis un professeur.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "37a96e38-3403-4ad4-bbee-bd1ece6ea737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', ' am', ' a', ' teacher', '.', '', '']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = tokens_a + [''] * 2\n",
    "tokens_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f4609874-5273-4773-a570-3d559721e46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Je', ' suis', ' un', ' prof', 'esse', 'ur', '.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fcfd2f12-8cb1-400e-bc1a-76ff03fab43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_token = max(len(tokens_a), len(tokens_b)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4c9bf7c0-b143-4dad-8077-ec1c42ba0622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4, 7, 7])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_encoder_attn = tensor_encoder_attn[:, 0:1, :, 1:max_len_token, 1:max_len_token] # layers, batch, heads, seq_len, seq_len\n",
    "tensor_encoder_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8d13656f-d642-470d-968c-f2626c031e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4, 7, 7])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cross_attn = tensor_cross_attn[:, 0:1, :, 1:max_len_token, 1:max_len_token] # layers, batch, heads, seq_len, seq_len\n",
    "tensor_cross_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "555b4070-4d84-4da4-b1a2-1143342806c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4, 7, 7])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_decoder_attn = tensor_decoder_attn[:, 0:1, :, 1:max_len_token, 1:max_len_token] # layers, batch, heads, seq_len, seq_len\n",
    "tensor_decoder_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e62b6d9c-02e6-4e50-a610-1ea24492c19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-bfa775725d4e4b1892815aefff7a86a0\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                Attention: <select id=\"filter\"><option value=\"0\">Encoder</option>\n",
       "<option value=\"1\">Decoder</option>\n",
       "<option value=\"2\">Cross</option></select>\n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/**\n",
       " * @fileoverview Transformer Visualization D3 javascript code.\n",
       " *\n",
       " * Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n",
       " *\n",
       " * Change log:\n",
       " *\n",
       " * 02/01/19  Jesse Vig   Initial implementation\n",
       " * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n",
       " * 01/19/21  Jesse Vig   Support light/dark modes\n",
       " * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n",
       " * 05/03/21  Jesse Vig   Adjust visualization height dynamically\n",
       " * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n",
       " **/\n",
       "\n",
       "require.config({\n",
       "  paths: {\n",
       "      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
       "    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
       "  }\n",
       "});\n",
       "\n",
       "requirejs(['jquery', 'd3'], function($, d3) {\n",
       "\n",
       "        const params = {\"attention\": [{\"name\": \"Encoder\", \"attn\": [[[[0.0001244435552507639, 0.00011266420187894255, 0.0002516293607186526, 0.00010624696005834267, 0.00013604268315248191, 0.0011972725624218583, 0.0013834101846441627], [0.0003067910438403487, 0.0003797105164267123, 0.0004895984893664718, 0.00017375298193655908, 0.0003455829282756895, 0.0015692779561504722, 0.0019053813302889466], [1.8029280909104273e-05, 1.5208333934424445e-05, 5.61276356165763e-05, 7.531493338319706e-06, 2.1538462533499114e-05, 0.0011991065694019198, 0.0014792606234550476], [0.0003953646228183061, 0.00038482435047626495, 0.0006200792849995196, 0.0003139626351185143, 0.0004176825168542564, 0.001966077135875821, 0.0020922126714140177], [0.00013869850954506546, 0.00014263740740716457, 0.00029617061954922974, 6.48902787361294e-05, 0.00013018528989050537, 0.0014126208843663335, 0.0017707661027088761], [8.030361264843577e-09, 7.032122439909472e-09, 4.2573751102281676e-07, 9.114856092118373e-10, 1.5806364217496593e-08, 0.000583222892601043, 0.0015968417283147573], [4.478146742314948e-09, 3.804466519596872e-09, 2.3985046482266625e-07, 5.783113987689603e-10, 9.901888198271536e-09, 0.0005666323122568429, 0.0015012265648692846]], [[0.00027167663211002946, 0.00020005452097393572, 0.00035258856951259077, 0.0001818927557906136, 0.0002781347429845482, 0.0016339885769411922, 0.0021504932083189487], [0.00021942505554761738, 0.00020838889759033918, 0.00037267437437549233, 0.0002254763967357576, 0.00021141963952686638, 0.001391371712088585, 0.0016143773682415485], [4.66657911601942e-05, 2.8734992156387307e-05, 6.315022619673982e-05, 2.5611572709749453e-05, 3.186591493431479e-05, 0.001683767419308424, 0.0012509392108768225], [0.0005671802209690213, 0.0004542050010059029, 0.0006745471619069576, 0.0004948038258589804, 0.00043935165740549564, 0.002174914116039872, 0.0014460764359682798], [0.00028907047817483544, 0.00024943429161794484, 0.0003815964446403086, 0.00022193806944414973, 0.00020766039961017668, 0.0019101742655038834, 0.00178981339558959], [9.300777747967004e-08, 3.669113013415881e-08, 2.9633733333866985e-07, 2.6537792408021232e-08, 4.920474694358745e-08, 0.0005396560300141573, 0.0010779434815049171], [7.621545705660537e-08, 1.4437678608203441e-08, 1.8405880553018505e-07, 1.7301337251751647e-08, 2.9575497606515455e-08, 0.0005127748590894043, 0.0008898610831238329]], [[0.0007952657761052251, 0.000874561257660389, 0.001177239348180592, 0.0008014417253434658, 0.0009596769232302904, 0.001269379397854209, 0.0017803125083446503], [0.00041581137338653207, 0.0005089842597953975, 0.0005281173507682979, 0.00040344413719139993, 0.0006173507426865399, 0.0019264526199549437, 0.0014920583926141262], [9.19521989999339e-05, 9.744636190589517e-05, 0.00014071501209400594, 8.363241795450449e-05, 0.00010897513129748404, 0.0017378325574100018, 0.0015215340536087751], [0.0005913384375162423, 0.0004664700827561319, 0.0008208560175262392, 0.0007805845816619694, 0.000483395648188889, 0.0016029059188440442, 0.0018685426330193877], [0.0005404638941399753, 0.0004083497915416956, 0.000848443596623838, 0.0005872796173207462, 0.0007631818298250437, 0.0019634985364973545, 0.001559940748848021], [3.8652063949484727e-07, 2.2344001138208114e-07, 1.478648641750624e-06, 2.3362976264706958e-07, 2.789491588828241e-07, 0.0007234656368382275, 0.0009868611814454198], [3.016745324657677e-07, 2.2261338017415255e-07, 1.6161548046511598e-06, 2.1723104737247922e-07, 3.183396302119945e-07, 0.0007928681443445385, 0.0007818498997949064]], [[0.0009111871477216482, 0.0007607329171150923, 0.0008668781374581158, 0.0005971425562165678, 0.0007442832575179636, 0.002466397825628519, 0.002316609723493457], [0.0006017641280777752, 0.0008089006878435612, 0.0006385153392329812, 0.0009037574636749923, 0.00065309286583215, 0.002065225737169385, 0.001821330632083118], [0.00010643474524840713, 9.622338984627277e-05, 0.0001098678694688715, 7.453877333318815e-05, 0.00013202666013967246, 0.0020079489331692457, 0.0018175721634179354], [0.0005654582055285573, 0.0005507713067345321, 0.0007766984635964036, 0.0005084159784018993, 0.0007362078758887947, 0.0017437950009480119, 0.001939503476023674], [0.000545306655112654, 0.00041268038330599666, 0.0005978436674922705, 0.0003216119948774576, 0.0004915076424367726, 0.0018738089129328728, 0.001962667563930154], [2.1019232576691138e-07, 1.6695875615369005e-07, 3.068323906063597e-07, 9.437309245186043e-08, 2.6731277102953754e-07, 0.002782493829727173, 0.0017241952009499073], [1.1789978060505746e-07, 1.2063320298238978e-07, 1.9224439995468856e-07, 7.515063771279529e-08, 2.1620864742999402e-07, 0.00227953027933836, 0.0017691429238766432]]], [[[0.0006736136274412274, 0.0007827648078091443, 0.0009466311894357204, 0.0007495568133890629, 0.000747990095987916, 0.0022481216583400965, 0.0017716604052111506], [0.0006707112188450992, 0.0007978093926794827, 0.0009514944977127016, 0.0007586644496768713, 0.0007552676252089441, 0.002272462472319603, 0.0017774841981008649], [0.0006558556342497468, 0.0007697020773775876, 0.0009414860978722572, 0.0007273551309481263, 0.0007353550754487514, 0.0022797374986112118, 0.0017596272518858314], [0.0006744883139617741, 0.0008064088760875165, 0.0009532605763524771, 0.0007546594715677202, 0.0007490649004466832, 0.0022631227038800716, 0.0017744177021086216], [0.0006843882147222757, 0.0007876748568378389, 0.0009710174635984004, 0.0007631796761415899, 0.0007571456371806562, 0.0022874013520777225, 0.0017770547419786453], [0.0006209186394698918, 0.0007394569693133235, 0.0009067946812137961, 0.0006954854470677674, 0.0006965604261495173, 0.002279226202517748, 0.0017474234336987138], [0.0006109087262302637, 0.0007302847225219011, 0.0008970409398898482, 0.0006975356955081224, 0.0006873281090520322, 0.0022942631039768457, 0.0017433528555557132]], [[0.00039741594810038805, 0.00036316816112957895, 0.0006120743928477168, 0.0004714024253189564, 0.00028561288490891457, 0.001856151269748807, 0.0019523273222148418], [0.0003702104149851948, 0.00034935781150124967, 0.0006142938509583473, 0.0004640950355678797, 0.000279448926448822, 0.001872523222118616, 0.0019456201698631048], [0.0003651919250842184, 0.0003406937757972628, 0.0005949000478722155, 0.0004470513667911291, 0.0002694002178031951, 0.0018642584327608347, 0.00195637927390635], [0.0003868200583383441, 0.0003670902515295893, 0.0006138746393844485, 0.0004709497152362019, 0.00028897947049699724, 0.001857637893408537, 0.0019554717000573874], [0.00037732577766291797, 0.00036234536673873663, 0.0005974201485514641, 0.0004526441334746778, 0.0002779133792500943, 0.0018552833935245872, 0.0019559680949896574], [0.00034810806391760707, 0.00032161467242985964, 0.0005625317571684718, 0.0004147374420426786, 0.0002470424515195191, 0.0018635448068380356, 0.0019463645294308662], [0.00034463501651771367, 0.0003224253887310624, 0.0005597746348939836, 0.0004175882204435766, 0.0002472986525390297, 0.0018574731657281518, 0.001960442401468754]], [[0.0006720508099533617, 0.0007765429327264428, 0.000929080240894109, 0.0006095852586440742, 0.0007760734879411757, 0.0017631046939641237, 0.0019561403896659613], [0.0006775740766897798, 0.0007850467809475958, 0.0009350682375952601, 0.0006176198949106038, 0.0008087765891104937, 0.001745070912875235, 0.0019781014416366816], [0.0006451876251958311, 0.0007548555149696767, 0.0009184554219245911, 0.0005862449761480093, 0.0007760583539493382, 0.0017534219659864902, 0.0019657667726278305], [0.0006744327838532627, 0.0007833449053578079, 0.0009477480198256671, 0.0006119955214671791, 0.0008033321355469525, 0.001752006821334362, 0.001972441328689456], [0.000663267623167485, 0.0007889120024628937, 0.0009342856938019395, 0.0006102737388573587, 0.0007925989339128137, 0.0017525126459077, 0.0019620885141193867], [0.0006444320897571743, 0.0007449389086104929, 0.000903993146494031, 0.0005703219212591648, 0.0007627464365214109, 0.0017556716920807958, 0.0019702457357198], [0.0006372114294208586, 0.0007402864866890013, 0.0008976082899607718, 0.0005650772363878787, 0.0007550687296316028, 0.0017432798631489277, 0.001971327932551503]], [[0.00031113281147554517, 0.0003014165267813951, 0.0005125253228470683, 0.00037184703978709877, 0.00048024949501268566, 0.0017863217508420348, 0.001973920501768589], [0.0003072340623475611, 0.0002994237293023616, 0.0005116547690704465, 0.0003664434188976884, 0.00048007292207330465, 0.0017902979161590338, 0.001973962876945734], [0.00029468172579072416, 0.00029182492289692163, 0.0005087347817607224, 0.00035208059125579894, 0.0004642635176423937, 0.0017909975722432137, 0.0019813987892121077], [0.0003161942586302757, 0.0003067788784392178, 0.000521274923812598, 0.0003709437733050436, 0.00048547962796874344, 0.001802865881472826, 0.001987478695809841], [0.00030716718174517155, 0.00030071279616095126, 0.0005146324401721358, 0.0003698128566611558, 0.0004759263538289815, 0.001801209757104516, 0.0020009540021419525], [0.00027253921143710613, 0.00026216564583592117, 0.00046917388681322336, 0.0003255120827816427, 0.00043420138536021113, 0.0017805311363190413, 0.001993263605982065], [0.00026691571110859513, 0.00025551332510076463, 0.0004653250507544726, 0.00031945930095389485, 0.0004261027497705072, 0.001778463483788073, 0.001992118777707219]]]], \"left_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}, {\"name\": \"Decoder\", \"attn\": [[[[0.00196792627684772, 0.0019924244843423367, 0.001948700868524611, 0.0019674133509397507, 0.001986196031793952, 0.0019617523066699505, 0.0019482981879264116], [0.001966990064829588, 0.001984128961339593, 0.001958775334060192, 0.0019473930587992072, 0.0019849056843668222, 0.0019569052383303642, 0.0019542898517102003], [0.0019887289963662624, 0.002008314011618495, 0.001972887199372053, 0.0019823319744318724, 0.001994878053665161, 0.0019597038626670837, 0.001955584157258272], [0.001962999114766717, 0.001986286835744977, 0.0019510093843564391, 0.001956256339326501, 0.001964643131941557, 0.001960468478500843, 0.001949843717738986], [0.0019726138561964035, 0.001988248201087117, 0.0019554898608475924, 0.0019500748021528125, 0.0019832623656839132, 0.001954542938619852, 0.00194749073125422], [0.001959957182407379, 0.001996913691982627, 0.001950957695953548, 0.0019815401174128056, 0.001991579309105873, 0.0019605965353548527, 0.0019538707565516233], [0.0019495681626722217, 0.001959865214303136, 0.0019312291406095028, 0.0019465419463813305, 0.0019550775177776814, 0.001962303416803479, 0.0019518262706696987]], [[0.0019666494335979223, 0.0019624142441898584, 0.0019821366295218468, 0.001977745443582535, 0.001985287992283702, 0.001969909993931651, 0.0019510264974087477], [0.0019896994344890118, 0.0019808660726994276, 0.0019979430362582207, 0.0020095149520784616, 0.001986938063055277, 0.0019593099132180214, 0.0019499990157783031], [0.0019712024368345737, 0.001960474532097578, 0.0019970338325947523, 0.001980607397854328, 0.0019794467370957136, 0.0019540139473974705, 0.0019549736753106117], [0.0019732865039259195, 0.0019506958778947592, 0.00198213174007833, 0.0019695444498211145, 0.0019744280725717545, 0.0019560763612389565, 0.0019491029670462012], [0.001985149225220084, 0.001985670533031225, 0.001988215371966362, 0.002003168687224388, 0.001970443641766906, 0.001954195322468877, 0.0019476808374747634], [0.0020008408464491367, 0.001971989404410124, 0.0019777831621468067, 0.0019894782453775406, 0.0019867755472660065, 0.001956373453140259, 0.001948135788552463], [0.0019539613276720047, 0.0019488681573420763, 0.0019527219701558352, 0.0019499191548675299, 0.0019432505359873176, 0.0019520032219588757, 0.0019513150909915566]], [[0.0019518331391736865, 0.0019612747710198164, 0.001951485755853355, 0.0019824847113341093, 0.001951782964169979, 0.001955929910764098, 0.0019495667656883597], [0.001940602553077042, 0.001978034619241953, 0.0019699065014719963, 0.0019708089530467987, 0.0019540726207196712, 0.0019522877410054207, 0.001954081002622843], [0.0019494262523949146, 0.001985308015719056, 0.0019489009864628315, 0.001982596004381776, 0.001960010966286063, 0.0019511992577463388, 0.001959258923307061], [0.001946115749888122, 0.001977519830688834, 0.0019524636445567012, 0.00198703957721591, 0.0019587548449635506, 0.001956515247002244, 0.0019519133493304253], [0.0019592095632106066, 0.001972056459635496, 0.001964532071724534, 0.002002664376050234, 0.001978265354409814, 0.0019637923687696457, 0.0019529638811945915], [0.0019618533551692963, 0.001956533407792449, 0.0019528113771229982, 0.001988637261092663, 0.0019488141406327486, 0.0019522934453561902, 0.0019554379396140575], [0.0019321413710713387, 0.001950260833837092, 0.001937765977345407, 0.0019813338294625282, 0.0019244705326855183, 0.0019524224335327744, 0.0019536945037543774]], [[0.0019946424290537834, 0.001978556625545025, 0.0019819268491119146, 0.0020125596784055233, 0.0019923143554478884, 0.0019566707778722048, 0.001952262013219297], [0.001977883977815509, 0.001959608867764473, 0.001968641532585025, 0.0019824227783828974, 0.001966861542314291, 0.0019600316882133484, 0.0019565471448004246], [0.0019723870791494846, 0.0019556484185159206, 0.001960964873433113, 0.0019755391404032707, 0.001972456928342581, 0.001957378815859556, 0.0019547129049897194], [0.001961882458999753, 0.0019668051972985268, 0.0019589923322200775, 0.0019610989838838577, 0.00198723329231143, 0.001958656357601285, 0.0019553399179130793], [0.0019818944856524467, 0.0019921762868762016, 0.0019859876483678818, 0.0020148924086242914, 0.0020066804718226194, 0.0019581401720643044, 0.0019555019680410624], [0.0019624934066087008, 0.0019748746417462826, 0.0019767461344599724, 0.0019766087643802166, 0.0019934377633035183, 0.001958707347512245, 0.0019602274987846613], [0.0019675912335515022, 0.0019456855952739716, 0.001945626107044518, 0.001967812655493617, 0.001956124557182193, 0.001953291706740856, 0.001951445359736681]]], [[[0.0019490944687277079, 0.002040244173258543, 0.001972732599824667, 0.0020170765928924084, 0.0019744010642170906, 0.0019489806145429611, 0.0019580277148634195], [0.0019505181116983294, 0.0020282771438360214, 0.001955499639734626, 0.0020059761591255665, 0.001974172657355666, 0.001960697118192911, 0.0019569220021367073], [0.0019391919486224651, 0.0020327751990407705, 0.001964116934686899, 0.001999113243073225, 0.0019542493391782045, 0.0019512843573465943, 0.001956053776666522], [0.0019507658435031772, 0.002041537081822753, 0.0019714776426553726, 0.0020011933520436287, 0.001962908310815692, 0.001952211488969624, 0.0019506693352013826], [0.0019526862306520343, 0.002036262769252062, 0.001965384231880307, 0.0020052569452673197, 0.0019656678196042776, 0.0019515340682119131, 0.001957800006493926], [0.0019503295188769698, 0.002046822104603052, 0.001967406366020441, 0.0020022052340209484, 0.00196312484331429, 0.0019479839829728007, 0.001950486097484827], [0.0019362756283953786, 0.002023534383624792, 0.0019627027213573456, 0.001992327393963933, 0.0019539780914783478, 0.0019497702596709132, 0.001956457505002618]], [[0.002060863422229886, 0.0019968345295637846, 0.002022764878347516, 0.0020458248909562826, 0.002036196645349264, 0.0019675507210195065, 0.0019582626409828663], [0.0020357484463602304, 0.0019831513054668903, 0.002005866263061762, 0.002020974177867174, 0.0020150926429778337, 0.0019768306519836187, 0.001954258419573307], [0.002031270880252123, 0.0019803617615252733, 0.0019974769093096256, 0.002025180496275425, 0.002012725453823805, 0.0019673951901495457, 0.0019562900997698307], [0.0020287095103412867, 0.001985005335882306, 0.002014914993196726, 0.002037788974121213, 0.002025246387347579, 0.0019728911574929953, 0.0019570074509829283], [0.0020581709686666727, 0.001997264102101326, 0.0020112188067287207, 0.0020568552426993847, 0.002033790573477745, 0.0019714203663170338, 0.0019549764692783356], [0.00203397823497653, 0.00198573200032115, 0.002010033233091235, 0.0020143697038292885, 0.00202631251886487, 0.0019687931053340435, 0.001955615356564522], [0.002010362921282649, 0.001973184524103999, 0.001980905421078205, 0.002011431148275733, 0.002005484886467457, 0.0019694743677973747, 0.001964760944247246]], [[0.001999486470595002, 0.0019916119053959846, 0.0019851403776556253, 0.0019777528941631317, 0.001976937986910343, 0.0019662242848426104, 0.0019642349798232317], [0.002001754241064191, 0.0019768252968788147, 0.0019654803909361362, 0.001967380987480283, 0.0019606712739914656, 0.001969889970496297, 0.0019595192279666662], [0.002003338420763612, 0.001983124064281583, 0.001970144221559167, 0.0019654082134366035, 0.0019660929683595896, 0.0019692459609359503, 0.001958344364538789], [0.002007861156016588, 0.001987168798223138, 0.0019840600434690714, 0.0019814164843410254, 0.001969703007489443, 0.0019690790213644505, 0.0019580498337745667], [0.0020121836569160223, 0.0019887920934706926, 0.001962231006473303, 0.0019824765622615814, 0.0019640529062598944, 0.0019631413742899895, 0.0019590745214372873], [0.0020100700203329325, 0.001996978186070919, 0.0019797938875854015, 0.001972790341824293, 0.0019728641491383314, 0.001970375655218959, 0.0019549482967704535], [0.001991769066080451, 0.001970176585018635, 0.0019551245495676994, 0.0019515450112521648, 0.001958750421181321, 0.0019640224054455757, 0.0019521234789863229]], [[0.0020028245635330677, 0.001996856415644288, 0.0019674976356327534, 0.001953901955857873, 0.0019940652418881655, 0.0019641646649688482, 0.001954959938302636], [0.001995844766497612, 0.0019799936562776566, 0.001966798910871148, 0.0019558218773454428, 0.0019870828837156296, 0.001965765841305256, 0.0019517035689204931], [0.0019948454573750496, 0.001976232510060072, 0.0019684943836182356, 0.00198021880351007, 0.0019936293829232454, 0.0019654498901218176, 0.0019556765910238028], [0.0019989353604614735, 0.001989632146432996, 0.0019704566802829504, 0.001966312061995268, 0.001993130659684539, 0.0019713016226887703, 0.0019522259244695306], [0.0020072446204721928, 0.002011402975767851, 0.0019655218347907066, 0.0019602912943810225, 0.0019958773627877235, 0.001974351704120636, 0.001953576225787401], [0.001987726893275976, 0.0019916302990168333, 0.001977794338017702, 0.00196055811829865, 0.001988864503800869, 0.0019709330517798662, 0.001954174367710948], [0.0019805817864835262, 0.0019713593646883965, 0.0019563334062695503, 0.0019620254170149565, 0.0019828020595014095, 0.0019660794641822577, 0.0019502335926517844]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"]}, {\"name\": \"Cross\", \"attn\": [[[[0.0018907840130850673, 0.0006545690703205764, 0.001013795379549265, 0.000669850327540189, 0.0009301647078245878, 0.0010438591707497835, 0.0009906711056828499], [0.0020754754077643156, 0.0016920268535614014, 0.0019179475493729115, 0.0018678444903343916, 0.0024772845208644867, 0.001952912425622344, 0.0017753101419657469], [0.0009741291869431734, 0.0012223860248923302, 0.0010262762662023306, 0.0007965767872519791, 0.0007938087219372392, 0.0008265154319815338, 0.0008562952280044556], [0.001958247972652316, 0.001424274523742497, 0.002145964652299881, 0.0016172884497791529, 0.0016958625055849552, 0.0015427826438099146, 0.001688189571723342], [0.0023526111617684364, 0.0009281941456720233, 0.001895618624985218, 0.0011766733368858695, 0.0016946783289313316, 0.0012597328750416636, 0.0015489495126530528], [0.0021699187345802784, 0.0014338481705635786, 0.001321551506407559, 0.0009670233121141791, 0.0013961985241621733, 0.0009149193065240979, 0.0010450856061652303], [0.0013964945683255792, 0.001091045793145895, 0.0015357251977548003, 0.0013037041062489152, 0.0015433450462296605, 0.0010065571404993534, 0.0013077325420454144]], [[0.0020809979178011417, 0.0005780319916084409, 0.0003582587232813239, 0.00022170293959788978, 0.0006386962486431003, 0.00034999646595679224, 0.0002838528307620436], [0.001976816914975643, 0.0024754067417234182, 0.000747966580092907, 0.0005993059603497386, 0.0011211080709472299, 0.0006994821596890688, 0.0006880377768538892], [0.0047377836890518665, 0.0014896432403475046, 0.0016231206245720387, 0.0009479708969593048, 0.0011624168837442994, 0.0011655943235382438, 0.0009784664725884795], [0.005005617626011372, 0.0017514583887532353, 0.0009242980740964413, 0.0005625000922009349, 0.0014633717946708202, 0.0008473880589008331, 0.0007787581998854876], [0.0023549937177449465, 0.0012948508374392986, 0.0006965223583392799, 0.00076757010538131, 0.00163964310195297, 0.0007374039269052446, 0.0006726642022840679], [0.0033066486939787865, 0.0023656035773456097, 0.002209259429946542, 0.0016873892163857818, 0.0017840780783444643, 0.0016797153512015939, 0.0015469115460291505], [0.003027813509106636, 0.0020618003327399492, 0.002366369590163231, 0.002517594024538994, 0.0019390099914744496, 0.0019140916410833597, 0.0021509991493076086]], [[0.0022792944218963385, 0.0019919401966035366, 0.0015943101607263088, 0.0022225435823202133, 0.0023558377288281918, 0.0018452840158715844, 0.0012925018090754747], [0.004644542466849089, 0.0010289803612977266, 0.0017524220747873187, 0.0022530509158968925, 0.0016745920293033123, 0.0016307373298332095, 0.0011530136689543724], [0.0057770926505327225, 0.0016698201652616262, 0.002215599175542593, 0.0022884567733854055, 0.0025761937722563744, 0.0021749669685959816, 0.0016060919733718038], [0.0026541748084127903, 0.0018652357393875718, 0.0013340865261852741, 0.0014521725242957473, 0.0013119588838890195, 0.0016397774452343583, 0.0014313450083136559], [0.0028310774359852076, 0.0021000802516937256, 0.0013143187388777733, 0.0018712301971390843, 0.0017781280912458897, 0.0017531097400933504, 0.0013506583636626601], [0.00380669254809618, 0.0017940871184691787, 0.0024804575368762016, 0.0027443759609013796, 0.0018057794077321887, 0.0024914664682000875, 0.0016505112871527672], [0.00334381521679461, 0.0019050216069445014, 0.0020033956971019506, 0.0022674137726426125, 0.00373381027020514, 0.001520611229352653, 0.0012498709838837385]], [[0.00552424555644393, 0.0010939461644738913, 0.001326721510849893, 0.0009733344777487218, 0.0010409073438495398, 0.0014692998956888914, 0.000877684447914362], [0.0023036382626742125, 0.0024707585107535124, 0.0011989648919552565, 0.000803917006123811, 0.0011387509293854237, 0.001030918792821467, 0.001159397535957396], [0.007560377474874258, 0.0020573525689542294, 0.0028990437276661396, 0.001539805205538869, 0.0024248172994703054, 0.0018842858262360096, 0.0014485267456620932], [0.0031075109727680683, 0.0012965998612344265, 0.0025684786960482597, 0.002231663092970848, 0.0021676449105143547, 0.0015130097744986415, 0.0018286932026967406], [0.0027166602667421103, 0.0007639912655577064, 0.0017179118003696203, 0.0014066805597394705, 0.0020939670503139496, 0.0013450714759528637, 0.0012892537051811814], [0.009904085658490658, 0.0025122209917753935, 0.0024667757097631693, 0.0014204089529812336, 0.0017825467512011528, 0.0016198607627302408, 0.001401702407747507], [0.0049746171571314335, 0.002077154815196991, 0.0032444277312606573, 0.0019068876281380653, 0.0020264459308236837, 0.0016202162951231003, 0.0017812108853831887]]], [[[0.002658477518707514, 0.0018156202277168632, 0.0015500118024647236, 0.0019565371330827475, 0.0014197926502674818, 0.0018661293433979154, 0.001664025243371725], [0.0018287630518898368, 0.001332121784798801, 0.0014771153219044209, 0.001737777842208743, 0.0010694186203181744, 0.0021008613985031843, 0.0019083819352090359], [0.0020340196788311005, 0.00138017104472965, 0.0014459844678640366, 0.002346516354009509, 0.0013693588552996516, 0.0024996260181069374, 0.0019088669214397669], [0.0018165640067309141, 0.0018687492702156305, 0.0025580604560673237, 0.002879032166674733, 0.0013702873839065433, 0.0025491330306977034, 0.0027570421807467937], [0.002547851298004389, 0.002078116638585925, 0.0022846078500151634, 0.0028454111889004707, 0.0014100633561611176, 0.0026494425255805254, 0.0025880641769617796], [0.001167359878309071, 0.0009526317589916289, 0.0019507530378177762, 0.0019946566317230463, 0.0010655438527464867, 0.0018504844047129154, 0.001813407288864255], [0.0025223346892744303, 0.0019883355125784874, 0.0022900791373103857, 0.002830222714692354, 0.0013058031909167767, 0.0034075395669788122, 0.00313263270072639]], [[0.0041648889891803265, 0.001808580826036632, 0.0017372251022607088, 0.0015502506867051125, 0.0021971329115331173, 0.0020957523956894875, 0.0019681593403220177], [0.0025980148930102587, 0.0013789499644190073, 0.0008632537792436779, 0.0008447412983514369, 0.0012080036103725433, 0.0012934653786942363, 0.0011971810599789023], [0.0018240157514810562, 0.001099047833122313, 0.0006770126055926085, 0.000941593898460269, 0.0010063174413517118, 0.0012845719465985894, 0.001231751055456698], [0.0013563779648393393, 0.0011823860695585608, 0.0010657774982973933, 0.0007591769681312144, 0.0008377753547392786, 0.0011545917950570583, 0.001278886222280562], [0.0019419321324676275, 0.001689083524979651, 0.0015402717981487513, 0.0011793404119089246, 0.0010166671127080917, 0.0016313658561557531, 0.001718121231533587], [0.0010806209174916148, 0.0009072840912267566, 0.0007059099152684212, 0.0007423328934237361, 0.0005641470779664814, 0.0009266595006920397, 0.0009160502813756466], [0.002317517763003707, 0.001541599165648222, 0.0012185124214738607, 0.0010929979616776109, 0.0010756734991446137, 0.001595399691723287, 0.00194378977175802]], [[0.0023334291763603687, 0.0018529348308220506, 0.0017325388034805655, 0.0019951246213167906, 0.0023333742283284664, 0.0017244890332221985, 0.0014216415584087372], [0.0016904674703255296, 0.0010095303878188133, 0.0013901152415201068, 0.0014887346187606454, 0.0015272045275196433, 0.0015667550032958388, 0.0016374621773138642], [0.001592176267877221, 0.0010459194891154766, 0.001009586383588612, 0.0019388339715078473, 0.0018184445798397064, 0.0016162182437255979, 0.0016091439174488187], [0.001971753081306815, 0.0016072383150458336, 0.001416531391441822, 0.0021071929950267076, 0.001886146841570735, 0.00175004405900836, 0.0016489687841385603], [0.002101360587403178, 0.0017673365073278546, 0.001594026805832982, 0.0023062624968588352, 0.0014427899150177836, 0.001959724584594369, 0.0019885131623595953], [0.0010898089967668056, 0.0016087691765278578, 0.0011215177364647388, 0.0014062768314033747, 0.0017474504420533776, 0.0012386838207021356, 0.0013976781629025936], [0.003187142312526703, 0.0030401155818253756, 0.0018296699272468686, 0.0026440047658979893, 0.0023320179898291826, 0.0023024810943752527, 0.0024973235558718443]], [[0.012145438231527805, 0.006730684544891119, 0.004475623369216919, 0.0025855042040348053, 0.002810277510434389, 0.003181559732183814, 0.004316877573728561], [0.004090435337275267, 0.003680396592244506, 0.0029678482096642256, 0.0023773955181241035, 0.0023502924013882875, 0.002523161703720689, 0.0028388211503624916], [0.0022812974639236927, 0.0024115988053381443, 0.002883482025936246, 0.0027147214859724045, 0.0022434962447732687, 0.002481033094227314, 0.003088010475039482], [0.0028317749965935946, 0.0027994737029075623, 0.004251715261489153, 0.003824458224698901, 0.0029001813381910324, 0.003224126761779189, 0.004609220195561647], [0.0025109010748565197, 0.0035420460626482964, 0.004287312738597393, 0.0034669942688196898, 0.002565769711509347, 0.0030328298453241587, 0.004481208976358175], [0.0021659929770976305, 0.002687802305445075, 0.0037452031392604113, 0.0033164550550282, 0.00226609013043344, 0.0027495045214891434, 0.0034568195696920156], [0.002631923882290721, 0.0026737998705357313, 0.004327686503529549, 0.0034943842329084873, 0.0023238896392285824, 0.0030171165708452463, 0.0035135725047439337]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-bfa775725d4e4b1892815aefff7a86a0\", \"include_layers\": [0, 1], \"include_heads\": [0, 1, 2, 3], \"total_heads\": 4}; // HACK: {\"attention\": [{\"name\": \"Encoder\", \"attn\": [[[[0.0001244435552507639, 0.00011266420187894255, 0.0002516293607186526, 0.00010624696005834267, 0.00013604268315248191, 0.0011972725624218583, 0.0013834101846441627], [0.0003067910438403487, 0.0003797105164267123, 0.0004895984893664718, 0.00017375298193655908, 0.0003455829282756895, 0.0015692779561504722, 0.0019053813302889466], [1.8029280909104273e-05, 1.5208333934424445e-05, 5.61276356165763e-05, 7.531493338319706e-06, 2.1538462533499114e-05, 0.0011991065694019198, 0.0014792606234550476], [0.0003953646228183061, 0.00038482435047626495, 0.0006200792849995196, 0.0003139626351185143, 0.0004176825168542564, 0.001966077135875821, 0.0020922126714140177], [0.00013869850954506546, 0.00014263740740716457, 0.00029617061954922974, 6.48902787361294e-05, 0.00013018528989050537, 0.0014126208843663335, 0.0017707661027088761], [8.030361264843577e-09, 7.032122439909472e-09, 4.2573751102281676e-07, 9.114856092118373e-10, 1.5806364217496593e-08, 0.000583222892601043, 0.0015968417283147573], [4.478146742314948e-09, 3.804466519596872e-09, 2.3985046482266625e-07, 5.783113987689603e-10, 9.901888198271536e-09, 0.0005666323122568429, 0.0015012265648692846]], [[0.00027167663211002946, 0.00020005452097393572, 0.00035258856951259077, 0.0001818927557906136, 0.0002781347429845482, 0.0016339885769411922, 0.0021504932083189487], [0.00021942505554761738, 0.00020838889759033918, 0.00037267437437549233, 0.0002254763967357576, 0.00021141963952686638, 0.001391371712088585, 0.0016143773682415485], [4.66657911601942e-05, 2.8734992156387307e-05, 6.315022619673982e-05, 2.5611572709749453e-05, 3.186591493431479e-05, 0.001683767419308424, 0.0012509392108768225], [0.0005671802209690213, 0.0004542050010059029, 0.0006745471619069576, 0.0004948038258589804, 0.00043935165740549564, 0.002174914116039872, 0.0014460764359682798], [0.00028907047817483544, 0.00024943429161794484, 0.0003815964446403086, 0.00022193806944414973, 0.00020766039961017668, 0.0019101742655038834, 0.00178981339558959], [9.300777747967004e-08, 3.669113013415881e-08, 2.9633733333866985e-07, 2.6537792408021232e-08, 4.920474694358745e-08, 0.0005396560300141573, 0.0010779434815049171], [7.621545705660537e-08, 1.4437678608203441e-08, 1.8405880553018505e-07, 1.7301337251751647e-08, 2.9575497606515455e-08, 0.0005127748590894043, 0.0008898610831238329]], [[0.0007952657761052251, 0.000874561257660389, 0.001177239348180592, 0.0008014417253434658, 0.0009596769232302904, 0.001269379397854209, 0.0017803125083446503], [0.00041581137338653207, 0.0005089842597953975, 0.0005281173507682979, 0.00040344413719139993, 0.0006173507426865399, 0.0019264526199549437, 0.0014920583926141262], [9.19521989999339e-05, 9.744636190589517e-05, 0.00014071501209400594, 8.363241795450449e-05, 0.00010897513129748404, 0.0017378325574100018, 0.0015215340536087751], [0.0005913384375162423, 0.0004664700827561319, 0.0008208560175262392, 0.0007805845816619694, 0.000483395648188889, 0.0016029059188440442, 0.0018685426330193877], [0.0005404638941399753, 0.0004083497915416956, 0.000848443596623838, 0.0005872796173207462, 0.0007631818298250437, 0.0019634985364973545, 0.001559940748848021], [3.8652063949484727e-07, 2.2344001138208114e-07, 1.478648641750624e-06, 2.3362976264706958e-07, 2.789491588828241e-07, 0.0007234656368382275, 0.0009868611814454198], [3.016745324657677e-07, 2.2261338017415255e-07, 1.6161548046511598e-06, 2.1723104737247922e-07, 3.183396302119945e-07, 0.0007928681443445385, 0.0007818498997949064]], [[0.0009111871477216482, 0.0007607329171150923, 0.0008668781374581158, 0.0005971425562165678, 0.0007442832575179636, 0.002466397825628519, 0.002316609723493457], [0.0006017641280777752, 0.0008089006878435612, 0.0006385153392329812, 0.0009037574636749923, 0.00065309286583215, 0.002065225737169385, 0.001821330632083118], [0.00010643474524840713, 9.622338984627277e-05, 0.0001098678694688715, 7.453877333318815e-05, 0.00013202666013967246, 0.0020079489331692457, 0.0018175721634179354], [0.0005654582055285573, 0.0005507713067345321, 0.0007766984635964036, 0.0005084159784018993, 0.0007362078758887947, 0.0017437950009480119, 0.001939503476023674], [0.000545306655112654, 0.00041268038330599666, 0.0005978436674922705, 0.0003216119948774576, 0.0004915076424367726, 0.0018738089129328728, 0.001962667563930154], [2.1019232576691138e-07, 1.6695875615369005e-07, 3.068323906063597e-07, 9.437309245186043e-08, 2.6731277102953754e-07, 0.002782493829727173, 0.0017241952009499073], [1.1789978060505746e-07, 1.2063320298238978e-07, 1.9224439995468856e-07, 7.515063771279529e-08, 2.1620864742999402e-07, 0.00227953027933836, 0.0017691429238766432]]], [[[0.0006736136274412274, 0.0007827648078091443, 0.0009466311894357204, 0.0007495568133890629, 0.000747990095987916, 0.0022481216583400965, 0.0017716604052111506], [0.0006707112188450992, 0.0007978093926794827, 0.0009514944977127016, 0.0007586644496768713, 0.0007552676252089441, 0.002272462472319603, 0.0017774841981008649], [0.0006558556342497468, 0.0007697020773775876, 0.0009414860978722572, 0.0007273551309481263, 0.0007353550754487514, 0.0022797374986112118, 0.0017596272518858314], [0.0006744883139617741, 0.0008064088760875165, 0.0009532605763524771, 0.0007546594715677202, 0.0007490649004466832, 0.0022631227038800716, 0.0017744177021086216], [0.0006843882147222757, 0.0007876748568378389, 0.0009710174635984004, 0.0007631796761415899, 0.0007571456371806562, 0.0022874013520777225, 0.0017770547419786453], [0.0006209186394698918, 0.0007394569693133235, 0.0009067946812137961, 0.0006954854470677674, 0.0006965604261495173, 0.002279226202517748, 0.0017474234336987138], [0.0006109087262302637, 0.0007302847225219011, 0.0008970409398898482, 0.0006975356955081224, 0.0006873281090520322, 0.0022942631039768457, 0.0017433528555557132]], [[0.00039741594810038805, 0.00036316816112957895, 0.0006120743928477168, 0.0004714024253189564, 0.00028561288490891457, 0.001856151269748807, 0.0019523273222148418], [0.0003702104149851948, 0.00034935781150124967, 0.0006142938509583473, 0.0004640950355678797, 0.000279448926448822, 0.001872523222118616, 0.0019456201698631048], [0.0003651919250842184, 0.0003406937757972628, 0.0005949000478722155, 0.0004470513667911291, 0.0002694002178031951, 0.0018642584327608347, 0.00195637927390635], [0.0003868200583383441, 0.0003670902515295893, 0.0006138746393844485, 0.0004709497152362019, 0.00028897947049699724, 0.001857637893408537, 0.0019554717000573874], [0.00037732577766291797, 0.00036234536673873663, 0.0005974201485514641, 0.0004526441334746778, 0.0002779133792500943, 0.0018552833935245872, 0.0019559680949896574], [0.00034810806391760707, 0.00032161467242985964, 0.0005625317571684718, 0.0004147374420426786, 0.0002470424515195191, 0.0018635448068380356, 0.0019463645294308662], [0.00034463501651771367, 0.0003224253887310624, 0.0005597746348939836, 0.0004175882204435766, 0.0002472986525390297, 0.0018574731657281518, 0.001960442401468754]], [[0.0006720508099533617, 0.0007765429327264428, 0.000929080240894109, 0.0006095852586440742, 0.0007760734879411757, 0.0017631046939641237, 0.0019561403896659613], [0.0006775740766897798, 0.0007850467809475958, 0.0009350682375952601, 0.0006176198949106038, 0.0008087765891104937, 0.001745070912875235, 0.0019781014416366816], [0.0006451876251958311, 0.0007548555149696767, 0.0009184554219245911, 0.0005862449761480093, 0.0007760583539493382, 0.0017534219659864902, 0.0019657667726278305], [0.0006744327838532627, 0.0007833449053578079, 0.0009477480198256671, 0.0006119955214671791, 0.0008033321355469525, 0.001752006821334362, 0.001972441328689456], [0.000663267623167485, 0.0007889120024628937, 0.0009342856938019395, 0.0006102737388573587, 0.0007925989339128137, 0.0017525126459077, 0.0019620885141193867], [0.0006444320897571743, 0.0007449389086104929, 0.000903993146494031, 0.0005703219212591648, 0.0007627464365214109, 0.0017556716920807958, 0.0019702457357198], [0.0006372114294208586, 0.0007402864866890013, 0.0008976082899607718, 0.0005650772363878787, 0.0007550687296316028, 0.0017432798631489277, 0.001971327932551503]], [[0.00031113281147554517, 0.0003014165267813951, 0.0005125253228470683, 0.00037184703978709877, 0.00048024949501268566, 0.0017863217508420348, 0.001973920501768589], [0.0003072340623475611, 0.0002994237293023616, 0.0005116547690704465, 0.0003664434188976884, 0.00048007292207330465, 0.0017902979161590338, 0.001973962876945734], [0.00029468172579072416, 0.00029182492289692163, 0.0005087347817607224, 0.00035208059125579894, 0.0004642635176423937, 0.0017909975722432137, 0.0019813987892121077], [0.0003161942586302757, 0.0003067788784392178, 0.000521274923812598, 0.0003709437733050436, 0.00048547962796874344, 0.001802865881472826, 0.001987478695809841], [0.00030716718174517155, 0.00030071279616095126, 0.0005146324401721358, 0.0003698128566611558, 0.0004759263538289815, 0.001801209757104516, 0.0020009540021419525], [0.00027253921143710613, 0.00026216564583592117, 0.00046917388681322336, 0.0003255120827816427, 0.00043420138536021113, 0.0017805311363190413, 0.001993263605982065], [0.00026691571110859513, 0.00025551332510076463, 0.0004653250507544726, 0.00031945930095389485, 0.0004261027497705072, 0.001778463483788073, 0.001992118777707219]]]], \"left_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}, {\"name\": \"Decoder\", \"attn\": [[[[0.00196792627684772, 0.0019924244843423367, 0.001948700868524611, 0.0019674133509397507, 0.001986196031793952, 0.0019617523066699505, 0.0019482981879264116], [0.001966990064829588, 0.001984128961339593, 0.001958775334060192, 0.0019473930587992072, 0.0019849056843668222, 0.0019569052383303642, 0.0019542898517102003], [0.0019887289963662624, 0.002008314011618495, 0.001972887199372053, 0.0019823319744318724, 0.001994878053665161, 0.0019597038626670837, 0.001955584157258272], [0.001962999114766717, 0.001986286835744977, 0.0019510093843564391, 0.001956256339326501, 0.001964643131941557, 0.001960468478500843, 0.001949843717738986], [0.0019726138561964035, 0.001988248201087117, 0.0019554898608475924, 0.0019500748021528125, 0.0019832623656839132, 0.001954542938619852, 0.00194749073125422], [0.001959957182407379, 0.001996913691982627, 0.001950957695953548, 0.0019815401174128056, 0.001991579309105873, 0.0019605965353548527, 0.0019538707565516233], [0.0019495681626722217, 0.001959865214303136, 0.0019312291406095028, 0.0019465419463813305, 0.0019550775177776814, 0.001962303416803479, 0.0019518262706696987]], [[0.0019666494335979223, 0.0019624142441898584, 0.0019821366295218468, 0.001977745443582535, 0.001985287992283702, 0.001969909993931651, 0.0019510264974087477], [0.0019896994344890118, 0.0019808660726994276, 0.0019979430362582207, 0.0020095149520784616, 0.001986938063055277, 0.0019593099132180214, 0.0019499990157783031], [0.0019712024368345737, 0.001960474532097578, 0.0019970338325947523, 0.001980607397854328, 0.0019794467370957136, 0.0019540139473974705, 0.0019549736753106117], [0.0019732865039259195, 0.0019506958778947592, 0.00198213174007833, 0.0019695444498211145, 0.0019744280725717545, 0.0019560763612389565, 0.0019491029670462012], [0.001985149225220084, 0.001985670533031225, 0.001988215371966362, 0.002003168687224388, 0.001970443641766906, 0.001954195322468877, 0.0019476808374747634], [0.0020008408464491367, 0.001971989404410124, 0.0019777831621468067, 0.0019894782453775406, 0.0019867755472660065, 0.001956373453140259, 0.001948135788552463], [0.0019539613276720047, 0.0019488681573420763, 0.0019527219701558352, 0.0019499191548675299, 0.0019432505359873176, 0.0019520032219588757, 0.0019513150909915566]], [[0.0019518331391736865, 0.0019612747710198164, 0.001951485755853355, 0.0019824847113341093, 0.001951782964169979, 0.001955929910764098, 0.0019495667656883597], [0.001940602553077042, 0.001978034619241953, 0.0019699065014719963, 0.0019708089530467987, 0.0019540726207196712, 0.0019522877410054207, 0.001954081002622843], [0.0019494262523949146, 0.001985308015719056, 0.0019489009864628315, 0.001982596004381776, 0.001960010966286063, 0.0019511992577463388, 0.001959258923307061], [0.001946115749888122, 0.001977519830688834, 0.0019524636445567012, 0.00198703957721591, 0.0019587548449635506, 0.001956515247002244, 0.0019519133493304253], [0.0019592095632106066, 0.001972056459635496, 0.001964532071724534, 0.002002664376050234, 0.001978265354409814, 0.0019637923687696457, 0.0019529638811945915], [0.0019618533551692963, 0.001956533407792449, 0.0019528113771229982, 0.001988637261092663, 0.0019488141406327486, 0.0019522934453561902, 0.0019554379396140575], [0.0019321413710713387, 0.001950260833837092, 0.001937765977345407, 0.0019813338294625282, 0.0019244705326855183, 0.0019524224335327744, 0.0019536945037543774]], [[0.0019946424290537834, 0.001978556625545025, 0.0019819268491119146, 0.0020125596784055233, 0.0019923143554478884, 0.0019566707778722048, 0.001952262013219297], [0.001977883977815509, 0.001959608867764473, 0.001968641532585025, 0.0019824227783828974, 0.001966861542314291, 0.0019600316882133484, 0.0019565471448004246], [0.0019723870791494846, 0.0019556484185159206, 0.001960964873433113, 0.0019755391404032707, 0.001972456928342581, 0.001957378815859556, 0.0019547129049897194], [0.001961882458999753, 0.0019668051972985268, 0.0019589923322200775, 0.0019610989838838577, 0.00198723329231143, 0.001958656357601285, 0.0019553399179130793], [0.0019818944856524467, 0.0019921762868762016, 0.0019859876483678818, 0.0020148924086242914, 0.0020066804718226194, 0.0019581401720643044, 0.0019555019680410624], [0.0019624934066087008, 0.0019748746417462826, 0.0019767461344599724, 0.0019766087643802166, 0.0019934377633035183, 0.001958707347512245, 0.0019602274987846613], [0.0019675912335515022, 0.0019456855952739716, 0.001945626107044518, 0.001967812655493617, 0.001956124557182193, 0.001953291706740856, 0.001951445359736681]]], [[[0.0019490944687277079, 0.002040244173258543, 0.001972732599824667, 0.0020170765928924084, 0.0019744010642170906, 0.0019489806145429611, 0.0019580277148634195], [0.0019505181116983294, 0.0020282771438360214, 0.001955499639734626, 0.0020059761591255665, 0.001974172657355666, 0.001960697118192911, 0.0019569220021367073], [0.0019391919486224651, 0.0020327751990407705, 0.001964116934686899, 0.001999113243073225, 0.0019542493391782045, 0.0019512843573465943, 0.001956053776666522], [0.0019507658435031772, 0.002041537081822753, 0.0019714776426553726, 0.0020011933520436287, 0.001962908310815692, 0.001952211488969624, 0.0019506693352013826], [0.0019526862306520343, 0.002036262769252062, 0.001965384231880307, 0.0020052569452673197, 0.0019656678196042776, 0.0019515340682119131, 0.001957800006493926], [0.0019503295188769698, 0.002046822104603052, 0.001967406366020441, 0.0020022052340209484, 0.00196312484331429, 0.0019479839829728007, 0.001950486097484827], [0.0019362756283953786, 0.002023534383624792, 0.0019627027213573456, 0.001992327393963933, 0.0019539780914783478, 0.0019497702596709132, 0.001956457505002618]], [[0.002060863422229886, 0.0019968345295637846, 0.002022764878347516, 0.0020458248909562826, 0.002036196645349264, 0.0019675507210195065, 0.0019582626409828663], [0.0020357484463602304, 0.0019831513054668903, 0.002005866263061762, 0.002020974177867174, 0.0020150926429778337, 0.0019768306519836187, 0.001954258419573307], [0.002031270880252123, 0.0019803617615252733, 0.0019974769093096256, 0.002025180496275425, 0.002012725453823805, 0.0019673951901495457, 0.0019562900997698307], [0.0020287095103412867, 0.001985005335882306, 0.002014914993196726, 0.002037788974121213, 0.002025246387347579, 0.0019728911574929953, 0.0019570074509829283], [0.0020581709686666727, 0.001997264102101326, 0.0020112188067287207, 0.0020568552426993847, 0.002033790573477745, 0.0019714203663170338, 0.0019549764692783356], [0.00203397823497653, 0.00198573200032115, 0.002010033233091235, 0.0020143697038292885, 0.00202631251886487, 0.0019687931053340435, 0.001955615356564522], [0.002010362921282649, 0.001973184524103999, 0.001980905421078205, 0.002011431148275733, 0.002005484886467457, 0.0019694743677973747, 0.001964760944247246]], [[0.001999486470595002, 0.0019916119053959846, 0.0019851403776556253, 0.0019777528941631317, 0.001976937986910343, 0.0019662242848426104, 0.0019642349798232317], [0.002001754241064191, 0.0019768252968788147, 0.0019654803909361362, 0.001967380987480283, 0.0019606712739914656, 0.001969889970496297, 0.0019595192279666662], [0.002003338420763612, 0.001983124064281583, 0.001970144221559167, 0.0019654082134366035, 0.0019660929683595896, 0.0019692459609359503, 0.001958344364538789], [0.002007861156016588, 0.001987168798223138, 0.0019840600434690714, 0.0019814164843410254, 0.001969703007489443, 0.0019690790213644505, 0.0019580498337745667], [0.0020121836569160223, 0.0019887920934706926, 0.001962231006473303, 0.0019824765622615814, 0.0019640529062598944, 0.0019631413742899895, 0.0019590745214372873], [0.0020100700203329325, 0.001996978186070919, 0.0019797938875854015, 0.001972790341824293, 0.0019728641491383314, 0.001970375655218959, 0.0019549482967704535], [0.001991769066080451, 0.001970176585018635, 0.0019551245495676994, 0.0019515450112521648, 0.001958750421181321, 0.0019640224054455757, 0.0019521234789863229]], [[0.0020028245635330677, 0.001996856415644288, 0.0019674976356327534, 0.001953901955857873, 0.0019940652418881655, 0.0019641646649688482, 0.001954959938302636], [0.001995844766497612, 0.0019799936562776566, 0.001966798910871148, 0.0019558218773454428, 0.0019870828837156296, 0.001965765841305256, 0.0019517035689204931], [0.0019948454573750496, 0.001976232510060072, 0.0019684943836182356, 0.00198021880351007, 0.0019936293829232454, 0.0019654498901218176, 0.0019556765910238028], [0.0019989353604614735, 0.001989632146432996, 0.0019704566802829504, 0.001966312061995268, 0.001993130659684539, 0.0019713016226887703, 0.0019522259244695306], [0.0020072446204721928, 0.002011402975767851, 0.0019655218347907066, 0.0019602912943810225, 0.0019958773627877235, 0.001974351704120636, 0.001953576225787401], [0.001987726893275976, 0.0019916302990168333, 0.001977794338017702, 0.00196055811829865, 0.001988864503800869, 0.0019709330517798662, 0.001954174367710948], [0.0019805817864835262, 0.0019713593646883965, 0.0019563334062695503, 0.0019620254170149565, 0.0019828020595014095, 0.0019660794641822577, 0.0019502335926517844]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"]}, {\"name\": \"Cross\", \"attn\": [[[[0.0018907840130850673, 0.0006545690703205764, 0.001013795379549265, 0.000669850327540189, 0.0009301647078245878, 0.0010438591707497835, 0.0009906711056828499], [0.0020754754077643156, 0.0016920268535614014, 0.0019179475493729115, 0.0018678444903343916, 0.0024772845208644867, 0.001952912425622344, 0.0017753101419657469], [0.0009741291869431734, 0.0012223860248923302, 0.0010262762662023306, 0.0007965767872519791, 0.0007938087219372392, 0.0008265154319815338, 0.0008562952280044556], [0.001958247972652316, 0.001424274523742497, 0.002145964652299881, 0.0016172884497791529, 0.0016958625055849552, 0.0015427826438099146, 0.001688189571723342], [0.0023526111617684364, 0.0009281941456720233, 0.001895618624985218, 0.0011766733368858695, 0.0016946783289313316, 0.0012597328750416636, 0.0015489495126530528], [0.0021699187345802784, 0.0014338481705635786, 0.001321551506407559, 0.0009670233121141791, 0.0013961985241621733, 0.0009149193065240979, 0.0010450856061652303], [0.0013964945683255792, 0.001091045793145895, 0.0015357251977548003, 0.0013037041062489152, 0.0015433450462296605, 0.0010065571404993534, 0.0013077325420454144]], [[0.0020809979178011417, 0.0005780319916084409, 0.0003582587232813239, 0.00022170293959788978, 0.0006386962486431003, 0.00034999646595679224, 0.0002838528307620436], [0.001976816914975643, 0.0024754067417234182, 0.000747966580092907, 0.0005993059603497386, 0.0011211080709472299, 0.0006994821596890688, 0.0006880377768538892], [0.0047377836890518665, 0.0014896432403475046, 0.0016231206245720387, 0.0009479708969593048, 0.0011624168837442994, 0.0011655943235382438, 0.0009784664725884795], [0.005005617626011372, 0.0017514583887532353, 0.0009242980740964413, 0.0005625000922009349, 0.0014633717946708202, 0.0008473880589008331, 0.0007787581998854876], [0.0023549937177449465, 0.0012948508374392986, 0.0006965223583392799, 0.00076757010538131, 0.00163964310195297, 0.0007374039269052446, 0.0006726642022840679], [0.0033066486939787865, 0.0023656035773456097, 0.002209259429946542, 0.0016873892163857818, 0.0017840780783444643, 0.0016797153512015939, 0.0015469115460291505], [0.003027813509106636, 0.0020618003327399492, 0.002366369590163231, 0.002517594024538994, 0.0019390099914744496, 0.0019140916410833597, 0.0021509991493076086]], [[0.0022792944218963385, 0.0019919401966035366, 0.0015943101607263088, 0.0022225435823202133, 0.0023558377288281918, 0.0018452840158715844, 0.0012925018090754747], [0.004644542466849089, 0.0010289803612977266, 0.0017524220747873187, 0.0022530509158968925, 0.0016745920293033123, 0.0016307373298332095, 0.0011530136689543724], [0.0057770926505327225, 0.0016698201652616262, 0.002215599175542593, 0.0022884567733854055, 0.0025761937722563744, 0.0021749669685959816, 0.0016060919733718038], [0.0026541748084127903, 0.0018652357393875718, 0.0013340865261852741, 0.0014521725242957473, 0.0013119588838890195, 0.0016397774452343583, 0.0014313450083136559], [0.0028310774359852076, 0.0021000802516937256, 0.0013143187388777733, 0.0018712301971390843, 0.0017781280912458897, 0.0017531097400933504, 0.0013506583636626601], [0.00380669254809618, 0.0017940871184691787, 0.0024804575368762016, 0.0027443759609013796, 0.0018057794077321887, 0.0024914664682000875, 0.0016505112871527672], [0.00334381521679461, 0.0019050216069445014, 0.0020033956971019506, 0.0022674137726426125, 0.00373381027020514, 0.001520611229352653, 0.0012498709838837385]], [[0.00552424555644393, 0.0010939461644738913, 0.001326721510849893, 0.0009733344777487218, 0.0010409073438495398, 0.0014692998956888914, 0.000877684447914362], [0.0023036382626742125, 0.0024707585107535124, 0.0011989648919552565, 0.000803917006123811, 0.0011387509293854237, 0.001030918792821467, 0.001159397535957396], [0.007560377474874258, 0.0020573525689542294, 0.0028990437276661396, 0.001539805205538869, 0.0024248172994703054, 0.0018842858262360096, 0.0014485267456620932], [0.0031075109727680683, 0.0012965998612344265, 0.0025684786960482597, 0.002231663092970848, 0.0021676449105143547, 0.0015130097744986415, 0.0018286932026967406], [0.0027166602667421103, 0.0007639912655577064, 0.0017179118003696203, 0.0014066805597394705, 0.0020939670503139496, 0.0013450714759528637, 0.0012892537051811814], [0.009904085658490658, 0.0025122209917753935, 0.0024667757097631693, 0.0014204089529812336, 0.0017825467512011528, 0.0016198607627302408, 0.001401702407747507], [0.0049746171571314335, 0.002077154815196991, 0.0032444277312606573, 0.0019068876281380653, 0.0020264459308236837, 0.0016202162951231003, 0.0017812108853831887]]], [[[0.002658477518707514, 0.0018156202277168632, 0.0015500118024647236, 0.0019565371330827475, 0.0014197926502674818, 0.0018661293433979154, 0.001664025243371725], [0.0018287630518898368, 0.001332121784798801, 0.0014771153219044209, 0.001737777842208743, 0.0010694186203181744, 0.0021008613985031843, 0.0019083819352090359], [0.0020340196788311005, 0.00138017104472965, 0.0014459844678640366, 0.002346516354009509, 0.0013693588552996516, 0.0024996260181069374, 0.0019088669214397669], [0.0018165640067309141, 0.0018687492702156305, 0.0025580604560673237, 0.002879032166674733, 0.0013702873839065433, 0.0025491330306977034, 0.0027570421807467937], [0.002547851298004389, 0.002078116638585925, 0.0022846078500151634, 0.0028454111889004707, 0.0014100633561611176, 0.0026494425255805254, 0.0025880641769617796], [0.001167359878309071, 0.0009526317589916289, 0.0019507530378177762, 0.0019946566317230463, 0.0010655438527464867, 0.0018504844047129154, 0.001813407288864255], [0.0025223346892744303, 0.0019883355125784874, 0.0022900791373103857, 0.002830222714692354, 0.0013058031909167767, 0.0034075395669788122, 0.00313263270072639]], [[0.0041648889891803265, 0.001808580826036632, 0.0017372251022607088, 0.0015502506867051125, 0.0021971329115331173, 0.0020957523956894875, 0.0019681593403220177], [0.0025980148930102587, 0.0013789499644190073, 0.0008632537792436779, 0.0008447412983514369, 0.0012080036103725433, 0.0012934653786942363, 0.0011971810599789023], [0.0018240157514810562, 0.001099047833122313, 0.0006770126055926085, 0.000941593898460269, 0.0010063174413517118, 0.0012845719465985894, 0.001231751055456698], [0.0013563779648393393, 0.0011823860695585608, 0.0010657774982973933, 0.0007591769681312144, 0.0008377753547392786, 0.0011545917950570583, 0.001278886222280562], [0.0019419321324676275, 0.001689083524979651, 0.0015402717981487513, 0.0011793404119089246, 0.0010166671127080917, 0.0016313658561557531, 0.001718121231533587], [0.0010806209174916148, 0.0009072840912267566, 0.0007059099152684212, 0.0007423328934237361, 0.0005641470779664814, 0.0009266595006920397, 0.0009160502813756466], [0.002317517763003707, 0.001541599165648222, 0.0012185124214738607, 0.0010929979616776109, 0.0010756734991446137, 0.001595399691723287, 0.00194378977175802]], [[0.0023334291763603687, 0.0018529348308220506, 0.0017325388034805655, 0.0019951246213167906, 0.0023333742283284664, 0.0017244890332221985, 0.0014216415584087372], [0.0016904674703255296, 0.0010095303878188133, 0.0013901152415201068, 0.0014887346187606454, 0.0015272045275196433, 0.0015667550032958388, 0.0016374621773138642], [0.001592176267877221, 0.0010459194891154766, 0.001009586383588612, 0.0019388339715078473, 0.0018184445798397064, 0.0016162182437255979, 0.0016091439174488187], [0.001971753081306815, 0.0016072383150458336, 0.001416531391441822, 0.0021071929950267076, 0.001886146841570735, 0.00175004405900836, 0.0016489687841385603], [0.002101360587403178, 0.0017673365073278546, 0.001594026805832982, 0.0023062624968588352, 0.0014427899150177836, 0.001959724584594369, 0.0019885131623595953], [0.0010898089967668056, 0.0016087691765278578, 0.0011215177364647388, 0.0014062768314033747, 0.0017474504420533776, 0.0012386838207021356, 0.0013976781629025936], [0.003187142312526703, 0.0030401155818253756, 0.0018296699272468686, 0.0026440047658979893, 0.0023320179898291826, 0.0023024810943752527, 0.0024973235558718443]], [[0.012145438231527805, 0.006730684544891119, 0.004475623369216919, 0.0025855042040348053, 0.002810277510434389, 0.003181559732183814, 0.004316877573728561], [0.004090435337275267, 0.003680396592244506, 0.0029678482096642256, 0.0023773955181241035, 0.0023502924013882875, 0.002523161703720689, 0.0028388211503624916], [0.0022812974639236927, 0.0024115988053381443, 0.002883482025936246, 0.0027147214859724045, 0.0022434962447732687, 0.002481033094227314, 0.003088010475039482], [0.0028317749965935946, 0.0027994737029075623, 0.004251715261489153, 0.003824458224698901, 0.0029001813381910324, 0.003224126761779189, 0.004609220195561647], [0.0025109010748565197, 0.0035420460626482964, 0.004287312738597393, 0.0034669942688196898, 0.002565769711509347, 0.0030328298453241587, 0.004481208976358175], [0.0021659929770976305, 0.002687802305445075, 0.0037452031392604113, 0.0033164550550282, 0.00226609013043344, 0.0027495045214891434, 0.0034568195696920156], [0.002631923882290721, 0.0026737998705357313, 0.004327686503529549, 0.0034943842329084873, 0.0023238896392285824, 0.0030171165708452463, 0.0035135725047439337]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-bfa775725d4e4b1892815aefff7a86a0\", \"include_layers\": [0, 1], \"include_heads\": [0, 1, 2, 3], \"total_heads\": 4} is a template marker that is replaced by actual params.\n",
       "        const config = {};\n",
       "\n",
       "        const MIN_X = 0;\n",
       "        const MIN_Y = 0;\n",
       "        const DIV_WIDTH = 970;\n",
       "        const THUMBNAIL_PADDING = 5;\n",
       "        const DETAIL_WIDTH = 300;\n",
       "        const DETAIL_ATTENTION_WIDTH = 140;\n",
       "        const DETAIL_BOX_WIDTH = 80;\n",
       "        const DETAIL_BOX_HEIGHT = 18;\n",
       "        const DETAIL_PADDING = 15;\n",
       "        const ATTN_PADDING = 0;\n",
       "        const DETAIL_HEADING_HEIGHT = 25;\n",
       "        const HEADING_TEXT_SIZE = 15;\n",
       "        const HEADING_PADDING = 5;\n",
       "        const TEXT_SIZE = 13;\n",
       "        const TEXT_PADDING = 5;\n",
       "        const LAYER_COLORS = d3.schemeCategory10;\n",
       "        const PALETTE = {\n",
       "            'light': {\n",
       "                'text': 'black',\n",
       "                'background': 'white',\n",
       "                'highlight': '#F5F5F5'\n",
       "            },\n",
       "            'dark': {\n",
       "                'text': '#ccc',\n",
       "                'background': 'black',\n",
       "                'highlight': '#222'\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function render() {\n",
       "\n",
       "            // Set global state variables\n",
       "\n",
       "            var attData = config.attention[config.filter];\n",
       "            config.leftText = attData.left_text;\n",
       "            config.rightText = attData.right_text;\n",
       "            config.attn = attData.attn;\n",
       "            config.numLayers = config.attn.length;\n",
       "            config.numHeads = config.attn[0].length;\n",
       "            config.thumbnailBoxHeight = 7 * (12 / config.totalHeads);\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            config.thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n",
       "            config.thumbnailWidth = (DIV_WIDTH - axisSize) / config.totalHeads;\n",
       "            config.detailHeight = Math.max(config.leftText.length, config.rightText.length) * DETAIL_BOX_HEIGHT + 2 * DETAIL_PADDING + DETAIL_HEADING_HEIGHT;\n",
       "            config.divHeight = Math.max(config.numLayers * config.thumbnailHeight + axisSize, config.detailHeight);\n",
       "\n",
       "            const vis = $(`#${config.rootDivId} #vis`)\n",
       "            vis.empty();\n",
       "            vis.attr(\"height\", config.divHeight);\n",
       "            config.svg = d3.select(`#${config.rootDivId} #vis`)\n",
       "                .append('svg')\n",
       "                .attr(\"width\", DIV_WIDTH)\n",
       "                .attr(\"height\", config.divHeight)\n",
       "                .attr(\"fill\", getBackgroundColor());\n",
       "\n",
       "            renderAxisLabels();\n",
       "\n",
       "            var i;\n",
       "            var j;\n",
       "            for (i = 0; i < config.numLayers; i++) {\n",
       "                for (j = 0; j < config.numHeads; j++) {\n",
       "                    renderThumbnail(i, j);\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function renderAxisLabels() {\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            const tableWidth = config.thumbnailWidth * config.heads.length;\n",
       "            config.svg.append(\"text\")\n",
       "                .text(\"Heads\")\n",
       "                .attr(\"fill\", \"black\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n",
       "                .attr(\"x\", axisSize + tableWidth / 2)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", 0)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "            for (let i = 0; i < config.numHeads; i++) {\n",
       "                config.svg.append(\"text\")\n",
       "                    .text(config.heads[i])\n",
       "                    .attr(\"fill\", \"black\")\n",
       "                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                    .attr(\"x\", axisSize + (i + .5) * config.thumbnailWidth)\n",
       "                    .attr(\"text-anchor\", \"middle\")\n",
       "                    .attr(\"y\", HEADING_TEXT_SIZE + HEADING_PADDING)\n",
       "                    .attr(\"dy\", TEXT_SIZE);\n",
       "            }\n",
       "            let x = 0;\n",
       "            let y = axisSize + config.thumbnailHeight * config.layers.length / 2;\n",
       "            console.log(\"x\", x, y)\n",
       "            config.svg.append(\"text\")\n",
       "                .text(\"Layers\")\n",
       "                .attr(\"fill\", \"black\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .attr(\"transform\", \"rotate(270, \" + x  + \", \" + y + \")\")\n",
       "                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "            for (let i = 0; i < config.numLayers; i++) {\n",
       "                x = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE; // HACK\n",
       "                y = axisSize + (i + .5) * config.thumbnailHeight;\n",
       "                config.svg.append(\"text\")\n",
       "                    .text(config.layers[i])\n",
       "                    .attr(\"fill\", \"black\")\n",
       "                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                    .attr(\"x\", x)\n",
       "                    .attr(\"text-anchor\", \"end\")\n",
       "                    .attr(\"y\", y)\n",
       "                    .attr(\"dy\", TEXT_SIZE / 2);\n",
       "            }\n",
       "        }\n",
       "\n",
       "\n",
       "        function renderThumbnail(layerIndex, headIndex) {\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING\n",
       "            const x = headIndex * config.thumbnailWidth + axisSize;\n",
       "            const y = layerIndex * config.thumbnailHeight + axisSize;\n",
       "            renderThumbnailAttn(x, y, config.attn[layerIndex][headIndex], layerIndex, headIndex);\n",
       "        }\n",
       "\n",
       "        function renderDetail(att, layerIndex, headIndex) {\n",
       "            const axisSize = TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            var xOffset = .8 * config.thumbnailWidth;\n",
       "            var maxX = DIV_WIDTH;\n",
       "            var maxY = config.divHeight - 3;\n",
       "            var leftPos = axisSize + headIndex * config.thumbnailWidth;\n",
       "            var x = leftPos + THUMBNAIL_PADDING + xOffset;\n",
       "            if (x < MIN_X) {\n",
       "                x = MIN_X;\n",
       "            } else if (x + DETAIL_WIDTH > maxX) {\n",
       "                x = leftPos + THUMBNAIL_PADDING - DETAIL_WIDTH + 8;\n",
       "            }\n",
       "            var posLeftText = x;\n",
       "            var posAttention = posLeftText + DETAIL_BOX_WIDTH;\n",
       "            var posRightText = posAttention + DETAIL_ATTENTION_WIDTH;\n",
       "            var thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n",
       "            var yOffset = 20;\n",
       "            var y = layerIndex * thumbnailHeight + THUMBNAIL_PADDING + yOffset;\n",
       "            if (y < MIN_Y) {\n",
       "                y = MIN_Y;\n",
       "            } else if (y + config.detailHeight > maxY) {\n",
       "                y = maxY - config.detailHeight;\n",
       "            }\n",
       "            renderDetailFrame(x, y, layerIndex);\n",
       "            y = y + DETAIL_PADDING;\n",
       "            renderDetailHeading(x, y, layerIndex, headIndex);\n",
       "            y = y + DETAIL_HEADING_HEIGHT;\n",
       "            renderDetailText(config.leftText, \"leftText\", posLeftText, y , layerIndex);\n",
       "            renderDetailAttn(posAttention, y, att, layerIndex, headIndex);\n",
       "            renderDetailText(config.rightText, \"rightText\", posRightText, y, layerIndex);\n",
       "        }\n",
       "\n",
       "        function renderDetailHeading(x, y, layerIndex, headIndex) {\n",
       "            var fillColor = getTextColor();\n",
       "            config.svg.append(\"text\")\n",
       "                .classed(\"detail\", true)\n",
       "                .text('Layer ' + config.layers[layerIndex] + \", Head \" + config.heads[headIndex])\n",
       "                .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .style(\"cursor\", \"default\")\n",
       "                .style(\"-webkit-user-select\", \"none\")\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .attr(\"x\", x + DETAIL_WIDTH / 2)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", DETAIL_HEADING_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_WIDTH)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "        }\n",
       "\n",
       "        function renderDetailText(text, id, x, y, layerIndex) {\n",
       "            var tokenContainer = config.svg.append(\"svg:g\")\n",
       "                .classed(\"detail\", true)\n",
       "                .selectAll(\"g\")\n",
       "                .data(text)\n",
       "                .enter()\n",
       "                .append(\"g\");\n",
       "\n",
       "            var fillColor = getTextColor();\n",
       "\n",
       "            tokenContainer.append(\"rect\")\n",
       "                .classed(\"highlight\", true)\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .style(\"opacity\", 0.0)\n",
       "                .attr(\"height\", DETAIL_BOX_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_BOX_WIDTH)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", function (d, i) {\n",
       "                    return y + i * DETAIL_BOX_HEIGHT;\n",
       "                });\n",
       "\n",
       "            var textContainer = tokenContainer.append(\"text\")\n",
       "                .classed(\"token\", true)\n",
       "                .text(function (d) {\n",
       "                    return d;\n",
       "                })\n",
       "                .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                .style(\"cursor\", \"default\")\n",
       "                .style(\"-webkit-user-select\", \"none\")\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", function (d, i) {\n",
       "                    return i * DETAIL_BOX_HEIGHT + y;\n",
       "                })\n",
       "                .attr(\"height\", DETAIL_BOX_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_BOX_WIDTH)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "\n",
       "            if (id == \"leftText\") {\n",
       "                textContainer.style(\"text-anchor\", \"end\")\n",
       "                    .attr(\"dx\", DETAIL_BOX_WIDTH - 2);\n",
       "                tokenContainer.on(\"mouseover\", function (d, index) {\n",
       "                    highlightSelection(index);\n",
       "                });\n",
       "                tokenContainer.on(\"mouseleave\", function () {\n",
       "                    unhighlightSelection();\n",
       "                });\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function highlightSelection(index) {\n",
       "            config.svg.select(\"#leftText\")\n",
       "                .selectAll(\".highlight\")\n",
       "                .style(\"opacity\", function (d, i) {\n",
       "                    return i == index ? 1.0 : 0.0;\n",
       "                });\n",
       "            config.svg.selectAll(\".attn-line-group\")\n",
       "                .style(\"opacity\", function (d, i) {\n",
       "                    return i == index ? 1.0 : 0.0;\n",
       "                });\n",
       "        }\n",
       "\n",
       "        function unhighlightSelection() {\n",
       "            config.svg.select(\"#leftText\")\n",
       "                .selectAll(\".highlight\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "            config.svg.selectAll(\".attn-line-group\")\n",
       "                .style(\"opacity\", 1);\n",
       "        }\n",
       "\n",
       "        function renderThumbnailAttn(x, y, att, layerIndex, headIndex) {\n",
       "\n",
       "            var attnContainer = config.svg.append(\"svg:g\");\n",
       "\n",
       "            var attnBackground = attnContainer.append(\"rect\")\n",
       "                .attr(\"id\", 'attn_background_' + layerIndex + \"_\" + headIndex)\n",
       "                .classed(\"attn_background\", true)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.thumbnailHeight)\n",
       "                .attr(\"width\", config.thumbnailWidth)\n",
       "                .attr(\"stroke-width\", 2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", 0)\n",
       "                .attr(\"fill\", getBackgroundColor());\n",
       "            var x1 = x + THUMBNAIL_PADDING;\n",
       "            var x2 = x1 + config.thumbnailWidth - 14;\n",
       "            var y1 = y + THUMBNAIL_PADDING;\n",
       "\n",
       "            attnContainer.selectAll(\"g\")\n",
       "                .data(att)\n",
       "                .enter()\n",
       "                .append(\"g\") // Add group for each source token\n",
       "                .attr(\"source-index\", function (d, i) { // Save index of source token\n",
       "                    return i;\n",
       "                })\n",
       "                .selectAll(\"line\")\n",
       "                .data(function (d) { // Loop over all target tokens\n",
       "                    return d;\n",
       "                })\n",
       "                .enter() // When entering\n",
       "                .append(\"line\")\n",
       "                .attr(\"x1\", x1)\n",
       "                .attr(\"y1\", function (d) {\n",
       "                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n",
       "                    return y1 + (sourceIndex + .5) * config.thumbnailBoxHeight;\n",
       "                })\n",
       "                .attr(\"x2\", x2)\n",
       "                .attr(\"y2\", function (d, targetIndex) {\n",
       "                    return y1 + (targetIndex + .5) * config.thumbnailBoxHeight;\n",
       "                })\n",
       "                .attr(\"stroke-width\", 2.2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", function (d) {\n",
       "                    return d;\n",
       "                });\n",
       "\n",
       "            var clickRegion = attnContainer.append(\"rect\")\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.thumbnailHeight)\n",
       "                .attr(\"width\", config.thumbnailWidth)\n",
       "                .style(\"opacity\", 0);\n",
       "\n",
       "            clickRegion.on(\"click\", function (d, index) {\n",
       "                var attnBackgroundOther = config.svg.selectAll(\".attn_background\");\n",
       "                attnBackgroundOther.attr(\"fill\", getBackgroundColor());\n",
       "                attnBackgroundOther.attr(\"stroke-opacity\", 0);\n",
       "\n",
       "                config.svg.selectAll(\".detail\").remove();\n",
       "                if (config.detail_layer != layerIndex || config.detail_head != headIndex) {\n",
       "                    renderDetail(att, layerIndex, headIndex);\n",
       "                    config.detail_layer = layerIndex;\n",
       "                    config.detail_head = headIndex;\n",
       "                    attnBackground.attr(\"fill\", getHighlightColor());\n",
       "                    attnBackground.attr(\"stroke-opacity\", .8);\n",
       "                } else {\n",
       "                    config.detail_layer = null;\n",
       "                    config.detail_head = null;\n",
       "                    attnBackground.attr(\"fill\", getBackgroundColor());\n",
       "                    attnBackground.attr(\"stroke-opacity\", 0);\n",
       "                }\n",
       "            });\n",
       "\n",
       "            clickRegion.on(\"mouseover\", function (d) {\n",
       "                d3.select(this).style(\"cursor\", \"pointer\");\n",
       "            });\n",
       "        }\n",
       "\n",
       "        function renderDetailFrame(x, y, layerIndex) {\n",
       "            var detailFrame = config.svg.append(\"rect\")\n",
       "                .classed(\"detail\", true)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.detailHeight)\n",
       "                .attr(\"width\", DETAIL_WIDTH)\n",
       "                .style(\"opacity\", 1)\n",
       "                .attr(\"stroke-width\", 1.5)\n",
       "                .attr(\"stroke-opacity\", 0.7)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex));\n",
       "        }\n",
       "\n",
       "        function renderDetailAttn(x, y, att, layerIndex) {\n",
       "            var attnContainer = config.svg.append(\"svg:g\")\n",
       "                .classed(\"detail\", true)\n",
       "                .attr(\"pointer-events\", \"none\");\n",
       "            attnContainer.selectAll(\"g\")\n",
       "                .data(att)\n",
       "                .enter()\n",
       "                .append(\"g\") // Add group for each source token\n",
       "                .classed('attn-line-group', true)\n",
       "                .attr(\"source-index\", function (d, i) { // Save index of source token\n",
       "                    return i;\n",
       "                })\n",
       "                .selectAll(\"line\")\n",
       "                .data(function (d) { // Loop over all target tokens\n",
       "                    return d;\n",
       "                })\n",
       "                .enter()\n",
       "                .append(\"line\")\n",
       "                .attr(\"x1\", x + ATTN_PADDING)\n",
       "                .attr(\"y1\", function (d) {\n",
       "                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n",
       "                    return y + (sourceIndex + .5) * DETAIL_BOX_HEIGHT;\n",
       "                })\n",
       "                .attr(\"x2\", x + DETAIL_ATTENTION_WIDTH - ATTN_PADDING)\n",
       "                .attr(\"y2\", function (d, targetIndex) {\n",
       "                    return y + (targetIndex + .5) * DETAIL_BOX_HEIGHT;\n",
       "                })\n",
       "                .attr(\"stroke-width\", 2.2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", function (d) {\n",
       "                    return d;\n",
       "                });\n",
       "        }\n",
       "\n",
       "        function getLayerColor(layer) {\n",
       "          return LAYER_COLORS[config.layers[layer] % 10];\n",
       "        }\n",
       "\n",
       "        function getTextColor() {\n",
       "            return PALETTE[config.mode]['text']\n",
       "        }\n",
       "\n",
       "        function getBackgroundColor() {\n",
       "           return PALETTE[config.mode]['background']\n",
       "        }\n",
       "\n",
       "        function getHighlightColor() {\n",
       "           return PALETTE[config.mode]['highlight']\n",
       "        }\n",
       "\n",
       "        function initialize() {\n",
       "            config.attention = params['attention'];\n",
       "            config.filter = params['default_filter'];\n",
       "            config.mode = params['display_mode'];\n",
       "            config.layers = params['include_layers']\n",
       "            config.heads = params['include_heads']\n",
       "            config.totalHeads = params['total_heads']\n",
       "            config.rootDivId = params['root_div_id'];\n",
       "            $(`#${config.rootDivId} #filter`).on('change', function (e) {\n",
       "                config.filter = e.currentTarget.value;\n",
       "                render();\n",
       "            });\n",
       "        }\n",
       "\n",
       "        initialize();\n",
       "        render();\n",
       "\n",
       "    });"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_view(\n",
    "    encoder_attention=tensor_encoder_attn,\n",
    "    decoder_attention=tensor_decoder_attn,\n",
    "    cross_attention=tensor_cross_attn,\n",
    "    encoder_tokens=tokens_a,\n",
    "    decoder_tokens=tokens_b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5a14a8b2-fd6e-43b9-981b-eebae0dbc987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-6be7e19aea9448bf8448673c90bd76f4\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                Layer: <select id=\"layer\"></select>\n",
       "                Attention: <select id=\"filter\"><option value=\"0\">Encoder</option>\n",
       "<option value=\"1\">Decoder</option>\n",
       "<option value=\"2\">Cross</option></select>\n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/**\n",
       " * @fileoverview Transformer Visualization D3 javascript code.\n",
       " *\n",
       " *\n",
       " *  Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n",
       " *\n",
       " * Change log:\n",
       " *\n",
       " * 12/19/18  Jesse Vig   Assorted cleanup. Changed orientation of attention matrices.\n",
       " * 12/29/20  Jesse Vig   Significant refactor.\n",
       " * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n",
       " * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n",
       " * 05/03/21  Jesse Vig   Adjust height of visualization dynamically\n",
       " * 07/25/21  Jesse Vig   Support layer filtering\n",
       " * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n",
       " **/\n",
       "\n",
       "require.config({\n",
       "  paths: {\n",
       "      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
       "    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
       "  }\n",
       "});\n",
       "\n",
       "requirejs(['jquery', 'd3'], function ($, d3) {\n",
       "\n",
       "    const params = {\"attention\": [{\"name\": \"Encoder\", \"attn\": [[[[0.0001244435552507639, 0.00011266420187894255, 0.0002516293607186526, 0.00010624696005834267, 0.00013604268315248191, 0.0011972725624218583, 0.0013834101846441627], [0.0003067910438403487, 0.0003797105164267123, 0.0004895984893664718, 0.00017375298193655908, 0.0003455829282756895, 0.0015692779561504722, 0.0019053813302889466], [1.8029280909104273e-05, 1.5208333934424445e-05, 5.61276356165763e-05, 7.531493338319706e-06, 2.1538462533499114e-05, 0.0011991065694019198, 0.0014792606234550476], [0.0003953646228183061, 0.00038482435047626495, 0.0006200792849995196, 0.0003139626351185143, 0.0004176825168542564, 0.001966077135875821, 0.0020922126714140177], [0.00013869850954506546, 0.00014263740740716457, 0.00029617061954922974, 6.48902787361294e-05, 0.00013018528989050537, 0.0014126208843663335, 0.0017707661027088761], [8.030361264843577e-09, 7.032122439909472e-09, 4.2573751102281676e-07, 9.114856092118373e-10, 1.5806364217496593e-08, 0.000583222892601043, 0.0015968417283147573], [4.478146742314948e-09, 3.804466519596872e-09, 2.3985046482266625e-07, 5.783113987689603e-10, 9.901888198271536e-09, 0.0005666323122568429, 0.0015012265648692846]], [[0.00027167663211002946, 0.00020005452097393572, 0.00035258856951259077, 0.0001818927557906136, 0.0002781347429845482, 0.0016339885769411922, 0.0021504932083189487], [0.00021942505554761738, 0.00020838889759033918, 0.00037267437437549233, 0.0002254763967357576, 0.00021141963952686638, 0.001391371712088585, 0.0016143773682415485], [4.66657911601942e-05, 2.8734992156387307e-05, 6.315022619673982e-05, 2.5611572709749453e-05, 3.186591493431479e-05, 0.001683767419308424, 0.0012509392108768225], [0.0005671802209690213, 0.0004542050010059029, 0.0006745471619069576, 0.0004948038258589804, 0.00043935165740549564, 0.002174914116039872, 0.0014460764359682798], [0.00028907047817483544, 0.00024943429161794484, 0.0003815964446403086, 0.00022193806944414973, 0.00020766039961017668, 0.0019101742655038834, 0.00178981339558959], [9.300777747967004e-08, 3.669113013415881e-08, 2.9633733333866985e-07, 2.6537792408021232e-08, 4.920474694358745e-08, 0.0005396560300141573, 0.0010779434815049171], [7.621545705660537e-08, 1.4437678608203441e-08, 1.8405880553018505e-07, 1.7301337251751647e-08, 2.9575497606515455e-08, 0.0005127748590894043, 0.0008898610831238329]], [[0.0007952657761052251, 0.000874561257660389, 0.001177239348180592, 0.0008014417253434658, 0.0009596769232302904, 0.001269379397854209, 0.0017803125083446503], [0.00041581137338653207, 0.0005089842597953975, 0.0005281173507682979, 0.00040344413719139993, 0.0006173507426865399, 0.0019264526199549437, 0.0014920583926141262], [9.19521989999339e-05, 9.744636190589517e-05, 0.00014071501209400594, 8.363241795450449e-05, 0.00010897513129748404, 0.0017378325574100018, 0.0015215340536087751], [0.0005913384375162423, 0.0004664700827561319, 0.0008208560175262392, 0.0007805845816619694, 0.000483395648188889, 0.0016029059188440442, 0.0018685426330193877], [0.0005404638941399753, 0.0004083497915416956, 0.000848443596623838, 0.0005872796173207462, 0.0007631818298250437, 0.0019634985364973545, 0.001559940748848021], [3.8652063949484727e-07, 2.2344001138208114e-07, 1.478648641750624e-06, 2.3362976264706958e-07, 2.789491588828241e-07, 0.0007234656368382275, 0.0009868611814454198], [3.016745324657677e-07, 2.2261338017415255e-07, 1.6161548046511598e-06, 2.1723104737247922e-07, 3.183396302119945e-07, 0.0007928681443445385, 0.0007818498997949064]], [[0.0009111871477216482, 0.0007607329171150923, 0.0008668781374581158, 0.0005971425562165678, 0.0007442832575179636, 0.002466397825628519, 0.002316609723493457], [0.0006017641280777752, 0.0008089006878435612, 0.0006385153392329812, 0.0009037574636749923, 0.00065309286583215, 0.002065225737169385, 0.001821330632083118], [0.00010643474524840713, 9.622338984627277e-05, 0.0001098678694688715, 7.453877333318815e-05, 0.00013202666013967246, 0.0020079489331692457, 0.0018175721634179354], [0.0005654582055285573, 0.0005507713067345321, 0.0007766984635964036, 0.0005084159784018993, 0.0007362078758887947, 0.0017437950009480119, 0.001939503476023674], [0.000545306655112654, 0.00041268038330599666, 0.0005978436674922705, 0.0003216119948774576, 0.0004915076424367726, 0.0018738089129328728, 0.001962667563930154], [2.1019232576691138e-07, 1.6695875615369005e-07, 3.068323906063597e-07, 9.437309245186043e-08, 2.6731277102953754e-07, 0.002782493829727173, 0.0017241952009499073], [1.1789978060505746e-07, 1.2063320298238978e-07, 1.9224439995468856e-07, 7.515063771279529e-08, 2.1620864742999402e-07, 0.00227953027933836, 0.0017691429238766432]]], [[[0.0006736136274412274, 0.0007827648078091443, 0.0009466311894357204, 0.0007495568133890629, 0.000747990095987916, 0.0022481216583400965, 0.0017716604052111506], [0.0006707112188450992, 0.0007978093926794827, 0.0009514944977127016, 0.0007586644496768713, 0.0007552676252089441, 0.002272462472319603, 0.0017774841981008649], [0.0006558556342497468, 0.0007697020773775876, 0.0009414860978722572, 0.0007273551309481263, 0.0007353550754487514, 0.0022797374986112118, 0.0017596272518858314], [0.0006744883139617741, 0.0008064088760875165, 0.0009532605763524771, 0.0007546594715677202, 0.0007490649004466832, 0.0022631227038800716, 0.0017744177021086216], [0.0006843882147222757, 0.0007876748568378389, 0.0009710174635984004, 0.0007631796761415899, 0.0007571456371806562, 0.0022874013520777225, 0.0017770547419786453], [0.0006209186394698918, 0.0007394569693133235, 0.0009067946812137961, 0.0006954854470677674, 0.0006965604261495173, 0.002279226202517748, 0.0017474234336987138], [0.0006109087262302637, 0.0007302847225219011, 0.0008970409398898482, 0.0006975356955081224, 0.0006873281090520322, 0.0022942631039768457, 0.0017433528555557132]], [[0.00039741594810038805, 0.00036316816112957895, 0.0006120743928477168, 0.0004714024253189564, 0.00028561288490891457, 0.001856151269748807, 0.0019523273222148418], [0.0003702104149851948, 0.00034935781150124967, 0.0006142938509583473, 0.0004640950355678797, 0.000279448926448822, 0.001872523222118616, 0.0019456201698631048], [0.0003651919250842184, 0.0003406937757972628, 0.0005949000478722155, 0.0004470513667911291, 0.0002694002178031951, 0.0018642584327608347, 0.00195637927390635], [0.0003868200583383441, 0.0003670902515295893, 0.0006138746393844485, 0.0004709497152362019, 0.00028897947049699724, 0.001857637893408537, 0.0019554717000573874], [0.00037732577766291797, 0.00036234536673873663, 0.0005974201485514641, 0.0004526441334746778, 0.0002779133792500943, 0.0018552833935245872, 0.0019559680949896574], [0.00034810806391760707, 0.00032161467242985964, 0.0005625317571684718, 0.0004147374420426786, 0.0002470424515195191, 0.0018635448068380356, 0.0019463645294308662], [0.00034463501651771367, 0.0003224253887310624, 0.0005597746348939836, 0.0004175882204435766, 0.0002472986525390297, 0.0018574731657281518, 0.001960442401468754]], [[0.0006720508099533617, 0.0007765429327264428, 0.000929080240894109, 0.0006095852586440742, 0.0007760734879411757, 0.0017631046939641237, 0.0019561403896659613], [0.0006775740766897798, 0.0007850467809475958, 0.0009350682375952601, 0.0006176198949106038, 0.0008087765891104937, 0.001745070912875235, 0.0019781014416366816], [0.0006451876251958311, 0.0007548555149696767, 0.0009184554219245911, 0.0005862449761480093, 0.0007760583539493382, 0.0017534219659864902, 0.0019657667726278305], [0.0006744327838532627, 0.0007833449053578079, 0.0009477480198256671, 0.0006119955214671791, 0.0008033321355469525, 0.001752006821334362, 0.001972441328689456], [0.000663267623167485, 0.0007889120024628937, 0.0009342856938019395, 0.0006102737388573587, 0.0007925989339128137, 0.0017525126459077, 0.0019620885141193867], [0.0006444320897571743, 0.0007449389086104929, 0.000903993146494031, 0.0005703219212591648, 0.0007627464365214109, 0.0017556716920807958, 0.0019702457357198], [0.0006372114294208586, 0.0007402864866890013, 0.0008976082899607718, 0.0005650772363878787, 0.0007550687296316028, 0.0017432798631489277, 0.001971327932551503]], [[0.00031113281147554517, 0.0003014165267813951, 0.0005125253228470683, 0.00037184703978709877, 0.00048024949501268566, 0.0017863217508420348, 0.001973920501768589], [0.0003072340623475611, 0.0002994237293023616, 0.0005116547690704465, 0.0003664434188976884, 0.00048007292207330465, 0.0017902979161590338, 0.001973962876945734], [0.00029468172579072416, 0.00029182492289692163, 0.0005087347817607224, 0.00035208059125579894, 0.0004642635176423937, 0.0017909975722432137, 0.0019813987892121077], [0.0003161942586302757, 0.0003067788784392178, 0.000521274923812598, 0.0003709437733050436, 0.00048547962796874344, 0.001802865881472826, 0.001987478695809841], [0.00030716718174517155, 0.00030071279616095126, 0.0005146324401721358, 0.0003698128566611558, 0.0004759263538289815, 0.001801209757104516, 0.0020009540021419525], [0.00027253921143710613, 0.00026216564583592117, 0.00046917388681322336, 0.0003255120827816427, 0.00043420138536021113, 0.0017805311363190413, 0.001993263605982065], [0.00026691571110859513, 0.00025551332510076463, 0.0004653250507544726, 0.00031945930095389485, 0.0004261027497705072, 0.001778463483788073, 0.001992118777707219]]]], \"left_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}, {\"name\": \"Decoder\", \"attn\": [[[[0.00196792627684772, 0.0019924244843423367, 0.001948700868524611, 0.0019674133509397507, 0.001986196031793952, 0.0019617523066699505, 0.0019482981879264116], [0.001966990064829588, 0.001984128961339593, 0.001958775334060192, 0.0019473930587992072, 0.0019849056843668222, 0.0019569052383303642, 0.0019542898517102003], [0.0019887289963662624, 0.002008314011618495, 0.001972887199372053, 0.0019823319744318724, 0.001994878053665161, 0.0019597038626670837, 0.001955584157258272], [0.001962999114766717, 0.001986286835744977, 0.0019510093843564391, 0.001956256339326501, 0.001964643131941557, 0.001960468478500843, 0.001949843717738986], [0.0019726138561964035, 0.001988248201087117, 0.0019554898608475924, 0.0019500748021528125, 0.0019832623656839132, 0.001954542938619852, 0.00194749073125422], [0.001959957182407379, 0.001996913691982627, 0.001950957695953548, 0.0019815401174128056, 0.001991579309105873, 0.0019605965353548527, 0.0019538707565516233], [0.0019495681626722217, 0.001959865214303136, 0.0019312291406095028, 0.0019465419463813305, 0.0019550775177776814, 0.001962303416803479, 0.0019518262706696987]], [[0.0019666494335979223, 0.0019624142441898584, 0.0019821366295218468, 0.001977745443582535, 0.001985287992283702, 0.001969909993931651, 0.0019510264974087477], [0.0019896994344890118, 0.0019808660726994276, 0.0019979430362582207, 0.0020095149520784616, 0.001986938063055277, 0.0019593099132180214, 0.0019499990157783031], [0.0019712024368345737, 0.001960474532097578, 0.0019970338325947523, 0.001980607397854328, 0.0019794467370957136, 0.0019540139473974705, 0.0019549736753106117], [0.0019732865039259195, 0.0019506958778947592, 0.00198213174007833, 0.0019695444498211145, 0.0019744280725717545, 0.0019560763612389565, 0.0019491029670462012], [0.001985149225220084, 0.001985670533031225, 0.001988215371966362, 0.002003168687224388, 0.001970443641766906, 0.001954195322468877, 0.0019476808374747634], [0.0020008408464491367, 0.001971989404410124, 0.0019777831621468067, 0.0019894782453775406, 0.0019867755472660065, 0.001956373453140259, 0.001948135788552463], [0.0019539613276720047, 0.0019488681573420763, 0.0019527219701558352, 0.0019499191548675299, 0.0019432505359873176, 0.0019520032219588757, 0.0019513150909915566]], [[0.0019518331391736865, 0.0019612747710198164, 0.001951485755853355, 0.0019824847113341093, 0.001951782964169979, 0.001955929910764098, 0.0019495667656883597], [0.001940602553077042, 0.001978034619241953, 0.0019699065014719963, 0.0019708089530467987, 0.0019540726207196712, 0.0019522877410054207, 0.001954081002622843], [0.0019494262523949146, 0.001985308015719056, 0.0019489009864628315, 0.001982596004381776, 0.001960010966286063, 0.0019511992577463388, 0.001959258923307061], [0.001946115749888122, 0.001977519830688834, 0.0019524636445567012, 0.00198703957721591, 0.0019587548449635506, 0.001956515247002244, 0.0019519133493304253], [0.0019592095632106066, 0.001972056459635496, 0.001964532071724534, 0.002002664376050234, 0.001978265354409814, 0.0019637923687696457, 0.0019529638811945915], [0.0019618533551692963, 0.001956533407792449, 0.0019528113771229982, 0.001988637261092663, 0.0019488141406327486, 0.0019522934453561902, 0.0019554379396140575], [0.0019321413710713387, 0.001950260833837092, 0.001937765977345407, 0.0019813338294625282, 0.0019244705326855183, 0.0019524224335327744, 0.0019536945037543774]], [[0.0019946424290537834, 0.001978556625545025, 0.0019819268491119146, 0.0020125596784055233, 0.0019923143554478884, 0.0019566707778722048, 0.001952262013219297], [0.001977883977815509, 0.001959608867764473, 0.001968641532585025, 0.0019824227783828974, 0.001966861542314291, 0.0019600316882133484, 0.0019565471448004246], [0.0019723870791494846, 0.0019556484185159206, 0.001960964873433113, 0.0019755391404032707, 0.001972456928342581, 0.001957378815859556, 0.0019547129049897194], [0.001961882458999753, 0.0019668051972985268, 0.0019589923322200775, 0.0019610989838838577, 0.00198723329231143, 0.001958656357601285, 0.0019553399179130793], [0.0019818944856524467, 0.0019921762868762016, 0.0019859876483678818, 0.0020148924086242914, 0.0020066804718226194, 0.0019581401720643044, 0.0019555019680410624], [0.0019624934066087008, 0.0019748746417462826, 0.0019767461344599724, 0.0019766087643802166, 0.0019934377633035183, 0.001958707347512245, 0.0019602274987846613], [0.0019675912335515022, 0.0019456855952739716, 0.001945626107044518, 0.001967812655493617, 0.001956124557182193, 0.001953291706740856, 0.001951445359736681]]], [[[0.0019490944687277079, 0.002040244173258543, 0.001972732599824667, 0.0020170765928924084, 0.0019744010642170906, 0.0019489806145429611, 0.0019580277148634195], [0.0019505181116983294, 0.0020282771438360214, 0.001955499639734626, 0.0020059761591255665, 0.001974172657355666, 0.001960697118192911, 0.0019569220021367073], [0.0019391919486224651, 0.0020327751990407705, 0.001964116934686899, 0.001999113243073225, 0.0019542493391782045, 0.0019512843573465943, 0.001956053776666522], [0.0019507658435031772, 0.002041537081822753, 0.0019714776426553726, 0.0020011933520436287, 0.001962908310815692, 0.001952211488969624, 0.0019506693352013826], [0.0019526862306520343, 0.002036262769252062, 0.001965384231880307, 0.0020052569452673197, 0.0019656678196042776, 0.0019515340682119131, 0.001957800006493926], [0.0019503295188769698, 0.002046822104603052, 0.001967406366020441, 0.0020022052340209484, 0.00196312484331429, 0.0019479839829728007, 0.001950486097484827], [0.0019362756283953786, 0.002023534383624792, 0.0019627027213573456, 0.001992327393963933, 0.0019539780914783478, 0.0019497702596709132, 0.001956457505002618]], [[0.002060863422229886, 0.0019968345295637846, 0.002022764878347516, 0.0020458248909562826, 0.002036196645349264, 0.0019675507210195065, 0.0019582626409828663], [0.0020357484463602304, 0.0019831513054668903, 0.002005866263061762, 0.002020974177867174, 0.0020150926429778337, 0.0019768306519836187, 0.001954258419573307], [0.002031270880252123, 0.0019803617615252733, 0.0019974769093096256, 0.002025180496275425, 0.002012725453823805, 0.0019673951901495457, 0.0019562900997698307], [0.0020287095103412867, 0.001985005335882306, 0.002014914993196726, 0.002037788974121213, 0.002025246387347579, 0.0019728911574929953, 0.0019570074509829283], [0.0020581709686666727, 0.001997264102101326, 0.0020112188067287207, 0.0020568552426993847, 0.002033790573477745, 0.0019714203663170338, 0.0019549764692783356], [0.00203397823497653, 0.00198573200032115, 0.002010033233091235, 0.0020143697038292885, 0.00202631251886487, 0.0019687931053340435, 0.001955615356564522], [0.002010362921282649, 0.001973184524103999, 0.001980905421078205, 0.002011431148275733, 0.002005484886467457, 0.0019694743677973747, 0.001964760944247246]], [[0.001999486470595002, 0.0019916119053959846, 0.0019851403776556253, 0.0019777528941631317, 0.001976937986910343, 0.0019662242848426104, 0.0019642349798232317], [0.002001754241064191, 0.0019768252968788147, 0.0019654803909361362, 0.001967380987480283, 0.0019606712739914656, 0.001969889970496297, 0.0019595192279666662], [0.002003338420763612, 0.001983124064281583, 0.001970144221559167, 0.0019654082134366035, 0.0019660929683595896, 0.0019692459609359503, 0.001958344364538789], [0.002007861156016588, 0.001987168798223138, 0.0019840600434690714, 0.0019814164843410254, 0.001969703007489443, 0.0019690790213644505, 0.0019580498337745667], [0.0020121836569160223, 0.0019887920934706926, 0.001962231006473303, 0.0019824765622615814, 0.0019640529062598944, 0.0019631413742899895, 0.0019590745214372873], [0.0020100700203329325, 0.001996978186070919, 0.0019797938875854015, 0.001972790341824293, 0.0019728641491383314, 0.001970375655218959, 0.0019549482967704535], [0.001991769066080451, 0.001970176585018635, 0.0019551245495676994, 0.0019515450112521648, 0.001958750421181321, 0.0019640224054455757, 0.0019521234789863229]], [[0.0020028245635330677, 0.001996856415644288, 0.0019674976356327534, 0.001953901955857873, 0.0019940652418881655, 0.0019641646649688482, 0.001954959938302636], [0.001995844766497612, 0.0019799936562776566, 0.001966798910871148, 0.0019558218773454428, 0.0019870828837156296, 0.001965765841305256, 0.0019517035689204931], [0.0019948454573750496, 0.001976232510060072, 0.0019684943836182356, 0.00198021880351007, 0.0019936293829232454, 0.0019654498901218176, 0.0019556765910238028], [0.0019989353604614735, 0.001989632146432996, 0.0019704566802829504, 0.001966312061995268, 0.001993130659684539, 0.0019713016226887703, 0.0019522259244695306], [0.0020072446204721928, 0.002011402975767851, 0.0019655218347907066, 0.0019602912943810225, 0.0019958773627877235, 0.001974351704120636, 0.001953576225787401], [0.001987726893275976, 0.0019916302990168333, 0.001977794338017702, 0.00196055811829865, 0.001988864503800869, 0.0019709330517798662, 0.001954174367710948], [0.0019805817864835262, 0.0019713593646883965, 0.0019563334062695503, 0.0019620254170149565, 0.0019828020595014095, 0.0019660794641822577, 0.0019502335926517844]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"]}, {\"name\": \"Cross\", \"attn\": [[[[0.0018907840130850673, 0.0006545690703205764, 0.001013795379549265, 0.000669850327540189, 0.0009301647078245878, 0.0010438591707497835, 0.0009906711056828499], [0.0020754754077643156, 0.0016920268535614014, 0.0019179475493729115, 0.0018678444903343916, 0.0024772845208644867, 0.001952912425622344, 0.0017753101419657469], [0.0009741291869431734, 0.0012223860248923302, 0.0010262762662023306, 0.0007965767872519791, 0.0007938087219372392, 0.0008265154319815338, 0.0008562952280044556], [0.001958247972652316, 0.001424274523742497, 0.002145964652299881, 0.0016172884497791529, 0.0016958625055849552, 0.0015427826438099146, 0.001688189571723342], [0.0023526111617684364, 0.0009281941456720233, 0.001895618624985218, 0.0011766733368858695, 0.0016946783289313316, 0.0012597328750416636, 0.0015489495126530528], [0.0021699187345802784, 0.0014338481705635786, 0.001321551506407559, 0.0009670233121141791, 0.0013961985241621733, 0.0009149193065240979, 0.0010450856061652303], [0.0013964945683255792, 0.001091045793145895, 0.0015357251977548003, 0.0013037041062489152, 0.0015433450462296605, 0.0010065571404993534, 0.0013077325420454144]], [[0.0020809979178011417, 0.0005780319916084409, 0.0003582587232813239, 0.00022170293959788978, 0.0006386962486431003, 0.00034999646595679224, 0.0002838528307620436], [0.001976816914975643, 0.0024754067417234182, 0.000747966580092907, 0.0005993059603497386, 0.0011211080709472299, 0.0006994821596890688, 0.0006880377768538892], [0.0047377836890518665, 0.0014896432403475046, 0.0016231206245720387, 0.0009479708969593048, 0.0011624168837442994, 0.0011655943235382438, 0.0009784664725884795], [0.005005617626011372, 0.0017514583887532353, 0.0009242980740964413, 0.0005625000922009349, 0.0014633717946708202, 0.0008473880589008331, 0.0007787581998854876], [0.0023549937177449465, 0.0012948508374392986, 0.0006965223583392799, 0.00076757010538131, 0.00163964310195297, 0.0007374039269052446, 0.0006726642022840679], [0.0033066486939787865, 0.0023656035773456097, 0.002209259429946542, 0.0016873892163857818, 0.0017840780783444643, 0.0016797153512015939, 0.0015469115460291505], [0.003027813509106636, 0.0020618003327399492, 0.002366369590163231, 0.002517594024538994, 0.0019390099914744496, 0.0019140916410833597, 0.0021509991493076086]], [[0.0022792944218963385, 0.0019919401966035366, 0.0015943101607263088, 0.0022225435823202133, 0.0023558377288281918, 0.0018452840158715844, 0.0012925018090754747], [0.004644542466849089, 0.0010289803612977266, 0.0017524220747873187, 0.0022530509158968925, 0.0016745920293033123, 0.0016307373298332095, 0.0011530136689543724], [0.0057770926505327225, 0.0016698201652616262, 0.002215599175542593, 0.0022884567733854055, 0.0025761937722563744, 0.0021749669685959816, 0.0016060919733718038], [0.0026541748084127903, 0.0018652357393875718, 0.0013340865261852741, 0.0014521725242957473, 0.0013119588838890195, 0.0016397774452343583, 0.0014313450083136559], [0.0028310774359852076, 0.0021000802516937256, 0.0013143187388777733, 0.0018712301971390843, 0.0017781280912458897, 0.0017531097400933504, 0.0013506583636626601], [0.00380669254809618, 0.0017940871184691787, 0.0024804575368762016, 0.0027443759609013796, 0.0018057794077321887, 0.0024914664682000875, 0.0016505112871527672], [0.00334381521679461, 0.0019050216069445014, 0.0020033956971019506, 0.0022674137726426125, 0.00373381027020514, 0.001520611229352653, 0.0012498709838837385]], [[0.00552424555644393, 0.0010939461644738913, 0.001326721510849893, 0.0009733344777487218, 0.0010409073438495398, 0.0014692998956888914, 0.000877684447914362], [0.0023036382626742125, 0.0024707585107535124, 0.0011989648919552565, 0.000803917006123811, 0.0011387509293854237, 0.001030918792821467, 0.001159397535957396], [0.007560377474874258, 0.0020573525689542294, 0.0028990437276661396, 0.001539805205538869, 0.0024248172994703054, 0.0018842858262360096, 0.0014485267456620932], [0.0031075109727680683, 0.0012965998612344265, 0.0025684786960482597, 0.002231663092970848, 0.0021676449105143547, 0.0015130097744986415, 0.0018286932026967406], [0.0027166602667421103, 0.0007639912655577064, 0.0017179118003696203, 0.0014066805597394705, 0.0020939670503139496, 0.0013450714759528637, 0.0012892537051811814], [0.009904085658490658, 0.0025122209917753935, 0.0024667757097631693, 0.0014204089529812336, 0.0017825467512011528, 0.0016198607627302408, 0.001401702407747507], [0.0049746171571314335, 0.002077154815196991, 0.0032444277312606573, 0.0019068876281380653, 0.0020264459308236837, 0.0016202162951231003, 0.0017812108853831887]]], [[[0.002658477518707514, 0.0018156202277168632, 0.0015500118024647236, 0.0019565371330827475, 0.0014197926502674818, 0.0018661293433979154, 0.001664025243371725], [0.0018287630518898368, 0.001332121784798801, 0.0014771153219044209, 0.001737777842208743, 0.0010694186203181744, 0.0021008613985031843, 0.0019083819352090359], [0.0020340196788311005, 0.00138017104472965, 0.0014459844678640366, 0.002346516354009509, 0.0013693588552996516, 0.0024996260181069374, 0.0019088669214397669], [0.0018165640067309141, 0.0018687492702156305, 0.0025580604560673237, 0.002879032166674733, 0.0013702873839065433, 0.0025491330306977034, 0.0027570421807467937], [0.002547851298004389, 0.002078116638585925, 0.0022846078500151634, 0.0028454111889004707, 0.0014100633561611176, 0.0026494425255805254, 0.0025880641769617796], [0.001167359878309071, 0.0009526317589916289, 0.0019507530378177762, 0.0019946566317230463, 0.0010655438527464867, 0.0018504844047129154, 0.001813407288864255], [0.0025223346892744303, 0.0019883355125784874, 0.0022900791373103857, 0.002830222714692354, 0.0013058031909167767, 0.0034075395669788122, 0.00313263270072639]], [[0.0041648889891803265, 0.001808580826036632, 0.0017372251022607088, 0.0015502506867051125, 0.0021971329115331173, 0.0020957523956894875, 0.0019681593403220177], [0.0025980148930102587, 0.0013789499644190073, 0.0008632537792436779, 0.0008447412983514369, 0.0012080036103725433, 0.0012934653786942363, 0.0011971810599789023], [0.0018240157514810562, 0.001099047833122313, 0.0006770126055926085, 0.000941593898460269, 0.0010063174413517118, 0.0012845719465985894, 0.001231751055456698], [0.0013563779648393393, 0.0011823860695585608, 0.0010657774982973933, 0.0007591769681312144, 0.0008377753547392786, 0.0011545917950570583, 0.001278886222280562], [0.0019419321324676275, 0.001689083524979651, 0.0015402717981487513, 0.0011793404119089246, 0.0010166671127080917, 0.0016313658561557531, 0.001718121231533587], [0.0010806209174916148, 0.0009072840912267566, 0.0007059099152684212, 0.0007423328934237361, 0.0005641470779664814, 0.0009266595006920397, 0.0009160502813756466], [0.002317517763003707, 0.001541599165648222, 0.0012185124214738607, 0.0010929979616776109, 0.0010756734991446137, 0.001595399691723287, 0.00194378977175802]], [[0.0023334291763603687, 0.0018529348308220506, 0.0017325388034805655, 0.0019951246213167906, 0.0023333742283284664, 0.0017244890332221985, 0.0014216415584087372], [0.0016904674703255296, 0.0010095303878188133, 0.0013901152415201068, 0.0014887346187606454, 0.0015272045275196433, 0.0015667550032958388, 0.0016374621773138642], [0.001592176267877221, 0.0010459194891154766, 0.001009586383588612, 0.0019388339715078473, 0.0018184445798397064, 0.0016162182437255979, 0.0016091439174488187], [0.001971753081306815, 0.0016072383150458336, 0.001416531391441822, 0.0021071929950267076, 0.001886146841570735, 0.00175004405900836, 0.0016489687841385603], [0.002101360587403178, 0.0017673365073278546, 0.001594026805832982, 0.0023062624968588352, 0.0014427899150177836, 0.001959724584594369, 0.0019885131623595953], [0.0010898089967668056, 0.0016087691765278578, 0.0011215177364647388, 0.0014062768314033747, 0.0017474504420533776, 0.0012386838207021356, 0.0013976781629025936], [0.003187142312526703, 0.0030401155818253756, 0.0018296699272468686, 0.0026440047658979893, 0.0023320179898291826, 0.0023024810943752527, 0.0024973235558718443]], [[0.012145438231527805, 0.006730684544891119, 0.004475623369216919, 0.0025855042040348053, 0.002810277510434389, 0.003181559732183814, 0.004316877573728561], [0.004090435337275267, 0.003680396592244506, 0.0029678482096642256, 0.0023773955181241035, 0.0023502924013882875, 0.002523161703720689, 0.0028388211503624916], [0.0022812974639236927, 0.0024115988053381443, 0.002883482025936246, 0.0027147214859724045, 0.0022434962447732687, 0.002481033094227314, 0.003088010475039482], [0.0028317749965935946, 0.0027994737029075623, 0.004251715261489153, 0.003824458224698901, 0.0029001813381910324, 0.003224126761779189, 0.004609220195561647], [0.0025109010748565197, 0.0035420460626482964, 0.004287312738597393, 0.0034669942688196898, 0.002565769711509347, 0.0030328298453241587, 0.004481208976358175], [0.0021659929770976305, 0.002687802305445075, 0.0037452031392604113, 0.0033164550550282, 0.00226609013043344, 0.0027495045214891434, 0.0034568195696920156], [0.002631923882290721, 0.0026737998705357313, 0.004327686503529549, 0.0034943842329084873, 0.0023238896392285824, 0.0030171165708452463, 0.0035135725047439337]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}], \"default_filter\": \"0\", \"root_div_id\": \"bertviz-6be7e19aea9448bf8448673c90bd76f4\", \"layer\": null, \"heads\": null, \"include_layers\": [0, 1]}; // HACK: {\"attention\": [{\"name\": \"Encoder\", \"attn\": [[[[0.0001244435552507639, 0.00011266420187894255, 0.0002516293607186526, 0.00010624696005834267, 0.00013604268315248191, 0.0011972725624218583, 0.0013834101846441627], [0.0003067910438403487, 0.0003797105164267123, 0.0004895984893664718, 0.00017375298193655908, 0.0003455829282756895, 0.0015692779561504722, 0.0019053813302889466], [1.8029280909104273e-05, 1.5208333934424445e-05, 5.61276356165763e-05, 7.531493338319706e-06, 2.1538462533499114e-05, 0.0011991065694019198, 0.0014792606234550476], [0.0003953646228183061, 0.00038482435047626495, 0.0006200792849995196, 0.0003139626351185143, 0.0004176825168542564, 0.001966077135875821, 0.0020922126714140177], [0.00013869850954506546, 0.00014263740740716457, 0.00029617061954922974, 6.48902787361294e-05, 0.00013018528989050537, 0.0014126208843663335, 0.0017707661027088761], [8.030361264843577e-09, 7.032122439909472e-09, 4.2573751102281676e-07, 9.114856092118373e-10, 1.5806364217496593e-08, 0.000583222892601043, 0.0015968417283147573], [4.478146742314948e-09, 3.804466519596872e-09, 2.3985046482266625e-07, 5.783113987689603e-10, 9.901888198271536e-09, 0.0005666323122568429, 0.0015012265648692846]], [[0.00027167663211002946, 0.00020005452097393572, 0.00035258856951259077, 0.0001818927557906136, 0.0002781347429845482, 0.0016339885769411922, 0.0021504932083189487], [0.00021942505554761738, 0.00020838889759033918, 0.00037267437437549233, 0.0002254763967357576, 0.00021141963952686638, 0.001391371712088585, 0.0016143773682415485], [4.66657911601942e-05, 2.8734992156387307e-05, 6.315022619673982e-05, 2.5611572709749453e-05, 3.186591493431479e-05, 0.001683767419308424, 0.0012509392108768225], [0.0005671802209690213, 0.0004542050010059029, 0.0006745471619069576, 0.0004948038258589804, 0.00043935165740549564, 0.002174914116039872, 0.0014460764359682798], [0.00028907047817483544, 0.00024943429161794484, 0.0003815964446403086, 0.00022193806944414973, 0.00020766039961017668, 0.0019101742655038834, 0.00178981339558959], [9.300777747967004e-08, 3.669113013415881e-08, 2.9633733333866985e-07, 2.6537792408021232e-08, 4.920474694358745e-08, 0.0005396560300141573, 0.0010779434815049171], [7.621545705660537e-08, 1.4437678608203441e-08, 1.8405880553018505e-07, 1.7301337251751647e-08, 2.9575497606515455e-08, 0.0005127748590894043, 0.0008898610831238329]], [[0.0007952657761052251, 0.000874561257660389, 0.001177239348180592, 0.0008014417253434658, 0.0009596769232302904, 0.001269379397854209, 0.0017803125083446503], [0.00041581137338653207, 0.0005089842597953975, 0.0005281173507682979, 0.00040344413719139993, 0.0006173507426865399, 0.0019264526199549437, 0.0014920583926141262], [9.19521989999339e-05, 9.744636190589517e-05, 0.00014071501209400594, 8.363241795450449e-05, 0.00010897513129748404, 0.0017378325574100018, 0.0015215340536087751], [0.0005913384375162423, 0.0004664700827561319, 0.0008208560175262392, 0.0007805845816619694, 0.000483395648188889, 0.0016029059188440442, 0.0018685426330193877], [0.0005404638941399753, 0.0004083497915416956, 0.000848443596623838, 0.0005872796173207462, 0.0007631818298250437, 0.0019634985364973545, 0.001559940748848021], [3.8652063949484727e-07, 2.2344001138208114e-07, 1.478648641750624e-06, 2.3362976264706958e-07, 2.789491588828241e-07, 0.0007234656368382275, 0.0009868611814454198], [3.016745324657677e-07, 2.2261338017415255e-07, 1.6161548046511598e-06, 2.1723104737247922e-07, 3.183396302119945e-07, 0.0007928681443445385, 0.0007818498997949064]], [[0.0009111871477216482, 0.0007607329171150923, 0.0008668781374581158, 0.0005971425562165678, 0.0007442832575179636, 0.002466397825628519, 0.002316609723493457], [0.0006017641280777752, 0.0008089006878435612, 0.0006385153392329812, 0.0009037574636749923, 0.00065309286583215, 0.002065225737169385, 0.001821330632083118], [0.00010643474524840713, 9.622338984627277e-05, 0.0001098678694688715, 7.453877333318815e-05, 0.00013202666013967246, 0.0020079489331692457, 0.0018175721634179354], [0.0005654582055285573, 0.0005507713067345321, 0.0007766984635964036, 0.0005084159784018993, 0.0007362078758887947, 0.0017437950009480119, 0.001939503476023674], [0.000545306655112654, 0.00041268038330599666, 0.0005978436674922705, 0.0003216119948774576, 0.0004915076424367726, 0.0018738089129328728, 0.001962667563930154], [2.1019232576691138e-07, 1.6695875615369005e-07, 3.068323906063597e-07, 9.437309245186043e-08, 2.6731277102953754e-07, 0.002782493829727173, 0.0017241952009499073], [1.1789978060505746e-07, 1.2063320298238978e-07, 1.9224439995468856e-07, 7.515063771279529e-08, 2.1620864742999402e-07, 0.00227953027933836, 0.0017691429238766432]]], [[[0.0006736136274412274, 0.0007827648078091443, 0.0009466311894357204, 0.0007495568133890629, 0.000747990095987916, 0.0022481216583400965, 0.0017716604052111506], [0.0006707112188450992, 0.0007978093926794827, 0.0009514944977127016, 0.0007586644496768713, 0.0007552676252089441, 0.002272462472319603, 0.0017774841981008649], [0.0006558556342497468, 0.0007697020773775876, 0.0009414860978722572, 0.0007273551309481263, 0.0007353550754487514, 0.0022797374986112118, 0.0017596272518858314], [0.0006744883139617741, 0.0008064088760875165, 0.0009532605763524771, 0.0007546594715677202, 0.0007490649004466832, 0.0022631227038800716, 0.0017744177021086216], [0.0006843882147222757, 0.0007876748568378389, 0.0009710174635984004, 0.0007631796761415899, 0.0007571456371806562, 0.0022874013520777225, 0.0017770547419786453], [0.0006209186394698918, 0.0007394569693133235, 0.0009067946812137961, 0.0006954854470677674, 0.0006965604261495173, 0.002279226202517748, 0.0017474234336987138], [0.0006109087262302637, 0.0007302847225219011, 0.0008970409398898482, 0.0006975356955081224, 0.0006873281090520322, 0.0022942631039768457, 0.0017433528555557132]], [[0.00039741594810038805, 0.00036316816112957895, 0.0006120743928477168, 0.0004714024253189564, 0.00028561288490891457, 0.001856151269748807, 0.0019523273222148418], [0.0003702104149851948, 0.00034935781150124967, 0.0006142938509583473, 0.0004640950355678797, 0.000279448926448822, 0.001872523222118616, 0.0019456201698631048], [0.0003651919250842184, 0.0003406937757972628, 0.0005949000478722155, 0.0004470513667911291, 0.0002694002178031951, 0.0018642584327608347, 0.00195637927390635], [0.0003868200583383441, 0.0003670902515295893, 0.0006138746393844485, 0.0004709497152362019, 0.00028897947049699724, 0.001857637893408537, 0.0019554717000573874], [0.00037732577766291797, 0.00036234536673873663, 0.0005974201485514641, 0.0004526441334746778, 0.0002779133792500943, 0.0018552833935245872, 0.0019559680949896574], [0.00034810806391760707, 0.00032161467242985964, 0.0005625317571684718, 0.0004147374420426786, 0.0002470424515195191, 0.0018635448068380356, 0.0019463645294308662], [0.00034463501651771367, 0.0003224253887310624, 0.0005597746348939836, 0.0004175882204435766, 0.0002472986525390297, 0.0018574731657281518, 0.001960442401468754]], [[0.0006720508099533617, 0.0007765429327264428, 0.000929080240894109, 0.0006095852586440742, 0.0007760734879411757, 0.0017631046939641237, 0.0019561403896659613], [0.0006775740766897798, 0.0007850467809475958, 0.0009350682375952601, 0.0006176198949106038, 0.0008087765891104937, 0.001745070912875235, 0.0019781014416366816], [0.0006451876251958311, 0.0007548555149696767, 0.0009184554219245911, 0.0005862449761480093, 0.0007760583539493382, 0.0017534219659864902, 0.0019657667726278305], [0.0006744327838532627, 0.0007833449053578079, 0.0009477480198256671, 0.0006119955214671791, 0.0008033321355469525, 0.001752006821334362, 0.001972441328689456], [0.000663267623167485, 0.0007889120024628937, 0.0009342856938019395, 0.0006102737388573587, 0.0007925989339128137, 0.0017525126459077, 0.0019620885141193867], [0.0006444320897571743, 0.0007449389086104929, 0.000903993146494031, 0.0005703219212591648, 0.0007627464365214109, 0.0017556716920807958, 0.0019702457357198], [0.0006372114294208586, 0.0007402864866890013, 0.0008976082899607718, 0.0005650772363878787, 0.0007550687296316028, 0.0017432798631489277, 0.001971327932551503]], [[0.00031113281147554517, 0.0003014165267813951, 0.0005125253228470683, 0.00037184703978709877, 0.00048024949501268566, 0.0017863217508420348, 0.001973920501768589], [0.0003072340623475611, 0.0002994237293023616, 0.0005116547690704465, 0.0003664434188976884, 0.00048007292207330465, 0.0017902979161590338, 0.001973962876945734], [0.00029468172579072416, 0.00029182492289692163, 0.0005087347817607224, 0.00035208059125579894, 0.0004642635176423937, 0.0017909975722432137, 0.0019813987892121077], [0.0003161942586302757, 0.0003067788784392178, 0.000521274923812598, 0.0003709437733050436, 0.00048547962796874344, 0.001802865881472826, 0.001987478695809841], [0.00030716718174517155, 0.00030071279616095126, 0.0005146324401721358, 0.0003698128566611558, 0.0004759263538289815, 0.001801209757104516, 0.0020009540021419525], [0.00027253921143710613, 0.00026216564583592117, 0.00046917388681322336, 0.0003255120827816427, 0.00043420138536021113, 0.0017805311363190413, 0.001993263605982065], [0.00026691571110859513, 0.00025551332510076463, 0.0004653250507544726, 0.00031945930095389485, 0.0004261027497705072, 0.001778463483788073, 0.001992118777707219]]]], \"left_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}, {\"name\": \"Decoder\", \"attn\": [[[[0.00196792627684772, 0.0019924244843423367, 0.001948700868524611, 0.0019674133509397507, 0.001986196031793952, 0.0019617523066699505, 0.0019482981879264116], [0.001966990064829588, 0.001984128961339593, 0.001958775334060192, 0.0019473930587992072, 0.0019849056843668222, 0.0019569052383303642, 0.0019542898517102003], [0.0019887289963662624, 0.002008314011618495, 0.001972887199372053, 0.0019823319744318724, 0.001994878053665161, 0.0019597038626670837, 0.001955584157258272], [0.001962999114766717, 0.001986286835744977, 0.0019510093843564391, 0.001956256339326501, 0.001964643131941557, 0.001960468478500843, 0.001949843717738986], [0.0019726138561964035, 0.001988248201087117, 0.0019554898608475924, 0.0019500748021528125, 0.0019832623656839132, 0.001954542938619852, 0.00194749073125422], [0.001959957182407379, 0.001996913691982627, 0.001950957695953548, 0.0019815401174128056, 0.001991579309105873, 0.0019605965353548527, 0.0019538707565516233], [0.0019495681626722217, 0.001959865214303136, 0.0019312291406095028, 0.0019465419463813305, 0.0019550775177776814, 0.001962303416803479, 0.0019518262706696987]], [[0.0019666494335979223, 0.0019624142441898584, 0.0019821366295218468, 0.001977745443582535, 0.001985287992283702, 0.001969909993931651, 0.0019510264974087477], [0.0019896994344890118, 0.0019808660726994276, 0.0019979430362582207, 0.0020095149520784616, 0.001986938063055277, 0.0019593099132180214, 0.0019499990157783031], [0.0019712024368345737, 0.001960474532097578, 0.0019970338325947523, 0.001980607397854328, 0.0019794467370957136, 0.0019540139473974705, 0.0019549736753106117], [0.0019732865039259195, 0.0019506958778947592, 0.00198213174007833, 0.0019695444498211145, 0.0019744280725717545, 0.0019560763612389565, 0.0019491029670462012], [0.001985149225220084, 0.001985670533031225, 0.001988215371966362, 0.002003168687224388, 0.001970443641766906, 0.001954195322468877, 0.0019476808374747634], [0.0020008408464491367, 0.001971989404410124, 0.0019777831621468067, 0.0019894782453775406, 0.0019867755472660065, 0.001956373453140259, 0.001948135788552463], [0.0019539613276720047, 0.0019488681573420763, 0.0019527219701558352, 0.0019499191548675299, 0.0019432505359873176, 0.0019520032219588757, 0.0019513150909915566]], [[0.0019518331391736865, 0.0019612747710198164, 0.001951485755853355, 0.0019824847113341093, 0.001951782964169979, 0.001955929910764098, 0.0019495667656883597], [0.001940602553077042, 0.001978034619241953, 0.0019699065014719963, 0.0019708089530467987, 0.0019540726207196712, 0.0019522877410054207, 0.001954081002622843], [0.0019494262523949146, 0.001985308015719056, 0.0019489009864628315, 0.001982596004381776, 0.001960010966286063, 0.0019511992577463388, 0.001959258923307061], [0.001946115749888122, 0.001977519830688834, 0.0019524636445567012, 0.00198703957721591, 0.0019587548449635506, 0.001956515247002244, 0.0019519133493304253], [0.0019592095632106066, 0.001972056459635496, 0.001964532071724534, 0.002002664376050234, 0.001978265354409814, 0.0019637923687696457, 0.0019529638811945915], [0.0019618533551692963, 0.001956533407792449, 0.0019528113771229982, 0.001988637261092663, 0.0019488141406327486, 0.0019522934453561902, 0.0019554379396140575], [0.0019321413710713387, 0.001950260833837092, 0.001937765977345407, 0.0019813338294625282, 0.0019244705326855183, 0.0019524224335327744, 0.0019536945037543774]], [[0.0019946424290537834, 0.001978556625545025, 0.0019819268491119146, 0.0020125596784055233, 0.0019923143554478884, 0.0019566707778722048, 0.001952262013219297], [0.001977883977815509, 0.001959608867764473, 0.001968641532585025, 0.0019824227783828974, 0.001966861542314291, 0.0019600316882133484, 0.0019565471448004246], [0.0019723870791494846, 0.0019556484185159206, 0.001960964873433113, 0.0019755391404032707, 0.001972456928342581, 0.001957378815859556, 0.0019547129049897194], [0.001961882458999753, 0.0019668051972985268, 0.0019589923322200775, 0.0019610989838838577, 0.00198723329231143, 0.001958656357601285, 0.0019553399179130793], [0.0019818944856524467, 0.0019921762868762016, 0.0019859876483678818, 0.0020148924086242914, 0.0020066804718226194, 0.0019581401720643044, 0.0019555019680410624], [0.0019624934066087008, 0.0019748746417462826, 0.0019767461344599724, 0.0019766087643802166, 0.0019934377633035183, 0.001958707347512245, 0.0019602274987846613], [0.0019675912335515022, 0.0019456855952739716, 0.001945626107044518, 0.001967812655493617, 0.001956124557182193, 0.001953291706740856, 0.001951445359736681]]], [[[0.0019490944687277079, 0.002040244173258543, 0.001972732599824667, 0.0020170765928924084, 0.0019744010642170906, 0.0019489806145429611, 0.0019580277148634195], [0.0019505181116983294, 0.0020282771438360214, 0.001955499639734626, 0.0020059761591255665, 0.001974172657355666, 0.001960697118192911, 0.0019569220021367073], [0.0019391919486224651, 0.0020327751990407705, 0.001964116934686899, 0.001999113243073225, 0.0019542493391782045, 0.0019512843573465943, 0.001956053776666522], [0.0019507658435031772, 0.002041537081822753, 0.0019714776426553726, 0.0020011933520436287, 0.001962908310815692, 0.001952211488969624, 0.0019506693352013826], [0.0019526862306520343, 0.002036262769252062, 0.001965384231880307, 0.0020052569452673197, 0.0019656678196042776, 0.0019515340682119131, 0.001957800006493926], [0.0019503295188769698, 0.002046822104603052, 0.001967406366020441, 0.0020022052340209484, 0.00196312484331429, 0.0019479839829728007, 0.001950486097484827], [0.0019362756283953786, 0.002023534383624792, 0.0019627027213573456, 0.001992327393963933, 0.0019539780914783478, 0.0019497702596709132, 0.001956457505002618]], [[0.002060863422229886, 0.0019968345295637846, 0.002022764878347516, 0.0020458248909562826, 0.002036196645349264, 0.0019675507210195065, 0.0019582626409828663], [0.0020357484463602304, 0.0019831513054668903, 0.002005866263061762, 0.002020974177867174, 0.0020150926429778337, 0.0019768306519836187, 0.001954258419573307], [0.002031270880252123, 0.0019803617615252733, 0.0019974769093096256, 0.002025180496275425, 0.002012725453823805, 0.0019673951901495457, 0.0019562900997698307], [0.0020287095103412867, 0.001985005335882306, 0.002014914993196726, 0.002037788974121213, 0.002025246387347579, 0.0019728911574929953, 0.0019570074509829283], [0.0020581709686666727, 0.001997264102101326, 0.0020112188067287207, 0.0020568552426993847, 0.002033790573477745, 0.0019714203663170338, 0.0019549764692783356], [0.00203397823497653, 0.00198573200032115, 0.002010033233091235, 0.0020143697038292885, 0.00202631251886487, 0.0019687931053340435, 0.001955615356564522], [0.002010362921282649, 0.001973184524103999, 0.001980905421078205, 0.002011431148275733, 0.002005484886467457, 0.0019694743677973747, 0.001964760944247246]], [[0.001999486470595002, 0.0019916119053959846, 0.0019851403776556253, 0.0019777528941631317, 0.001976937986910343, 0.0019662242848426104, 0.0019642349798232317], [0.002001754241064191, 0.0019768252968788147, 0.0019654803909361362, 0.001967380987480283, 0.0019606712739914656, 0.001969889970496297, 0.0019595192279666662], [0.002003338420763612, 0.001983124064281583, 0.001970144221559167, 0.0019654082134366035, 0.0019660929683595896, 0.0019692459609359503, 0.001958344364538789], [0.002007861156016588, 0.001987168798223138, 0.0019840600434690714, 0.0019814164843410254, 0.001969703007489443, 0.0019690790213644505, 0.0019580498337745667], [0.0020121836569160223, 0.0019887920934706926, 0.001962231006473303, 0.0019824765622615814, 0.0019640529062598944, 0.0019631413742899895, 0.0019590745214372873], [0.0020100700203329325, 0.001996978186070919, 0.0019797938875854015, 0.001972790341824293, 0.0019728641491383314, 0.001970375655218959, 0.0019549482967704535], [0.001991769066080451, 0.001970176585018635, 0.0019551245495676994, 0.0019515450112521648, 0.001958750421181321, 0.0019640224054455757, 0.0019521234789863229]], [[0.0020028245635330677, 0.001996856415644288, 0.0019674976356327534, 0.001953901955857873, 0.0019940652418881655, 0.0019641646649688482, 0.001954959938302636], [0.001995844766497612, 0.0019799936562776566, 0.001966798910871148, 0.0019558218773454428, 0.0019870828837156296, 0.001965765841305256, 0.0019517035689204931], [0.0019948454573750496, 0.001976232510060072, 0.0019684943836182356, 0.00198021880351007, 0.0019936293829232454, 0.0019654498901218176, 0.0019556765910238028], [0.0019989353604614735, 0.001989632146432996, 0.0019704566802829504, 0.001966312061995268, 0.001993130659684539, 0.0019713016226887703, 0.0019522259244695306], [0.0020072446204721928, 0.002011402975767851, 0.0019655218347907066, 0.0019602912943810225, 0.0019958773627877235, 0.001974351704120636, 0.001953576225787401], [0.001987726893275976, 0.0019916302990168333, 0.001977794338017702, 0.00196055811829865, 0.001988864503800869, 0.0019709330517798662, 0.001954174367710948], [0.0019805817864835262, 0.0019713593646883965, 0.0019563334062695503, 0.0019620254170149565, 0.0019828020595014095, 0.0019660794641822577, 0.0019502335926517844]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"]}, {\"name\": \"Cross\", \"attn\": [[[[0.0018907840130850673, 0.0006545690703205764, 0.001013795379549265, 0.000669850327540189, 0.0009301647078245878, 0.0010438591707497835, 0.0009906711056828499], [0.0020754754077643156, 0.0016920268535614014, 0.0019179475493729115, 0.0018678444903343916, 0.0024772845208644867, 0.001952912425622344, 0.0017753101419657469], [0.0009741291869431734, 0.0012223860248923302, 0.0010262762662023306, 0.0007965767872519791, 0.0007938087219372392, 0.0008265154319815338, 0.0008562952280044556], [0.001958247972652316, 0.001424274523742497, 0.002145964652299881, 0.0016172884497791529, 0.0016958625055849552, 0.0015427826438099146, 0.001688189571723342], [0.0023526111617684364, 0.0009281941456720233, 0.001895618624985218, 0.0011766733368858695, 0.0016946783289313316, 0.0012597328750416636, 0.0015489495126530528], [0.0021699187345802784, 0.0014338481705635786, 0.001321551506407559, 0.0009670233121141791, 0.0013961985241621733, 0.0009149193065240979, 0.0010450856061652303], [0.0013964945683255792, 0.001091045793145895, 0.0015357251977548003, 0.0013037041062489152, 0.0015433450462296605, 0.0010065571404993534, 0.0013077325420454144]], [[0.0020809979178011417, 0.0005780319916084409, 0.0003582587232813239, 0.00022170293959788978, 0.0006386962486431003, 0.00034999646595679224, 0.0002838528307620436], [0.001976816914975643, 0.0024754067417234182, 0.000747966580092907, 0.0005993059603497386, 0.0011211080709472299, 0.0006994821596890688, 0.0006880377768538892], [0.0047377836890518665, 0.0014896432403475046, 0.0016231206245720387, 0.0009479708969593048, 0.0011624168837442994, 0.0011655943235382438, 0.0009784664725884795], [0.005005617626011372, 0.0017514583887532353, 0.0009242980740964413, 0.0005625000922009349, 0.0014633717946708202, 0.0008473880589008331, 0.0007787581998854876], [0.0023549937177449465, 0.0012948508374392986, 0.0006965223583392799, 0.00076757010538131, 0.00163964310195297, 0.0007374039269052446, 0.0006726642022840679], [0.0033066486939787865, 0.0023656035773456097, 0.002209259429946542, 0.0016873892163857818, 0.0017840780783444643, 0.0016797153512015939, 0.0015469115460291505], [0.003027813509106636, 0.0020618003327399492, 0.002366369590163231, 0.002517594024538994, 0.0019390099914744496, 0.0019140916410833597, 0.0021509991493076086]], [[0.0022792944218963385, 0.0019919401966035366, 0.0015943101607263088, 0.0022225435823202133, 0.0023558377288281918, 0.0018452840158715844, 0.0012925018090754747], [0.004644542466849089, 0.0010289803612977266, 0.0017524220747873187, 0.0022530509158968925, 0.0016745920293033123, 0.0016307373298332095, 0.0011530136689543724], [0.0057770926505327225, 0.0016698201652616262, 0.002215599175542593, 0.0022884567733854055, 0.0025761937722563744, 0.0021749669685959816, 0.0016060919733718038], [0.0026541748084127903, 0.0018652357393875718, 0.0013340865261852741, 0.0014521725242957473, 0.0013119588838890195, 0.0016397774452343583, 0.0014313450083136559], [0.0028310774359852076, 0.0021000802516937256, 0.0013143187388777733, 0.0018712301971390843, 0.0017781280912458897, 0.0017531097400933504, 0.0013506583636626601], [0.00380669254809618, 0.0017940871184691787, 0.0024804575368762016, 0.0027443759609013796, 0.0018057794077321887, 0.0024914664682000875, 0.0016505112871527672], [0.00334381521679461, 0.0019050216069445014, 0.0020033956971019506, 0.0022674137726426125, 0.00373381027020514, 0.001520611229352653, 0.0012498709838837385]], [[0.00552424555644393, 0.0010939461644738913, 0.001326721510849893, 0.0009733344777487218, 0.0010409073438495398, 0.0014692998956888914, 0.000877684447914362], [0.0023036382626742125, 0.0024707585107535124, 0.0011989648919552565, 0.000803917006123811, 0.0011387509293854237, 0.001030918792821467, 0.001159397535957396], [0.007560377474874258, 0.0020573525689542294, 0.0028990437276661396, 0.001539805205538869, 0.0024248172994703054, 0.0018842858262360096, 0.0014485267456620932], [0.0031075109727680683, 0.0012965998612344265, 0.0025684786960482597, 0.002231663092970848, 0.0021676449105143547, 0.0015130097744986415, 0.0018286932026967406], [0.0027166602667421103, 0.0007639912655577064, 0.0017179118003696203, 0.0014066805597394705, 0.0020939670503139496, 0.0013450714759528637, 0.0012892537051811814], [0.009904085658490658, 0.0025122209917753935, 0.0024667757097631693, 0.0014204089529812336, 0.0017825467512011528, 0.0016198607627302408, 0.001401702407747507], [0.0049746171571314335, 0.002077154815196991, 0.0032444277312606573, 0.0019068876281380653, 0.0020264459308236837, 0.0016202162951231003, 0.0017812108853831887]]], [[[0.002658477518707514, 0.0018156202277168632, 0.0015500118024647236, 0.0019565371330827475, 0.0014197926502674818, 0.0018661293433979154, 0.001664025243371725], [0.0018287630518898368, 0.001332121784798801, 0.0014771153219044209, 0.001737777842208743, 0.0010694186203181744, 0.0021008613985031843, 0.0019083819352090359], [0.0020340196788311005, 0.00138017104472965, 0.0014459844678640366, 0.002346516354009509, 0.0013693588552996516, 0.0024996260181069374, 0.0019088669214397669], [0.0018165640067309141, 0.0018687492702156305, 0.0025580604560673237, 0.002879032166674733, 0.0013702873839065433, 0.0025491330306977034, 0.0027570421807467937], [0.002547851298004389, 0.002078116638585925, 0.0022846078500151634, 0.0028454111889004707, 0.0014100633561611176, 0.0026494425255805254, 0.0025880641769617796], [0.001167359878309071, 0.0009526317589916289, 0.0019507530378177762, 0.0019946566317230463, 0.0010655438527464867, 0.0018504844047129154, 0.001813407288864255], [0.0025223346892744303, 0.0019883355125784874, 0.0022900791373103857, 0.002830222714692354, 0.0013058031909167767, 0.0034075395669788122, 0.00313263270072639]], [[0.0041648889891803265, 0.001808580826036632, 0.0017372251022607088, 0.0015502506867051125, 0.0021971329115331173, 0.0020957523956894875, 0.0019681593403220177], [0.0025980148930102587, 0.0013789499644190073, 0.0008632537792436779, 0.0008447412983514369, 0.0012080036103725433, 0.0012934653786942363, 0.0011971810599789023], [0.0018240157514810562, 0.001099047833122313, 0.0006770126055926085, 0.000941593898460269, 0.0010063174413517118, 0.0012845719465985894, 0.001231751055456698], [0.0013563779648393393, 0.0011823860695585608, 0.0010657774982973933, 0.0007591769681312144, 0.0008377753547392786, 0.0011545917950570583, 0.001278886222280562], [0.0019419321324676275, 0.001689083524979651, 0.0015402717981487513, 0.0011793404119089246, 0.0010166671127080917, 0.0016313658561557531, 0.001718121231533587], [0.0010806209174916148, 0.0009072840912267566, 0.0007059099152684212, 0.0007423328934237361, 0.0005641470779664814, 0.0009266595006920397, 0.0009160502813756466], [0.002317517763003707, 0.001541599165648222, 0.0012185124214738607, 0.0010929979616776109, 0.0010756734991446137, 0.001595399691723287, 0.00194378977175802]], [[0.0023334291763603687, 0.0018529348308220506, 0.0017325388034805655, 0.0019951246213167906, 0.0023333742283284664, 0.0017244890332221985, 0.0014216415584087372], [0.0016904674703255296, 0.0010095303878188133, 0.0013901152415201068, 0.0014887346187606454, 0.0015272045275196433, 0.0015667550032958388, 0.0016374621773138642], [0.001592176267877221, 0.0010459194891154766, 0.001009586383588612, 0.0019388339715078473, 0.0018184445798397064, 0.0016162182437255979, 0.0016091439174488187], [0.001971753081306815, 0.0016072383150458336, 0.001416531391441822, 0.0021071929950267076, 0.001886146841570735, 0.00175004405900836, 0.0016489687841385603], [0.002101360587403178, 0.0017673365073278546, 0.001594026805832982, 0.0023062624968588352, 0.0014427899150177836, 0.001959724584594369, 0.0019885131623595953], [0.0010898089967668056, 0.0016087691765278578, 0.0011215177364647388, 0.0014062768314033747, 0.0017474504420533776, 0.0012386838207021356, 0.0013976781629025936], [0.003187142312526703, 0.0030401155818253756, 0.0018296699272468686, 0.0026440047658979893, 0.0023320179898291826, 0.0023024810943752527, 0.0024973235558718443]], [[0.012145438231527805, 0.006730684544891119, 0.004475623369216919, 0.0025855042040348053, 0.002810277510434389, 0.003181559732183814, 0.004316877573728561], [0.004090435337275267, 0.003680396592244506, 0.0029678482096642256, 0.0023773955181241035, 0.0023502924013882875, 0.002523161703720689, 0.0028388211503624916], [0.0022812974639236927, 0.0024115988053381443, 0.002883482025936246, 0.0027147214859724045, 0.0022434962447732687, 0.002481033094227314, 0.003088010475039482], [0.0028317749965935946, 0.0027994737029075623, 0.004251715261489153, 0.003824458224698901, 0.0029001813381910324, 0.003224126761779189, 0.004609220195561647], [0.0025109010748565197, 0.0035420460626482964, 0.004287312738597393, 0.0034669942688196898, 0.002565769711509347, 0.0030328298453241587, 0.004481208976358175], [0.0021659929770976305, 0.002687802305445075, 0.0037452031392604113, 0.0033164550550282, 0.00226609013043344, 0.0027495045214891434, 0.0034568195696920156], [0.002631923882290721, 0.0026737998705357313, 0.004327686503529549, 0.0034943842329084873, 0.0023238896392285824, 0.0030171165708452463, 0.0035135725047439337]]]], \"left_text\": [\"Je\", \" suis\", \" un\", \" prof\", \"esse\", \"ur\", \".\"], \"right_text\": [\"I\", \" am\", \" a\", \" teacher\", \".\", \"\", \"\"]}], \"default_filter\": \"0\", \"root_div_id\": \"bertviz-6be7e19aea9448bf8448673c90bd76f4\", \"layer\": null, \"heads\": null, \"include_layers\": [0, 1]} is a template marker that is replaced by actual params.\n",
       "    const TEXT_SIZE = 15;\n",
       "    const BOXWIDTH = 110;\n",
       "    const BOXHEIGHT = 22.5;\n",
       "    const MATRIX_WIDTH = 115;\n",
       "    const CHECKBOX_SIZE = 20;\n",
       "    const TEXT_TOP = 30;\n",
       "\n",
       "    console.log(\"d3 version\", d3.version)\n",
       "    let headColors;\n",
       "    try {\n",
       "        headColors = d3.scaleOrdinal(d3.schemeCategory10);\n",
       "    } catch (err) {\n",
       "        console.log('Older d3 version')\n",
       "        headColors = d3.scale.category10();\n",
       "    }\n",
       "    let config = {};\n",
       "    initialize();\n",
       "    renderVis();\n",
       "\n",
       "    function initialize() {\n",
       "        config.attention = params['attention'];\n",
       "        config.filter = params['default_filter'];\n",
       "        config.rootDivId = params['root_div_id'];\n",
       "        config.nLayers = config.attention[config.filter]['attn'].length;\n",
       "        config.nHeads = config.attention[config.filter]['attn'][0].length;\n",
       "        config.layers = params['include_layers']\n",
       "\n",
       "        if (params['heads']) {\n",
       "            config.headVis = new Array(config.nHeads).fill(false);\n",
       "            params['heads'].forEach(x => config.headVis[x] = true);\n",
       "        } else {\n",
       "            config.headVis = new Array(config.nHeads).fill(true);\n",
       "        }\n",
       "        config.initialTextLength = config.attention[config.filter].right_text.length;\n",
       "        config.layer_seq = (params['layer'] == null ? 0 : config.layers.findIndex(layer => params['layer'] === layer));\n",
       "        config.layer = config.layers[config.layer_seq]\n",
       "\n",
       "        let layerEl = $(`#${config.rootDivId} #layer`);\n",
       "        for (const layer of config.layers) {\n",
       "            layerEl.append($(\"<option />\").val(layer).text(layer));\n",
       "        }\n",
       "        layerEl.val(config.layer).change();\n",
       "        layerEl.on('change', function (e) {\n",
       "            config.layer = +e.currentTarget.value;\n",
       "            config.layer_seq = config.layers.findIndex(layer => config.layer === layer);\n",
       "            renderVis();\n",
       "        });\n",
       "\n",
       "        $(`#${config.rootDivId} #filter`).on('change', function (e) {\n",
       "            config.filter = e.currentTarget.value;\n",
       "            renderVis();\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function renderVis() {\n",
       "\n",
       "        // Load parameters\n",
       "        const attnData = config.attention[config.filter];\n",
       "        const leftText = attnData.left_text;\n",
       "        const rightText = attnData.right_text;\n",
       "\n",
       "        // Select attention for given layer\n",
       "        const layerAttention = attnData.attn[config.layer_seq];\n",
       "\n",
       "        // Clear vis\n",
       "        $(`#${config.rootDivId} #vis`).empty();\n",
       "\n",
       "        // Determine size of visualization\n",
       "        const height = Math.max(leftText.length, rightText.length) * BOXHEIGHT + TEXT_TOP;\n",
       "        const svg = d3.select(`#${config.rootDivId} #vis`)\n",
       "            .append('svg')\n",
       "            .attr(\"width\", \"100%\")\n",
       "            .attr(\"height\", height + \"px\");\n",
       "\n",
       "        // Display tokens on left and right side of visualization\n",
       "        renderText(svg, leftText, true, layerAttention, 0);\n",
       "        renderText(svg, rightText, false, layerAttention, MATRIX_WIDTH + BOXWIDTH);\n",
       "\n",
       "        // Render attention arcs\n",
       "        renderAttention(svg, layerAttention);\n",
       "\n",
       "        // Draw squares at top of visualization, one for each head\n",
       "        drawCheckboxes(0, svg, layerAttention);\n",
       "    }\n",
       "\n",
       "    function renderText(svg, text, isLeft, attention, leftPos) {\n",
       "\n",
       "        const textContainer = svg.append(\"svg:g\")\n",
       "            .attr(\"id\", isLeft ? \"left\" : \"right\");\n",
       "\n",
       "        // Add attention highlights superimposed over words\n",
       "        textContainer.append(\"g\")\n",
       "            .classed(\"attentionBoxes\", true)\n",
       "            .selectAll(\"g\")\n",
       "            .data(attention)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .attr(\"head-index\", (d, i) => i)\n",
       "            .selectAll(\"rect\")\n",
       "            .data(d => isLeft ? d : transpose(d)) // if right text, transpose attention to get right-to-left weights\n",
       "            .enter()\n",
       "            .append(\"rect\")\n",
       "            .attr(\"x\", function () {\n",
       "                var headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                return leftPos + boxOffsets(headIndex);\n",
       "            })\n",
       "            .attr(\"y\", (+1) * BOXHEIGHT)\n",
       "            .attr(\"width\", BOXWIDTH / activeHeads())\n",
       "            .attr(\"height\", BOXHEIGHT)\n",
       "            .attr(\"fill\", function () {\n",
       "                return headColors(+this.parentNode.getAttribute(\"head-index\"))\n",
       "            })\n",
       "            .style(\"opacity\", 0.0);\n",
       "\n",
       "        const tokenContainer = textContainer.append(\"g\").selectAll(\"g\")\n",
       "            .data(text)\n",
       "            .enter()\n",
       "            .append(\"g\");\n",
       "\n",
       "        // Add gray background that appears when hovering over text\n",
       "        tokenContainer.append(\"rect\")\n",
       "            .classed(\"background\", true)\n",
       "            .style(\"opacity\", 0.0)\n",
       "            .attr(\"fill\", \"lightgray\")\n",
       "            .attr(\"x\", leftPos)\n",
       "            .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT)\n",
       "            .attr(\"width\", BOXWIDTH)\n",
       "            .attr(\"height\", BOXHEIGHT);\n",
       "\n",
       "        // Add token text\n",
       "        const textEl = tokenContainer.append(\"text\")\n",
       "            .text(d => d)\n",
       "            .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "            .style(\"cursor\", \"default\")\n",
       "            .style(\"-webkit-user-select\", \"none\")\n",
       "            .attr(\"x\", leftPos)\n",
       "            .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT);\n",
       "\n",
       "        if (isLeft) {\n",
       "            textEl.style(\"text-anchor\", \"end\")\n",
       "                .attr(\"dx\", BOXWIDTH - 0.5 * TEXT_SIZE)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "        } else {\n",
       "            textEl.style(\"text-anchor\", \"start\")\n",
       "                .attr(\"dx\", +0.5 * TEXT_SIZE)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "        }\n",
       "\n",
       "        tokenContainer.on(\"mouseover\", function (d, index) {\n",
       "\n",
       "            // Show gray background for moused-over token\n",
       "            textContainer.selectAll(\".background\")\n",
       "                .style(\"opacity\", (d, i) => i === index ? 1.0 : 0.0)\n",
       "\n",
       "            // Reset visibility attribute for any previously highlighted attention arcs\n",
       "            svg.select(\"#attention\")\n",
       "                .selectAll(\"line[visibility='visible']\")\n",
       "                .attr(\"visibility\", null)\n",
       "\n",
       "            // Hide group containing attention arcs\n",
       "            svg.select(\"#attention\").attr(\"visibility\", \"hidden\");\n",
       "\n",
       "            // Set to visible appropriate attention arcs to be highlighted\n",
       "            if (isLeft) {\n",
       "                svg.select(\"#attention\").selectAll(\"line[left-token-index='\" + index + \"']\").attr(\"visibility\", \"visible\");\n",
       "            } else {\n",
       "                svg.select(\"#attention\").selectAll(\"line[right-token-index='\" + index + \"']\").attr(\"visibility\", \"visible\");\n",
       "            }\n",
       "\n",
       "            // Update color boxes superimposed over tokens\n",
       "            const id = isLeft ? \"right\" : \"left\";\n",
       "            const leftPos = isLeft ? MATRIX_WIDTH + BOXWIDTH : 0;\n",
       "            svg.select(\"#\" + id)\n",
       "                .selectAll(\".attentionBoxes\")\n",
       "                .selectAll(\"g\")\n",
       "                .attr(\"head-index\", (d, i) => i)\n",
       "                .selectAll(\"rect\")\n",
       "                .attr(\"x\", function () {\n",
       "                    const headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                    return leftPos + boxOffsets(headIndex);\n",
       "                })\n",
       "                .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT)\n",
       "                .attr(\"width\", BOXWIDTH / activeHeads())\n",
       "                .attr(\"height\", BOXHEIGHT)\n",
       "                .style(\"opacity\", function (d) {\n",
       "                    const headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                    if (config.headVis[headIndex])\n",
       "                        if (d) {\n",
       "                            return d[index];\n",
       "                        } else {\n",
       "                            return 0.0;\n",
       "                        }\n",
       "                    else\n",
       "                        return 0.0;\n",
       "                });\n",
       "        });\n",
       "\n",
       "        textContainer.on(\"mouseleave\", function () {\n",
       "\n",
       "            // Unhighlight selected token\n",
       "            d3.select(this).selectAll(\".background\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "\n",
       "            // Reset visibility attributes for previously selected lines\n",
       "            svg.select(\"#attention\")\n",
       "                .selectAll(\"line[visibility='visible']\")\n",
       "                .attr(\"visibility\", null) ;\n",
       "            svg.select(\"#attention\").attr(\"visibility\", \"visible\");\n",
       "\n",
       "            // Reset highlights superimposed over tokens\n",
       "            svg.selectAll(\".attentionBoxes\")\n",
       "                .selectAll(\"g\")\n",
       "                .selectAll(\"rect\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function renderAttention(svg, attention) {\n",
       "\n",
       "        // Remove previous dom elements\n",
       "        svg.select(\"#attention\").remove();\n",
       "\n",
       "        // Add new elements\n",
       "        svg.append(\"g\")\n",
       "            .attr(\"id\", \"attention\") // Container for all attention arcs\n",
       "            .selectAll(\".headAttention\")\n",
       "            .data(attention)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .classed(\"headAttention\", true) // Group attention arcs by head\n",
       "            .attr(\"head-index\", (d, i) => i)\n",
       "            .selectAll(\".tokenAttention\")\n",
       "            .data(d => d)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .classed(\"tokenAttention\", true) // Group attention arcs by left token\n",
       "            .attr(\"left-token-index\", (d, i) => i)\n",
       "            .selectAll(\"line\")\n",
       "            .data(d => d)\n",
       "            .enter()\n",
       "            .append(\"line\")\n",
       "            .attr(\"x1\", BOXWIDTH)\n",
       "            .attr(\"y1\", function () {\n",
       "                const leftTokenIndex = +this.parentNode.getAttribute(\"left-token-index\")\n",
       "                return TEXT_TOP + leftTokenIndex * BOXHEIGHT + (BOXHEIGHT / 2)\n",
       "            })\n",
       "            .attr(\"x2\", BOXWIDTH + MATRIX_WIDTH)\n",
       "            .attr(\"y2\", (d, rightTokenIndex) => TEXT_TOP + rightTokenIndex * BOXHEIGHT + (BOXHEIGHT / 2))\n",
       "            .attr(\"stroke-width\", 2)\n",
       "            .attr(\"stroke\", function () {\n",
       "                const headIndex = +this.parentNode.parentNode.getAttribute(\"head-index\");\n",
       "                return headColors(headIndex)\n",
       "            })\n",
       "            .attr(\"left-token-index\", function () {\n",
       "                return +this.parentNode.getAttribute(\"left-token-index\")\n",
       "            })\n",
       "            .attr(\"right-token-index\", (d, i) => i)\n",
       "        ;\n",
       "        updateAttention(svg)\n",
       "    }\n",
       "\n",
       "    function updateAttention(svg) {\n",
       "        svg.select(\"#attention\")\n",
       "            .selectAll(\"line\")\n",
       "            .attr(\"stroke-opacity\", function (d) {\n",
       "                const headIndex = +this.parentNode.parentNode.getAttribute(\"head-index\");\n",
       "                // If head is selected\n",
       "                if (config.headVis[headIndex]) {\n",
       "                    // Set opacity to attention weight divided by number of active heads\n",
       "                    return d / activeHeads()\n",
       "                } else {\n",
       "                    return 0.0;\n",
       "                }\n",
       "            })\n",
       "    }\n",
       "\n",
       "    function boxOffsets(i) {\n",
       "        const numHeadsAbove = config.headVis.reduce(\n",
       "            function (acc, val, cur) {\n",
       "                return val && cur < i ? acc + 1 : acc;\n",
       "            }, 0);\n",
       "        return numHeadsAbove * (BOXWIDTH / activeHeads());\n",
       "    }\n",
       "\n",
       "    function activeHeads() {\n",
       "        return config.headVis.reduce(function (acc, val) {\n",
       "            return val ? acc + 1 : acc;\n",
       "        }, 0);\n",
       "    }\n",
       "\n",
       "    function drawCheckboxes(top, svg) {\n",
       "        const checkboxContainer = svg.append(\"g\");\n",
       "        const checkbox = checkboxContainer.selectAll(\"rect\")\n",
       "            .data(config.headVis)\n",
       "            .enter()\n",
       "            .append(\"rect\")\n",
       "            .attr(\"fill\", (d, i) => headColors(i))\n",
       "            .attr(\"x\", (d, i) => i * CHECKBOX_SIZE)\n",
       "            .attr(\"y\", top)\n",
       "            .attr(\"width\", CHECKBOX_SIZE)\n",
       "            .attr(\"height\", CHECKBOX_SIZE);\n",
       "\n",
       "        function updateCheckboxes() {\n",
       "            checkboxContainer.selectAll(\"rect\")\n",
       "                .data(config.headVis)\n",
       "                .attr(\"fill\", (d, i) => d ? headColors(i): lighten(headColors(i)));\n",
       "        }\n",
       "\n",
       "        updateCheckboxes();\n",
       "\n",
       "        checkbox.on(\"click\", function (d, i) {\n",
       "            if (config.headVis[i] && activeHeads() === 1) return;\n",
       "            config.headVis[i] = !config.headVis[i];\n",
       "            updateCheckboxes();\n",
       "            updateAttention(svg);\n",
       "        });\n",
       "\n",
       "        checkbox.on(\"dblclick\", function (d, i) {\n",
       "            // If we double click on the only active head then reset\n",
       "            if (config.headVis[i] && activeHeads() === 1) {\n",
       "                config.headVis = new Array(config.nHeads).fill(true);\n",
       "            } else {\n",
       "                config.headVis = new Array(config.nHeads).fill(false);\n",
       "                config.headVis[i] = true;\n",
       "            }\n",
       "            updateCheckboxes();\n",
       "            updateAttention(svg);\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function lighten(color) {\n",
       "        const c = d3.hsl(color);\n",
       "        const increment = (1 - c.l) * 0.6;\n",
       "        c.l += increment;\n",
       "        c.s -= increment;\n",
       "        return c;\n",
       "    }\n",
       "\n",
       "    function transpose(mat) {\n",
       "        return mat[0].map(function (col, i) {\n",
       "            return mat.map(function (row) {\n",
       "                return row[i];\n",
       "            });\n",
       "        });\n",
       "    }\n",
       "\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head_view(\n",
    "    encoder_attention=tensor_encoder_attn,\n",
    "    decoder_attention=tensor_decoder_attn,\n",
    "    cross_attention=tensor_cross_attn,\n",
    "    encoder_tokens=tokens_a,\n",
    "    decoder_tokens=tokens_b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841b166-1814-4a7e-8787-faa663c0eddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
